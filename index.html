<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>AIToolbox: AI-Toolbox</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">AIToolbox
   </div>
   <div id="projectbrief">A library that offers tools for AI problem solving.</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('index.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">AI-Toolbox </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a href="https://github.com/Svalorzen/AI-Toolbox/actions/workflows/build_cmake.yml"><object type="image/svg+xml" data="https://github.com/Svalorzen/AI-Toolbox/actions/workflows/build_cmake.yml/badge.svg" style="pointer-events: none;">AI-Toolbox</object></a></p>
<p><a href="https://www.youtube.com/watch?v=qjSo41DVSXg"><img src="https://user-images.githubusercontent.com/1609228/99919181-3404dc00-2d1c-11eb-8593-0bf6af44cef8.png" alt="Library overview video" class="inline"/></a></p>
<p>This C++ toolbox is aimed at representing and solving common AI problems, implementing an easy-to-use interface which should be hopefully extensible to many problems, while keeping code readable.</p>
<p>Current development includes MDPs, POMDPs and related algorithms. This toolbox was originally developed taking inspiration from the Matlab <code>MDPToolbox</code>, which you can find <a href="https://miat.inrae.fr/MDPtoolbox/">here</a>, and from the <code>pomdp-solve</code> software written by A. R. Cassandra, which you can find <a href="http://www.pomdp.org/code/index.shtml">here</a>.</p>
<p>If you are new to the field of reinforcement learning, we have a few <a href="http://svalorzen.github.io/AI-Toolbox/tutorials.html">simple tutorials</a> that can help you get started. An excellent, more in depth introduction to the basics of reinforcement learning can be found freely online in <a href="http://incompleteideas.net/book/ebook/the-book.html">this book</a>.</p>
<p>If you use this toolbox for research, please consider citing our <a href="https://www.jmlr.org/papers/volume21/18-402/18-402.pdf">JMLR article</a>:</p>
<div class="fragment"><div class="line">@article{JMLR:v21:18-402,</div>
<div class="line">  author  = {Eugenio Bargiacchi and Diederik M. Roijers and Ann Now\&#39;{e}},</div>
<div class="line">  title   = {AI-Toolbox: A C++ library for Reinforcement Learning and Planning (with Python Bindings)},</div>
<div class="line">  journal = {Journal of Machine Learning Research},</div>
<div class="line">  year    = {2020},</div>
<div class="line">  volume  = {21},</div>
<div class="line">  number  = {102},</div>
<div class="line">  pages   = {1-12},</div>
<div class="line">  url     = {http://jmlr.org/papers/v21/18-402.html}</div>
<div class="line">}</div>
</div><!-- fragment --><h1>Example </h1>
<div class="fragment"><div class="line"><span class="comment">// The model can be any custom class that respects a 10-method interface.</span></div>
<div class="line"><span class="keyword">auto</span> model = <a class="code" href="namespaceAIToolbox_1_1POMDP.html#abcd38fafda125d7a1e44a0d04c56e0e0">makeTigerProblem</a>();</div>
<div class="line"><span class="keywordtype">unsigned</span> horizon = 10; <span class="comment">// The horizon of the solution.</span></div>
<div class="line"> </div>
<div class="line"><span class="comment">// The 0.0 is the convergence parameter. It gives a way to stop the</span></div>
<div class="line"><span class="comment">// computation if the policy has converged before the horizon.</span></div>
<div class="line"><a class="code" href="classAIToolbox_1_1POMDP_1_1IncrementalPruning.html">AIToolbox::POMDP::IncrementalPruning</a> solver(horizon, 0.0);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Solve the model and obtain the optimal value function.</span></div>
<div class="line"><span class="keyword">auto</span> [bound, valueFunction] = solver(model);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// We create a policy from the solution to compute the agent&#39;s actions.</span></div>
<div class="line"><span class="comment">// The parameters are the size of the model (SxAxO), and the value function.</span></div>
<div class="line"><a class="code" href="classAIToolbox_1_1POMDP_1_1Policy.html">AIToolbox::POMDP::Policy</a> policy(2, 3, 2, valueFunction);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// We begin a simulation with a uniform belief. We sample from the belief</span></div>
<div class="line"><span class="comment">// in order to get a &quot;real&quot; state for the world, since this code has to</span></div>
<div class="line"><span class="comment">// both emulate the environment and control the agent.</span></div>
<div class="line"><a class="code" href="namespaceAIToolbox_1_1POMDP.html#a4522c5e35483fb30b2c43c271781e8bc">AIToolbox::POMDP::Belief</a> b(2); b &lt;&lt; 0.5, 0.5;</div>
<div class="line"><span class="keyword">auto</span> s = <a class="code" href="namespaceAIToolbox.html#ad73352f6ce65aa95f76b7399cf0d2cb0">AIToolbox::sampleProbability</a>(b.size(), b, rand);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// We sample the first action. The id is to follow the policy tree later.</span></div>
<div class="line"><span class="keyword">auto</span> [a, id] = policy.sampleAction(b, horizon);</div>
<div class="line"> </div>
<div class="line"><span class="keywordtype">double</span> totalReward = 0.0;<span class="comment">// As an example, we store the overall reward.</span></div>
<div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">int</span> t = horizon - 1; t &gt;= 0; --t) {</div>
<div class="line">    <span class="comment">// We advance the world one step.</span></div>
<div class="line">    <span class="keyword">auto</span> [s1, o, r] = model.sampleSOR(s, a);</div>
<div class="line">    totalReward += r;</div>
<div class="line"> </div>
<div class="line">    <span class="comment">// We select our next action from the observation we got.</span></div>
<div class="line">    std::tie(a, <span class="keywordtype">id</span>) = policy.sampleAction(<span class="keywordtype">id</span>, o, t);</div>
<div class="line"> </div>
<div class="line">    s = s1; <span class="comment">// Finally we update the world for the next timestep.</span></div>
<div class="line">}</div>
</div><!-- fragment --><h1>Documentation </h1>
<p>The latest documentation is available <a href="http://svalorzen.github.io/AI-Toolbox/">here</a>. We have a few <a href="http://svalorzen.github.io/AI-Toolbox/tutorials.html">tutorials</a> that can help you get started with the toolbox. The tutorials are in C++, but the <code>examples</code> folder contains equivalent Python code which you can follow along just as well.</p>
<p>For Python docs you can find them by typing <code>help(AIToolbox)</code> from the interpreter. It should show the exported API for each class, along with any differences in input/output.</p>
<h1>Features </h1>
<h3>Cassandra POMDP Format Parsing</h3>
<p>Cassandra's POMDP format is a type of text file that contains a definition of an MDP or POMDP model. You can find some examples <a href="http://pomdp.org/examples/">here</a>. While it is absolutely not necessary to use this format, and you can define models via code, we do parse a reasonable subset of Cassandra's POMDP format, which allows to reuse already defined problems with this library. <a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Impl_1_1CassandraParser.html">Here's the docs on that</a>.</p>
<h3>Python 2 and 3 Bindings!</h3>
<p>The user interface of the library is pretty much the same with Python than what you would get by using simply C++. See the <code>examples</code> folder to see just how much Python and C++ code resemble each other. Since Python does not allow templates, the classes are binded with as many instantiations as possible.</p>
<p>Additionally, the library allows the usage of native Python generative models (where you don't need to specify the transition and reward functions, you only sample next state and reward). This allows for example to directly use OpenAI gym environments with minimal code writing.</p>
<p>That said, if you need to customize a specific implementation to make it perform better on your specific use-cases, or if you want to try something completely new, you will have to use C++.</p>
<h3>Utilities</h3>
<p>The library has an extensive set of utilities which would be too long to enumerate here. In particular, we have utilities for <a href="http://svalorzen.github.io/AI-Toolbox/Combinatorics_8hpp.html">combinatorics</a>, <a href="http://svalorzen.github.io/AI-Toolbox/Polytope_8hpp.html">polytopes</a>, <a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1LP.html">linear programming</a>, <a href="http://svalorzen.github.io/AI-Toolbox/Probability_8hpp.html">sampling and distributions</a>, <a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Statistics.html">automated statistics</a>, <a href="http://svalorzen.github.io/AI-Toolbox/include_2AIToolbox_2POMDP_2Utils_8hpp.html">belief updating</a>, <a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1Trie.html">many</a> <a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1FactorGraph.html">data</a> <a href="http://svalorzen.github.io/AI-Toolbox/FactoredMatrix_8hpp.html">structures</a>, <a href="http://svalorzen.github.io/AI-Toolbox/logging.html">logging</a>, <a href="http://svalorzen.github.io/AI-Toolbox/Seeder_8hpp.html">seeding</a> and much more.</p>
<h3>Bandit/Normal Games:</h3>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadCenter"></th><th class="markdownTableHeadCenter"><b>Models</b> </th><th class="markdownTableHeadCenter"></th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Bandit_1_1Model.html" title="Reinforcement Learning: An Introduction, Ch 2.1, Sutton &amp; Barto">Basic Model</a> </td><td class="markdownTableBodyCenter"></td><td class="markdownTableBodyCenter"></td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter"></td><td class="markdownTableBodyCenter"><b>Policies</b> </td><td class="markdownTableBodyCenter"></td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Bandit_1_1ESRLPolicy.html" title="Exploring selfish reinforcement learning in repeated games with stochastic rewards, Verbeeck et al.">Exploring Selfish Reinforcement Learning (ESRL)</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Bandit_1_1QGreedyPolicy.html" title="A Tutorial on Thompson Sampling, Russo et al.">Q-Greedy Policy</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Bandit_1_1QSoftmaxPolicy.html" title="Reinforcement Learning: An Introduction, Ch 2.3, Sutton &amp; Barto">Softmax Policy</a>  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Bandit_1_1LRPPolicy.html" title="Self-organization in large populations of mobile robots, Ch 3: Stochastic Learning Automata, Unsal">Linear Reward Penalty</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Bandit_1_1ThompsonSamplingPolicy.html" title="Thompson Sampling for 1-Dimensional Exponential Family Bandits, Korda et al.">Thompson Sampling (Student-t distribution)</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Bandit_1_1RandomPolicy.html">Random Policy</a>  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Bandit_1_1TopTwoThompsonSamplingPolicy.html" title="Simple Bayesian Algorithms for Best Arm Identification, Russo">Top-Two Thompson Sampling (Student-t distribution)</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Bandit_1_1SuccessiveRejectsPolicy.html" title="Best Arm Identification in Multi-Armed Bandits, Audibert et al.">Successive Rejects</a> </td><td class="markdownTableBodyCenter"></td></tr>
</table>
<h3>Single Agent MDP/Stochastic Games:</h3>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadCenter"></th><th class="markdownTableHeadCenter"><b>Models</b> </th><th class="markdownTableHeadCenter"></th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1Model.html">Basic Model</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1SparseModel.html">Sparse Model</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1MaximumLikelihoodModel.html">Maximum Likelihood Model</a>  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1SparseMaximumLikelihoodModel.html">Sparse Maximum Likelihood Model</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1ThompsonModel.html">Thompson Model (Dirichlet + Student-t distributions)</a> </td><td class="markdownTableBodyCenter"></td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"></td><td class="markdownTableBodyCenter"><b>Algorithms</b> </td><td class="markdownTableBodyCenter"></td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1DynaQ.html," title="Reinforcement Learning: An Introduction, Ch 9.2, Sutton &amp; Barto">Dyna-Q</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1Dyna2.html" title="Sample-Based Learning and Search with Permanent and Transient Memories, Silver et al.">Dyna2</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1ExpectedSARSA.html" title="A Theoretical and Empirical Analysis of Expected Sarsa, van Seijen et al.">Expected SARSA</a>  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1HystereticQLearning.html" title="Hysteretic Q-Learning : an algorithm for decentralized reinforcement learning in cooperative multi-agent teams, Matignon et al.">Hysteretic Q-Learning</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1ImportanceSampling.html" title="Eligibility Traces for Off-Policy Policy Evaluation, Precup">Importance Sampling</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1LinearProgramming.html">Linear Programming</a>  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1MCTS.html" title="Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search, Coulom">Monte Carlo Tree Search (MCTS)</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1PolicyEvaluation.html" title="Reinforcement Learning: An Introduction, Ch 4.1, Sutton &amp; Barto">Policy Evaluation</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1PolicyIteration.html" title="Reinforcement Learning: An Introduction, Ch 4.3, Sutton &amp; Barto">Policy Iteration</a>  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1PrioritizedSweeping.html" title="Reinforcement Learning: An Introduction, Ch 9.4, Sutton &amp; Barto">Prioritized Sweeping</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1QLearning.html" title="Reinforcement Learning: An Introduction, Ch 6.5, Sutton &amp; Barto">Q-Learning</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1DoubleQLearning.html" title="Double Q-learning, van Hasselt">Double Q-Learning</a>  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1QL.html" title="Q(λ) with Off-Policy Corrections, Harutyunyan et al.">Q(λ)</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1RLearning.html" title="A Reinforcement Learning Method for Maximizing Undiscounted Rewards, Schwartz">R-Learning</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1SARSAL.html" title="Reinforcement Learning: An Introduction, Ch 7.5, Sutton &amp; Barto">SARSA(λ)</a>  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1SARSA.html" title="Reinforcement Learning: An Introduction, Ch 6.4, Sutton &amp; Barto">SARSA</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1RetraceL.html" title="Safe and efficient off-policy reinforcement learning, Munos et al.">Retrace(λ)</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1TreeBackupL.html" title="Eligibility Traces for Off-Policy Policy Evaluation, Precup">Tree Backup(λ)</a>  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1ValueIteration.html" title="Reinforcement Learning: An Introduction, Ch 4.4, Sutton &amp; Barto">Value Iteration</a> </td><td class="markdownTableBodyCenter"></td><td class="markdownTableBodyCenter"></td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"></td><td class="markdownTableBodyCenter"><b>Policies</b> </td><td class="markdownTableBodyCenter"></td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1Policy.html">Basic Policy</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1EpsilonPolicy.html">Epsilon-Greedy Policy</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1QSoftmaxPolicy.html">Softmax Policy</a>  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1QGreedyPolicy.html">Q-Greedy Policy</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1PGAAPPPolicy.html" title="Multi-Agent Learning with Policy Prediction, Zhang et al.">PGA-APP</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1MDP_1_1WoLFPolicy.html" title="Rational and Convergent Learning in Stochastic Games, Bowling et al.">Win or Learn Fast Policy Iteration (WoLF)</a>  </td></tr>
</table>
<h3>Single Agent POMDP:</h3>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadCenter"></th><th class="markdownTableHeadCenter"><b>Models</b> </th><th class="markdownTableHeadCenter"></th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1POMDP_1_1Model.html">Basic Model</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1POMDP_1_1SparseModel.html">Sparse Model</a> </td><td class="markdownTableBodyCenter"></td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter"></td><td class="markdownTableBodyCenter"><b>Algorithms</b> </td><td class="markdownTableBodyCenter"></td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1POMDP_1_1AMDP.html" title="Probabilistic robotics, Ch 16: Approximate POMDP Techniques, Thrun">Augmented MDP (AMDP)</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1POMDP_1_1BlindStrategies.html" title="Incremental methods for computing bounds in partially observable Markov decision processes, Hauskrecht">Blind Strategies</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1POMDP_1_1FastInformedBound.html" title="Value-Function Approximations for Partially Observable Markov Decision Processes, Hauskrecht">Fast Informed Bound</a>  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1POMDP_1_1GapMin.html" title="Closing the Gap: Improved Bounds on Optimal POMDP Solutions, Poupart et al.">GapMin</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1POMDP_1_1IncrementalPruning.html" title="Incremental Pruning: A Simple, Fast, Exact Method for Partially Observable Markov Decision Processes, Cassandra et al.">Incremental Pruning</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1POMDP_1_1LinearSupport.html" title="Algorithms for Partially Observable Markov Decision Processes, Phd Thesis, Cheng">Linear Support</a>  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1POMDP_1_1PERSEUS.html" title="Perseus: Randomized Point-based Value Iteration for POMDPs, Spaan et al.">PERSEUS</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1POMDP_1_1POMCP.html" title="Monte-Carlo Planning in Large POMDPs, Silver et al.">POMCP with UCB1</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1POMDP_1_1PBVI.html" title="Point-based value iteration: An anytime algorithm for POMDPs, Pineau et al.">Point Based Value Iteration (PBVI)</a>  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1POMDP_1_1QMDP.html" title="Probabilistic robotics, Ch 16: Approximate POMDP Techniques, Thrun">QMDP</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1POMDP_1_1RTBSS.html" title="Real-Time Decision Making for Large POMDPs, Paquet et al.">Real-Time Belief State Search (RTBSS)</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1POMDP_1_1SARSOP.html" title="SARSOP: Efficient Point-Based POMDP Planning by Approximating Optimally Reachable Belief Spaces, Kurniawati et al.">SARSOP</a>  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1POMDP_1_1Witness.html" title="Planning and acting in partially observable stochastic domains, Kaelbling et al.">Witness</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1POMDP_1_1rPOMCP.html" title="Dynamic Resource Allocation for Multi-Camera Systems, Phd Thesis, Bargiacchi">rPOMCP</a> </td><td class="markdownTableBodyCenter"></td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter"></td><td class="markdownTableBodyCenter"><b>Policies</b> </td><td class="markdownTableBodyCenter"></td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1POMDP_1_1Policy.html">Basic Policy</a> </td><td class="markdownTableBodyCenter"></td><td class="markdownTableBodyCenter"></td></tr>
</table>
<h3>Factored/Joint Multi-Agent:</h3>
<h4>Bandits:</h4>
<p>Not in Python yet.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadCenter"></th><th class="markdownTableHeadCenter"><b>Models</b> </th><th class="markdownTableHeadCenter"></th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1Bandit_1_1Model.html" title="Learning to Coordinate with Coordination Graphs in Repeated Single-Stage Multi-Agent Decision Problems, Bargiacchi et al.">Basic Model</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1Bandit_1_1FlattenedModel.html" title="Learning to Coordinate with Coordination Graphs in Repeated Single-Stage Multi-Agent Decision Problems, Bargiacchi et al.">Flattened Model</a> </td><td class="markdownTableBodyCenter"></td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter"></td><td class="markdownTableBodyCenter"><b>Algorithms</b> </td><td class="markdownTableBodyCenter"></td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1Bandit_1_1MaxPlus.html" title="Collaborative Multiagent Reinforcement Learning by Payoff Propagation, Kok et al.">Max-Plus</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1Bandit_1_1MultiObjectiveVariableElimination.html" title="Multi-Objective Variable Elimination for Collaborative Graphical Games, Roijers et al.">Multi-Objective Variable Elimination (MOVE)</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1Bandit_1_1UCVE.html" title="Learning to Coordinate with Coordination Graphs in Repeated Single-Stage Multi-Agent Decision Problems, Bargiacchi et al.">Upper Confidence Variable Elimination (UCVE)</a>  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1Bandit_1_1VariableElimination.html" title="Multiagent Planning with Factored MDPs, Guestrin et al.">Variable Elimination</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1Bandit_1_1LocalSearch.html" title="Heuristic Coordination in Cooperative Multi-Agent Reinforcement Learning, Petri et al.">Local Search</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1Bandit_1_1ReusingIterativeLocalSearch.html" title="Heuristic Coordination in Cooperative Multi-Agent Reinforcement Learning, Petri et al.">Reusing Iterative Local Search</a>  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"></td><td class="markdownTableBodyCenter"><b>Policies</b> </td><td class="markdownTableBodyCenter"></td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1Bandit_1_1QGreedyPolicy.html">Q-Greedy Policy</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1Bandit_1_1RandomPolicy.html">Random Policy</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1Bandit_1_1LLRPolicy.html" title="Combinatorial Network Optimization with Unknown Variables: Multi-Armed Bandits with Linear Rewards, Gai et al.">Learning with Linear Rewards (LLR)</a>  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1Bandit_1_1MAUCEPolicy.html" title="Learning to Coordinate with Coordination Graphs in Repeated Single-Stage Multi-Agent Decision Problems, Bargiacchi et al.">Multi-Agent Upper Confidence Exploration (MAUCE)</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1Bandit_1_1ThompsonSamplingPolicy.html" title="Multi-Agent Thompson Sampling for Bandit Applications with Sparse Neighbourhood Structures, Verstraeten et al.">Multi-Agent Thompson-Sampling (Student-t distribution)</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1Bandit_1_1SingleActionPolicy.html">Single-Action Policy</a>  </td></tr>
</table>
<h4>MDP:</h4>
<p>Not in Python yet.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadCenter"></th><th class="markdownTableHeadCenter"><b>Models</b> </th><th class="markdownTableHeadCenter"></th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1MDP_1_1CooperativeModel.html">Cooperative Basic Model</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1MDP_1_1CooperativeMaximumLikelihoodModel.html">Cooperative Maximum Likelihood Model</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1MDP_1_1CooperativeThompsonModel.html">Cooperative Thompson Model (Dirichlet + Student-t distributions)</a>  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter"></td><td class="markdownTableBodyCenter"><b>Algorithms</b> </td><td class="markdownTableBodyCenter"></td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1MDP_1_1FactoredLP.html" title="Max-norm Projections for Factored MDPs, Guestrin et al.">FactoredLP</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1MDP_1_1LinearProgramming.html" title="Multiagent Planning with Factored MDPs, Guestrin et al.">Multi Agent Linear Programming</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1MDP_1_1JointActionLearner.html" title="The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems, Claus et al.">Joint Action Learners</a>  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1MDP_1_1SparseCooperativeQLearning.html" title="Sparse Cooperative Q-learning, Kok et al.">Sparse Cooperative Q-Learning</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1MDP_1_1CooperativePrioritizedSweeping.html" title="Model-based Multi-Agent Reinforcement Learning with Cooperative Prioritized Sweeping, Bargiacchi et al.">Cooperative Prioritized Sweeping</a> </td><td class="markdownTableBodyCenter"></td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"></td><td class="markdownTableBodyCenter"><b>Policies</b> </td><td class="markdownTableBodyCenter"></td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1MDP_1_1BanditPolicyAdaptor.html">All Bandit Policies</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1MDP_1_1EpsilonPolicy.html">Epsilon-Greedy Policy</a> </td><td class="markdownTableBodyCenter"><a href="http://svalorzen.github.io/AI-Toolbox/classAIToolbox_1_1Factored_1_1MDP_1_1QGreedyPolicy.html">Q-Greedy Policy</a>  </td></tr>
</table>
<h1>Build Instructions </h1>
<h2>Dependencies </h2>
<p>To build the library you need:</p>
<ul>
<li><a href="http://www.cmake.org/">cmake</a> &gt;= 3.12</li>
<li>the <a href="http://www.boost.org/">boost library</a> &gt;= 1.67</li>
<li>the <a href="http://eigen.tuxfamily.org/index.php?title=Main_Page">Eigen 3.3 library</a>.</li>
<li>the <a href="http://lpsolve.sourceforge.net/5.5/">lp_solve library</a> (a shared library must be available to compile the Python wrapper).</li>
</ul>
<p>In addition, C++20 support is now required (<b>this means at least g++-10</b>)</p>
<p>On a Ubuntu system, you can install these dependencies with the following command:</p>
<div class="fragment"><div class="line">sudo apt install g++-10 cmake libboost1.71-all-dev liblpsolve55-dev lp-solve libeigen3-dev</div>
</div><!-- fragment --><h2>Building </h2>
<p>Once you have all required dependencies, you can simply execute the following commands from the project's main folder:</p>
<div class="fragment"><div class="line">mkdir build</div>
<div class="line">cd build/</div>
<div class="line">cmake ..</div>
<div class="line">make</div>
</div><!-- fragment --><p><code>cmake</code> can be called with a series of flags in order to customize the output, if building everything is not desirable. The following flags are available:</p>
<div class="fragment"><div class="line">CMAKE_BUILD_TYPE   # Defines the build type</div>
<div class="line">MAKE_ALL           # Builds all there is to build in the project, but Python.</div>
<div class="line">MAKE_LIB           # Builds the whole core C++ libraries (MDP, POMDP, etc..)</div>
<div class="line">MAKE_MDP           # Builds only the core C++ MDP library</div>
<div class="line">MAKE_FMDP          # Builds only the core C++ Factored/Multi-Agent and MDP libraries</div>
<div class="line">MAKE_POMDP         # Builds only the core C++ POMDP and MDP libraries</div>
<div class="line">MAKE_TESTS         # Builds the library&#39;s tests for the compiled core libraries</div>
<div class="line">MAKE_EXAMPLES      # Builds the library&#39;s examples using the compiled core libraries</div>
<div class="line">MAKE_PYTHON        # Builds Python bindings for the compiled core libraries</div>
<div class="line">AI_PYTHON_VERSION  # Selects the Python version you want (2 or 3). If not</div>
<div class="line">                   #   specified, we try to guess based on your default interpreter.</div>
<div class="line">AI_LOGGING_ENABLED # Whether the library logging code is enabled at runtime.</div>
</div><!-- fragment --><p>These flags can be combined as needed. For example:</p>
<div class="fragment"><div class="line"># Will build MDP and MDP Python 3 bindings</div>
<div class="line">cmake -DCMAKE_BUILD_TYPE=Debug -DMAKE_MDP=1 -DMAKE_PYTHON=1 -DAI_PYTHON_VERSION=3 ..</div>
</div><!-- fragment --><p>The default flags when nothing is specified are <code>MAKE_ALL</code> and <code>CMAKE_BUILD_TYPE=Release</code>.</p>
<p>Note that by default <code>MAKE_ALL</code> does not build the Python bindings, as they have a minor performance hit on the C++ static libraries. You can easily enable them by using the flag <code>MAKE_PYTHON</code>.</p>
<p>The static library files will be available directly in the build directory. Three separate libraries are built: <code>AIToolboxMDP</code>, <code>AIToolboxPOMDP</code> and <code>AIToolboxFMDP</code>. In case you want to link against either the POMDP library or the Factored MDP library, you will also need to link against the MDP one, since both of them use MDP functionality.</p>
<p>A number of small tests are included which you can find in the <code>test/</code> folder. You can execute them after building the project using the following command directly from the <code>build</code> directory, just after you finish <code>make</code>:</p>
<div class="fragment"><div class="line">ctest</div>
</div><!-- fragment --><p>The tests also offer a brief introduction for the framework, waiting for a more complete descriptive write-up. Only the tests for the parts of the library that you compiled are going to be built.</p>
<p>To compile the library's documentation you need <a href="http://www.doxygen.nl/">Doxygen</a>. To use it it is sufficient to execute the following command from the project's root folder:</p>
<div class="fragment"><div class="line">doxygen</div>
</div><!-- fragment --><p>After that the documentation will be generated into an <code>html</code> folder in the main directory.</p>
<h1>Compiling a Program </h1>
<p>To compile a program that uses this library, simply link it against the compiled libraries you need, and possibly to the <code>lp_solve</code> libraries (if using POMDP or FMDP).</p>
<p>Please note that since both POMDP and FMDP libraries rely on the MDP code, you <b>MUST</b> specify those libraries <em>before</em> the MDP library when linking, otherwise it may result in <code>undefined reference</code> errors. The POMDP and Factored MDP libraries are not currently dependent on each other so their order does not matter.</p>
<p>For Python, you just need to import the <code>AIToolbox.so</code> module, and you'll be able to use the classes as exported to Python. All classes are documented, and you can run in the Python CLI </p><pre class="fragment">help(AIToolbox.MDP)
help(AIToolbox.POMDP)
</pre><p>to see the documentation for each specific class. </p>
</div></div><!-- PageDoc -->
</div><!-- contents -->
</div><!-- doc-content -->
<div class="ttc" id="anamespaceAIToolbox_1_1POMDP_html_abcd38fafda125d7a1e44a0d04c56e0e0"><div class="ttname"><a href="namespaceAIToolbox_1_1POMDP.html#abcd38fafda125d7a1e44a0d04c56e0e0">AIToolbox::POMDP::makeTigerProblem</a></div><div class="ttdeci">AIToolbox::POMDP::Model&lt; AIToolbox::MDP::Model &gt; makeTigerProblem()</div><div class="ttdoc">This function sets up the tiger problem in a Model.</div><div class="ttdef"><b>Definition:</b> TigerProblem.hpp:58</div></div>
<div class="ttc" id="anamespaceAIToolbox_html_ad73352f6ce65aa95f76b7399cf0d2cb0"><div class="ttname"><a href="namespaceAIToolbox.html#ad73352f6ce65aa95f76b7399cf0d2cb0">AIToolbox::sampleProbability</a></div><div class="ttdeci">size_t sampleProbability(const size_t d, const T &amp;in, G &amp;generator)</div><div class="ttdoc">This function samples an index from a probability vector.</div><div class="ttdef"><b>Definition:</b> Probability.hpp:188</div></div>
<div class="ttc" id="aclassAIToolbox_1_1POMDP_1_1Policy_html"><div class="ttname"><a href="classAIToolbox_1_1POMDP_1_1Policy.html">AIToolbox::POMDP::Policy</a></div><div class="ttdoc">This class represents a POMDP Policy.</div><div class="ttdef"><b>Definition:</b> Policy.hpp:26</div></div>
<div class="ttc" id="aclassAIToolbox_1_1POMDP_1_1IncrementalPruning_html"><div class="ttname"><a href="classAIToolbox_1_1POMDP_1_1IncrementalPruning.html">AIToolbox::POMDP::IncrementalPruning</a></div><div class="ttdoc">This class implements the Incremental Pruning algorithm.</div><div class="ttdef"><b>Definition:</b> IncrementalPruning.hpp:44</div></div>
<div class="ttc" id="anamespaceAIToolbox_1_1POMDP_html_a4522c5e35483fb30b2c43c271781e8bc"><div class="ttname"><a href="namespaceAIToolbox_1_1POMDP.html#a4522c5e35483fb30b2c43c271781e8bc">AIToolbox::POMDP::Belief</a></div><div class="ttdeci">ProbabilityVector Belief</div><div class="ttdoc">This represents a belief, which is a probability distribution over states.</div><div class="ttdef"><b>Definition:</b> Types.hpp:12</div></div>
<!-- HTML footer for doxygen 1.8.17-->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
</div>
</body>
</html>
