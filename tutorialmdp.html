<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>AIToolbox: MDP Beginner Tutorial</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">AIToolbox
   </div>
   <div id="projectbrief">A library that offers tools for AI problem solving.</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('tutorialmdp.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">MDP Beginner Tutorial </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This tutorial is meant to give you information about this library and the theoretical foundations behind it. It is not meant to be a full-on class about it, but I hope it will be enough to get you started.</p>
<p>This tutorial's code can be found in the <code>examples/MDP/tiger_antelope.cpp</code> file, including comments and additional nodes.</p>
<h1>Markov Decision Process </h1>
<p>A Markov Decision Process, or MDP in short, is a way to model a decision making process. Here we will focus on single agent decision making processes, since they are the most simple ones.</p>
<p>Let's start with some definitions:</p>
<ul>
<li>All starts from the <b>environment</b>. The environment is the setting of our decision making process. The environment changes over discrete timesteps; at each timestep the environment is in one, single, unique <b>state</b>. At each timestep it is said that the environment <b>transitions</b> from one state to another.</li>
<li>In MDPs, it is said that states need to be <b>markovian</b>. What this means is that a state <b>must</b> encode all the information necessary for the decision making process, without leaving anything aside. More precisely, the decision process must not need to look at the past in order to be effective. All information contained in the past history of the environment must be readily available from the current state. For example, time-dependent contingencies are not allowed. If a problem contains them, then timer-like counters must be embedded in the state for the modeling to work, so that a state always contain all the information needed and the past is not relevant for decisions.</li>
<li>The way the environment transitions from a state to another is not necessarily deterministic. From a particular state, the environment can transition to any other state following a certain probability distribution (which CAN be deterministic).</li>
<li>Our goal within the decision making process is to influence the way the environment transitions between states. In some sense, we prefer some states more than others, and thus we would like the environment to be in some state rather than another. We will encode that preference using <b>rewards</b>.</li>
<li>The <b>agent</b> is the entity that is taking the decisions. It is not necessarily corporeal, nor does it actually have to be inside the environment; most times it can be useful to visualize it in such terms though (a unit moving through an environment, for example).</li>
<li>The agent interacts with the environment through <b>actions</b>. An action modifies the way in which the environment transitions from a state to another. Thus, at each timestep, the agent needs to select the action which will maximize its obtained reward.</li>
<li>An agent interacts with the environment using a <b>policy</b>. The policy is what tells the agent what actions to take in each state. A policy can be deterministic or stochastic.</li>
<li>The agent is able to interact with the environment during a certain amount of timesteps. This is called the <b>horizon</b>. The horizon can be either finite, meaning that the agent will stop receiving rewards after a certain number of timesteps, and thus should think only for those, or infinite. In this last case the agent will keep interacting with the environment forever, and should thus plan accordingly.</li>
<li>The last thing we need to define is the <b>discount</b>. The agent tries to maximize the sum of all rewards obtained over time. The discount is a number between 0 and 1, which determines how much rewards obtained in future timesteps affect the agent's decisions, in a geometric progression, so that we try to maximize <img class="formulaInl" alt="$\sum_{t=0}^{T} \gamma^t r_t$" src="form_0.png"/>. A discount of 0 will make the agent greedy as all terms but for the first one become zero. The agent will then take actions that maximize the reward obtained in the next timestep, and nothing else. A discount of 1 will make the agent lazy, as a reward obtained new will be "worth" the same as a reward obtained in the future, so it will delay reward-obtaining actions possibly indefinitely.</li>
</ul>
<p>Armed with our new vocabulary, we can now define an MDP more formally. An MDP is a tuple &lt;<em>S</em>, <em>A</em>, <em>T</em>, <em>R</em>, <em>d</em>&gt;, where:</p>
<ul>
<li><em>S</em> is a set of states. This is basically a list of all possible states the environment can ever be in.</li>
<li><em>A</em> is a set of actions. This is a list of all the actions that an agent can take. Normally, in an MDP setting, we assume that the agent can select any actions all the time; as in, there are no states where some action is blocked.</li>
<li><em>T</em> is a <b>transition function</b>. This describes the way that the environment is allowed to evolve; in other words, the dynamics of the environment. It specifies for any triple &lt;<em>s</em>, <em>a</em>, <em>s'</em>&gt; the probability that the environment will transition from <em>s</em> to <em>s'</em>, if the agent selects action <em>a</em>.</li>
<li><em>R</em> is a <b>reward function</b>. Its shape is the same as <em>T</em>, and it contains the arbitrary rewards (positive or negative) that the agent will obtain depending on how the environment transitions. The reward function specifies for any triple &lt;<em>s</em>, <em>a</em>, <em>s'</em>&gt; the reward that the agent will obtain.</li>
<li>d is the discount factor, which we discussed above.</li>
</ul>
<h2>MDP Example</h2>
<p>Let's try to create an MDP to show how it would work out in practice. Our problem is going to be the following:</p>
<blockquote class="doxtable">
<p>Suppose you have a grid shaped world, of 11 cells by 11 cells. The world loops on itself like a torus, so that the top and bottom borders are connected with each other, and the same is true for the left and right borders. Two creatures walk this world: a tiger, and an antelope. Both creatures can move in the following way: up, down, left or right or stand still. When they decide to move, their movement is deterministic. The two creatures have different goals.</p>
<p>The goal of the antelope is to not get eaten by the tiger. However it is pretty clueless in doing so; in fact, it always moves or stands randomly, aside from when the tiger is directly next to it. In that case, it will move randomly anywhere, but towards the tiger.</p>
<p>The tiger has the goal of catching the antelope. Once it catches it, the game ends. What would be the best way for it to move? </p>
</blockquote>
<p>Let's think about how to encode this world into an MDP. We will go through each component of the MDP and try to fill it out.</p>
<h3>State Space (S)</h3>
<p>There seem to be no time dependent components, so that makes it easier for us to create the states. In this case a naive approach would be to use the current coordinates of both the tiger and the antelope as our state. Each pair of coordinate is unique and encodes all the information the tiger needs to act.</p>
<p>We might as well add some code in order to determine the distance between two coordinates, since we will need this later.</p>
<div class="fragment"><div class="line">constexpr <span class="keywordtype">int</span> SQUARE_SIZE = 11;</div>
<div class="line"> </div>
<div class="line"><span class="keyword">using</span> CoordType = std::array&lt;int, 4&gt;;</div>
<div class="line"><span class="keyword">enum</span> {</div>
<div class="line">    TIGER_X = 0,</div>
<div class="line">    TIGER_Y = 1,</div>
<div class="line">    ANTEL_X = 2,</div>
<div class="line">    ANTEL_Y = 3</div>
<div class="line">};</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Returns distance between coordinates. It is consistent with</span></div>
<div class="line"><span class="comment">// the wraparound world.</span></div>
<div class="line"><span class="keywordtype">int</span> wrapDiff( <span class="keywordtype">int</span> coord1, <span class="keywordtype">int</span> coord2 ) {</div>
<div class="line">    <span class="keywordtype">int</span> diff = coord2 - coord1;</div>
<div class="line"> </div>
<div class="line">    <span class="keywordtype">int</span> distance1 = std::abs( diff ), distance2 = SQUARE_SIZE - distance1;</div>
<div class="line">    <span class="keywordflow">if</span> ( distance1 &lt; distance2 ) <span class="keywordflow">return</span> diff;</div>
<div class="line">    <span class="keywordflow">return</span> diff &gt; 0 ? -distance2 : distance2;</div>
<div class="line">}</div>
</div><!-- fragment --><h3>Action Space (A)</h3>
<p>The tiger can move, and possibly stand still. Thus, it has 5 actions.</p>
<div class="fragment"><div class="line"><span class="keywordtype">size_t</span> A = 5;</div>
<div class="line"><span class="keyword">enum</span> {</div>
<div class="line">    <a class="code" href="namespaceAIToolbox_1_1MDP_1_1GridWorldEnums.html#ac819771000de8a5294554c3fd6e52103aa475cb272ee2c945091081a1da0db0ad">UP</a>    = 0,</div>
<div class="line">    <a class="code" href="namespaceAIToolbox_1_1MDP_1_1GridWorldEnums.html#ac819771000de8a5294554c3fd6e52103a34d343e1e0c2f12bb0ab4ccf99f51452">DOWN</a>  = 1,</div>
<div class="line">    <a class="code" href="namespaceAIToolbox_1_1MDP_1_1GridWorldEnums.html#ac819771000de8a5294554c3fd6e52103a744255c728603b95b2ed85394759b62d">LEFT</a>  = 2,</div>
<div class="line">    <a class="code" href="namespaceAIToolbox_1_1MDP_1_1GridWorldEnums.html#ac819771000de8a5294554c3fd6e52103a5308cc7f485f1749b02762d5638f1de2">RIGHT</a> = 3,</div>
<div class="line">    STAND = 4</div>
<div class="line">};</div>
</div><!-- fragment --><h3>Transition Function (T)</h3>
<p>Transition functions are generally the most time consuming part of defining an MDP, and where it's easier to make mistakes. The transition function is needed since, in general, exact MDP solving methods rely on it to find out the best policy for the problem. You do not necessarily need to manually create a table containing all of them, as long as you can compute them on the fly. This however can become <em>very</em> expensive computationally depending on the method you use; how much of the transition function you want to cache is your decision to make.</p>
<p>In this function we specify the probability of ending up in a certain tiger-antelope state, given that the tiger has taken a certain action from a certain initial state.</p>
<div class="fragment"><div class="line"><span class="keywordtype">double</span> getTransitionProbability( <span class="keyword">const</span> CoordType &amp; c1, <span class="keywordtype">size_t</span> action, <span class="keyword">const</span> CoordType &amp; c2 ) {</div>
<div class="line">    <span class="comment">// We compute the distances traveled by both the antelope and the tiger.</span></div>
<div class="line">    <span class="keywordtype">int</span> tigerMovementX = wrapDiff( c1[TIGER_X], c2[TIGER_X] );</div>
<div class="line">    <span class="keywordtype">int</span> tigerMovementY = wrapDiff( c1[TIGER_Y], c2[TIGER_Y] );</div>
<div class="line">    <span class="keywordtype">int</span> antelMovementX = wrapDiff( c1[ANTEL_X], c2[ANTEL_X] );</div>
<div class="line">    <span class="keywordtype">int</span> antelMovementY = wrapDiff( c1[ANTEL_Y], c2[ANTEL_Y] );</div>
<div class="line"> </div>
<div class="line">    <span class="comment">// Both the tiger and the antelope can only move by 1 cell max at each</span></div>
<div class="line">    <span class="comment">// timestep. Thus, if this is not the case, the transition is</span></div>
<div class="line">    <span class="comment">// impossible.</span></div>
<div class="line">    <span class="keywordflow">if</span> ( std::abs( tigerMovementX ) +</div>
<div class="line">         std::abs( tigerMovementY ) &gt; 1 ) <span class="keywordflow">return</span> 0.0;</div>
<div class="line"> </div>
<div class="line">    <span class="keywordflow">if</span> ( std::abs( antelMovementX ) +</div>
<div class="line">         std::abs( antelMovementY ) &gt; 1 ) <span class="keywordflow">return</span> 0.0;</div>
<div class="line"> </div>
<div class="line">    <span class="comment">// The tiger can move only in the direction specified by its action. If</span></div>
<div class="line">    <span class="comment">// it is not the case, the transition is impossible.</span></div>
<div class="line">    <span class="keywordflow">if</span> ( action == STAND &amp;&amp; ( tigerMovementX || tigerMovementY ) ) <span class="keywordflow">return</span> 0.0;</div>
<div class="line">    <span class="keywordflow">if</span> ( action == <a class="code" href="namespaceAIToolbox_1_1MDP_1_1GridWorldEnums.html#ac819771000de8a5294554c3fd6e52103aa475cb272ee2c945091081a1da0db0ad">UP</a>    &amp;&amp; tigerMovementY != 1  ) <span class="keywordflow">return</span> 0.0;</div>
<div class="line">    <span class="keywordflow">if</span> ( action == <a class="code" href="namespaceAIToolbox_1_1MDP_1_1GridWorldEnums.html#ac819771000de8a5294554c3fd6e52103a34d343e1e0c2f12bb0ab4ccf99f51452">DOWN</a>  &amp;&amp; tigerMovementY != -1 ) <span class="keywordflow">return</span> 0.0;</div>
<div class="line">    <span class="keywordflow">if</span> ( action == <a class="code" href="namespaceAIToolbox_1_1MDP_1_1GridWorldEnums.html#ac819771000de8a5294554c3fd6e52103a744255c728603b95b2ed85394759b62d">LEFT</a>  &amp;&amp; tigerMovementX != -1 ) <span class="keywordflow">return</span> 0.0;</div>
<div class="line">    <span class="keywordflow">if</span> ( action == <a class="code" href="namespaceAIToolbox_1_1MDP_1_1GridWorldEnums.html#ac819771000de8a5294554c3fd6e52103a5308cc7f485f1749b02762d5638f1de2">RIGHT</a> &amp;&amp; tigerMovementX != 1  ) <span class="keywordflow">return</span> 0.0;</div>
<div class="line"> </div>
<div class="line">    <span class="comment">// Now we check whether the tiger was next to the antelope or not</span></div>
<div class="line">    <span class="keywordtype">int</span> diffX = wrapDiff( c1[TIGER_X], c1[ANTEL_X] );</div>
<div class="line">    <span class="keywordtype">int</span> diffY = wrapDiff( c1[TIGER_Y], c1[ANTEL_Y] );</div>
<div class="line"> </div>
<div class="line">    <span class="comment">// If they were not adjacent, then the probability for any move of the</span></div>
<div class="line">    <span class="comment">// antelope is simply 1/5: it behaves randomly.</span></div>
<div class="line">    <span class="keywordflow">if</span> ( std::abs( diffX ) + std::abs( diffY ) &gt; 1 ) <span class="keywordflow">return</span> 1.0 / 5.0;</div>
<div class="line"> </div>
<div class="line">    <span class="comment">// Otherwise, first we check that the move was allowed, as</span></div>
<div class="line">    <span class="comment">// the antelope cannot move where the tiger was before.</span></div>
<div class="line">    <span class="keywordflow">if</span> ( c1[TIGER_X] == c2[ANTEL_X] &amp;&amp; c1[TIGER_Y] == c2[ANTEL_Y] ) <span class="keywordflow">return</span> 0.0;</div>
<div class="line"> </div>
<div class="line">    <span class="comment">// The last test is to check whether they were both in the same cell before.</span></div>
<div class="line">    <span class="comment">// In that case the game would have ended, and nothing would happen anymore.</span></div>
<div class="line">    <span class="comment">// We model this as a self-absorbing state, or a state that always transitions</span></div>
<div class="line">    <span class="comment">// to itself.</span></div>
<div class="line">    <span class="keywordflow">if</span> ( diffX + diffY == 0 ) {</div>
<div class="line">        <span class="keywordflow">if</span> ( c1 == c2 ) <span class="keywordflow">return</span> 1.0;</div>
<div class="line">        <span class="keywordflow">else</span> <span class="keywordflow">return</span> 0.0;</div>
<div class="line">    }</div>
<div class="line"> </div>
<div class="line">    <span class="comment">// Else the probability of this transition is 1 / 4, still random but without</span></div>
<div class="line">    <span class="comment">// a possible antelope action.</span></div>
<div class="line">    <span class="keywordflow">return</span> 1.0 / 4.0;</div>
<div class="line">}</div>
</div><!-- fragment --><h3>Reward Function (R)</h3>
<p>Here we define the reward function. Fortunately for us, this can be easily done: we will reward the tiger when it catches the antelope with a reward of 10, and otherwise it will get no reward. In this particular example the amount of reward the tiger obtains does not really matter, as long as it is positive, but 10 looks nice. When the agent has multiple ways to obtain reward, their relative magnitude starts to get an important role in the final policy of the agent!</p>
<div class="fragment"><div class="line"><span class="keywordtype">double</span> getReward( <span class="keyword">const</span> CoordType &amp; c ) {</div>
<div class="line">    <span class="keywordflow">if</span> ( c[TIGER_X] == c[ANTEL_X] &amp;&amp; c[TIGER_Y] == c[ANTEL_Y] ) <span class="keywordflow">return</span> 10.0;</div>
<div class="line">    <span class="keywordflow">return</span> 0.0;</div>
<div class="line">}</div>
</div><!-- fragment --><h3>Discount</h3>
<p>We want the tiger to catch the antelope. We want it to catch it no matter how far it is, and at the same time we do not want it to wait forever to catch it. To model this, a high discount value makes sense. A low discount value tends to rapidly devalue reward far in the future, and it can happen, if the tiger takes too much, that floating point errors will eat the reward before the solving method can compute the best policy. A high discount value avoids this problem; however, the time that the method takes to converge to the best solution increases as the number of timesteps it must look in the future increases.</p>
<div class="fragment"><div class="line">constexpr <span class="keywordtype">double</span> discount = 0.9;</div>
</div><!-- fragment --><h2>The MDP Model</h2>
<p>One more thing needs to be addressed. Every problem has in general a different type of state. In this case, it is a set of coordinates, but depending on your specific setting, it might be significantly different. This is problematic, because there's no simple way, for example, to iterate over custom states. Since states are unique, this library abstracts over this problem, and requires states to be integers. Thus, we write some code to automatically convert our states into integers:</p>
<div class="fragment"><div class="line"><span class="keywordtype">size_t</span> encodeState(<span class="keyword">const</span> CoordType &amp; coords) {</div>
<div class="line">    <span class="keywordtype">size_t</span> state = 0; <span class="keywordtype">unsigned</span> multiplier = 1;</div>
<div class="line">    <span class="keywordflow">for</span> ( <span class="keyword">auto</span> c : coords ) {</div>
<div class="line">        state += multiplier * c;</div>
<div class="line">        multiplier *= SQUARE_SIZE;</div>
<div class="line">    }</div>
<div class="line">    <span class="keywordflow">return</span> state;</div>
<div class="line">}</div>
<div class="line"> </div>
<div class="line">CoordType decodeState(<span class="keywordtype">size_t</span> state) {</div>
<div class="line">    CoordType coords;</div>
<div class="line">    <span class="keywordflow">for</span> ( <span class="keyword">auto</span> &amp; c : coords ) {</div>
<div class="line">        c = state % SQUARE_SIZE;</div>
<div class="line">        state /= SQUARE_SIZE;</div>
<div class="line">    }</div>
<div class="line">    <span class="keywordflow">return</span> coords;</div>
<div class="line">}</div>
</div><!-- fragment --><p>Finally, we are going to wrap all the functions we have written into a single class with an interface that <a class="el" href="namespaceAIToolbox.html">AIToolbox</a> can recognize. Here is our final wrapper:</p>
<div class="fragment"><div class="line"><span class="keyword">class </span>GridWorld {</div>
<div class="line">    <span class="keyword">public</span>:</div>
<div class="line">        <span class="comment">// The number of possible states of our model is equal to all the</span></div>
<div class="line">        <span class="comment">// possible coordinates that the antelope and the tiger could have.</span></div>
<div class="line">        <span class="keywordtype">size_t</span> getS()<span class="keyword"> const </span>{ <span class="keywordflow">return</span> SQUARE_SIZE * SQUARE_SIZE * SQUARE_SIZE * SQUARE_SIZE; }</div>
<div class="line">        <span class="comment">// This function returns the number of available actions to the agent</span></div>
<div class="line">        <span class="keywordtype">size_t</span> getA()<span class="keyword"> const </span>{ return ::A; }</div>
<div class="line">        <span class="comment">// This one gets the discount of the model</span></div>
<div class="line">        <span class="keywordtype">double</span> getDiscount()<span class="keyword"> const </span>{ return ::discount; }</div>
<div class="line"> </div>
<div class="line">        <span class="keywordtype">double</span> getTransitionProbability( <span class="keywordtype">size_t</span> s, <span class="keywordtype">size_t</span> a, <span class="keywordtype">size_t</span> s1 )<span class="keyword"> const </span>{</div>
<div class="line">            return ::getTransitionProbability( decodeState( s ), a, decodeState( s1 ) );</div>
<div class="line">        }</div>
<div class="line"> </div>
<div class="line">        <span class="comment">// This function needs to take parameters as the transition one,</span></div>
<div class="line">        <span class="comment">// but we are lucky and our model only needs one of them to know</span></div>
<div class="line">        <span class="comment">// the reward!</span></div>
<div class="line">        <span class="keywordtype">double</span> getExpectedReward( <span class="keywordtype">size_t</span>, <span class="keywordtype">size_t</span>, <span class="keywordtype">size_t</span> s1 )<span class="keyword"> const </span>{</div>
<div class="line">            return ::getReward( decodeState( s1 ) );</div>
<div class="line">        }</div>
<div class="line"> </div>
<div class="line">        <span class="comment">// These two functions are needed to keep template code in the library</span></div>
<div class="line">        <span class="comment">// simple, but you don&#39;t need to implement them for the method we use</span></div>
<div class="line">        <span class="comment">// in this example. Look into AIToolbox::MDP::is_generative_model and</span></div>
<div class="line">        <span class="comment">// AIToolbox::MDP::is_model to know more about this.</span></div>
<div class="line">        std::tuple&lt;size_t, double&gt; sampleSR(<span class="keywordtype">size_t</span>,<span class="keywordtype">size_t</span>) <span class="keyword">const</span>;</div>
<div class="line">        <span class="keywordtype">bool</span> isTerminal(<span class="keywordtype">size_t</span>) <span class="keyword">const</span>;</div>
<div class="line">};</div>
</div><!-- fragment --><p>Voilà! All that is needed now is simply some <a class="el" href="namespaceAIToolbox.html">AIToolbox</a> magic!</p>
<h3>The Actual Planning Code</h3>
<p>We use the <code><a class="el" href="classAIToolbox_1_1MDP_1_1ValueIteration.html" title="This class applies the value iteration algorithm on a Model.">AIToolbox::MDP::ValueIteration</a></code> algorithm in order to solve the MDP and obtain the solution (also called value function). From it, it's possible to create a policy which will dictate how the tiger will move in the environment to catch the antelope.</p>
<div class="fragment"><div class="line"><span class="keywordtype">int</span> main() {</div>
<div class="line">    GridWorld world;</div>
<div class="line"> </div>
<div class="line">    <span class="comment">// This is a method that solves MDPs completely. It has a couple of</span></div>
<div class="line">    <span class="comment">// parameters available, but in our case the defaults are perfectly</span></div>
<div class="line">    <span class="comment">// fine.</span></div>
<div class="line">    <a class="code" href="classAIToolbox_1_1MDP_1_1ValueIteration.html">AIToolbox::MDP::ValueIteration</a> solver;</div>
<div class="line"> </div>
<div class="line">    std::cout &lt;&lt; <span class="stringliteral">&quot;Starting solver!\n&quot;</span>;</div>
<div class="line">    <span class="comment">// This is where the magic happens. This could take around 10-20 minutes,</span></div>
<div class="line">    <span class="comment">// depending on your machine (most of the time is spent recomputing</span></div>
<div class="line">    <span class="comment">// transition probabilities, however, since we don&#39;t cache them here.</span></div>
<div class="line">    <span class="comment">// But you can play with the code and make it better!).</span></div>
<div class="line">    <span class="keyword">auto</span> solution = solver(world);</div>
<div class="line"> </div>
<div class="line">    std::cout &lt;&lt; <span class="stringliteral">&quot;Problem solved? &quot;</span> &lt;&lt; std::get&lt;0&gt;(solution) &lt;&lt; <span class="stringliteral">&quot;\n&quot;</span>;</div>
<div class="line"> </div>
<div class="line">    <a class="code" href="classAIToolbox_1_1MDP_1_1Policy.html">AIToolbox::MDP::Policy</a> policy(world.getS(), world.getA(), std::get&lt;1&gt;(solution));</div>
<div class="line"> </div>
<div class="line">    std::cout &lt;&lt; <span class="stringliteral">&quot;Printing best actions when prey is in (5,5):\n\n&quot;</span>;</div>
<div class="line">    <span class="keywordflow">for</span> ( <span class="keywordtype">int</span> y = 10; y &gt;= 0; --y ) {</div>
<div class="line">        <span class="keywordflow">for</span> ( <span class="keywordtype">int</span> x = 0; x &lt; 11; ++x ) {</div>
<div class="line">            std::cout &lt;&lt; policy.sampleAction( encodeState(CoordType{{x, y, 5, 5}}) ) &lt;&lt; <span class="stringliteral">&quot; &quot;</span>;</div>
<div class="line">        }</div>
<div class="line">        std::cout &lt;&lt; <span class="stringliteral">&quot;\n&quot;</span>;</div>
<div class="line">    }</div>
<div class="line"> </div>
<div class="line">    std::cout &lt;&lt; <span class="stringliteral">&quot;\nSaving policy to file for later usage...\n&quot;</span>;</div>
<div class="line">    {</div>
<div class="line">        <span class="comment">// You can load up this policy again using ifstreams.</span></div>
<div class="line">        <span class="comment">// You will not need to solve the model ever again, and you</span></div>
<div class="line">        <span class="comment">// can embed the policy into any application you want!</span></div>
<div class="line">        std::ofstream output(<span class="stringliteral">&quot;policy.txt&quot;</span>);</div>
<div class="line">        output &lt;&lt; policy;</div>
<div class="line">    }</div>
<div class="line"> </div>
<div class="line">    <span class="keywordflow">return</span> 0;</div>
<div class="line">}</div>
</div><!-- fragment --><p>The full code of this example can be found in the <code>examples/MDP/tiger_antelope.cpp</code> file, and is built automatically by adding <code>-DMAKE_EXAMPLES=1</code> when running CMake.</p>
<h2>Conclusions</h2>
<p>The code we saw was a very inefficient implementation, for a number of reasons. First, the particular method we used needs to repeatedly look up the transition probabilities of the MDP model. In our implementation, these need to be recomputed almost constantly. A better way would be to save them up into a single transition matrix once, and simply return the values of the table when asked. <a class="el" href="namespaceAIToolbox.html">AIToolbox</a> already does this for you in the <a class="el" href="classAIToolbox_1_1MDP_1_1Model.html" title="This class represents a Markov Decision Process.">AIToolbox::MDP::Model</a> class. This is done in the full example code, so go check it out!</p>
<p>In addition, our state space was way bigger than what was actually needed. This is because the problem in question has a very high symmetry. For example, it does not actually matter where the antelope is, since we could simply translate both the antelope and the tiger until the antelope is at coordinates 5,5. We can do this because the world is toroidal. This change allows us to remove the coordinates of the antelope from the state, greatly reducing the size of <em>S</em>.</p>
<p>Another thing is that the world is symmetrical, both vertically, horizontally and diagonally. Thus we could rewrite the transition function and the model so that only an eighth of the states were needed. Combined with the translational symmetry, this would enormously reduce the time needed to solve the MDP.</p>
<p>I hope this tutorial helped you understand the main concepts around MDPs, so that you can start to play around with the library by yourself. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<div class="ttc" id="anamespaceAIToolbox_1_1MDP_1_1GridWorldEnums_html_ac819771000de8a5294554c3fd6e52103a744255c728603b95b2ed85394759b62d"><div class="ttname"><a href="namespaceAIToolbox_1_1MDP_1_1GridWorldEnums.html#ac819771000de8a5294554c3fd6e52103a744255c728603b95b2ed85394759b62d">AIToolbox::MDP::GridWorldEnums::LEFT</a></div><div class="ttdeci">@ LEFT</div><div class="ttdef"><b>Definition:</b> GridWorld.hpp:15</div></div>
<div class="ttc" id="aclassAIToolbox_1_1MDP_1_1Policy_html"><div class="ttname"><a href="classAIToolbox_1_1MDP_1_1Policy.html">AIToolbox::MDP::Policy</a></div><div class="ttdoc">This class represents an MDP Policy.</div><div class="ttdef"><b>Definition:</b> Policy.hpp:28</div></div>
<div class="ttc" id="anamespaceAIToolbox_1_1MDP_1_1GridWorldEnums_html_ac819771000de8a5294554c3fd6e52103aa475cb272ee2c945091081a1da0db0ad"><div class="ttname"><a href="namespaceAIToolbox_1_1MDP_1_1GridWorldEnums.html#ac819771000de8a5294554c3fd6e52103aa475cb272ee2c945091081a1da0db0ad">AIToolbox::MDP::GridWorldEnums::UP</a></div><div class="ttdeci">@ UP</div><div class="ttdef"><b>Definition:</b> GridWorld.hpp:15</div></div>
<div class="ttc" id="anamespaceAIToolbox_1_1MDP_1_1GridWorldEnums_html_ac819771000de8a5294554c3fd6e52103a5308cc7f485f1749b02762d5638f1de2"><div class="ttname"><a href="namespaceAIToolbox_1_1MDP_1_1GridWorldEnums.html#ac819771000de8a5294554c3fd6e52103a5308cc7f485f1749b02762d5638f1de2">AIToolbox::MDP::GridWorldEnums::RIGHT</a></div><div class="ttdeci">@ RIGHT</div><div class="ttdef"><b>Definition:</b> GridWorld.hpp:15</div></div>
<div class="ttc" id="aclassAIToolbox_1_1MDP_1_1ValueIteration_html"><div class="ttname"><a href="classAIToolbox_1_1MDP_1_1ValueIteration.html">AIToolbox::MDP::ValueIteration</a></div><div class="ttdoc">This class applies the value iteration algorithm on a Model.</div><div class="ttdef"><b>Definition:</b> ValueIteration.hpp:27</div></div>
<div class="ttc" id="anamespaceAIToolbox_1_1MDP_1_1GridWorldEnums_html_ac819771000de8a5294554c3fd6e52103a34d343e1e0c2f12bb0ab4ccf99f51452"><div class="ttname"><a href="namespaceAIToolbox_1_1MDP_1_1GridWorldEnums.html#ac819771000de8a5294554c3fd6e52103a34d343e1e0c2f12bb0ab4ccf99f51452">AIToolbox::MDP::GridWorldEnums::DOWN</a></div><div class="ttdeci">@ DOWN</div><div class="ttdef"><b>Definition:</b> GridWorld.hpp:15</div></div>
<!-- HTML footer for doxygen 1.8.17-->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
</div>
</body>
</html>
