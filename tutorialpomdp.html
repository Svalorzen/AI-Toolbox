<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>AIToolbox: POMDP Beginner Tutorial</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">AIToolbox
   </div>
   <div id="projectbrief">A library that offers tools for AI problem solving.</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('tutorialpomdp.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">POMDP Beginner Tutorial </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This tutorial assumes you have read and understood the MDP tutorial.</p>
<p>This tutorial's code can be found in the <code>examples/POMDP/tiger_door.cpp</code> file, including comments and additional nodes.</p>
<p>Another good tutorial on POMDPs available online can be found <a href="http://pomdp.org/tutorial/">here</a>.</p>
<h1>Partially Observable Markov Decision Process </h1>
<p>A Partially Observable Markov Decision Process, or POMDP in short, is a generalization of the ideas of an MDP to environments where the agents does not directly know the current state of the underlying environment. It simply has no access to it.</p>
<p>Instead, after performing each action, the agent receives an <b>observation</b> which helps it restrict the range of the possible current states. The observation is obtained from the environment following an <b>observation function</b>, which dictates the probability of obtaining a certain observation given a certain state and action.</p>
<p>Indeed, a POMDP is defined as a tuple &lt;<em>S</em>, <em>A</em>, <em>O</em>, <em>T</em>, <em>R</em>, <em>W</em>, <em>d</em>&gt;, where:</p>
<ul>
<li>All previous MDP symbols maintain the same meaning.</li>
<li><em>O</em> is the set of possible observations the agent can receive.</li>
<li><em>W</em> is the observation function.</li>
</ul>
<p>The agent thus, at any timestep, maintains a <b>belief</b> of which states the environment is actually in: some will be more likely, and some less. The belief is then a discrete probability distribution over all states, indicating what the agent thinks likely.</p>
<p>In POMDP planning this belief can be maintained over time, as the agent knows the POMDP dynamics. Thus, given his previous belief and knowing which action has been performed and what observation was received, the agent can compute a new belief, with probabilities for every possible state of the environment.</p>
<p>Given that this is a complicated process, POMDP policies are usually in the shape of a tree. Depending on which action was performed, and which observation was received, the agent descends the appropriate tree branch and sees what action it should perform next. Thus, in POMDPs the horizon (the number of timesteps that you plan to act in the environment) is very important, as it significantly affects which actions you should take.</p>
<p>Keep in mind that POMDPs are much harder to solve than MDPs, so it's important to keep the size of the state,action and observation spaces as small as possible.</p>
<h2>POMDP Example</h2>
<p>Let's try to create an POMDP to show how it would work out in practice. Our problem is going to be the following:</p>
<blockquote class="doxtable">
<p>Suppose you are in front of two doors. Behind one is a treasure. Behind the other a man-eating tiger. You don't know in advance where each is, but you want to try to get to the treasure safely. Also, after picking, the doors are reset, so you can try again.</p>
<p>You can try to listen for the breathing of the tiger. The world around you is somewhat noisy, so you may sometimes think you hear something coming from the wrong door.</p>
<p>How much time should you spend listening for the tiger, before trying your luck and opening a door? </p>
</blockquote>
<p>Let's think about how to encode this world into an POMDP. We will go through each component of the POMDP and try to fill it out.</p>
<h3>State Space (S)</h3>
<p>There are only two possibilities: either the door on the left holds the treasure (and the tiger is behind the right), or vice-versa. This will be simple!</p>
<div class="fragment"><div class="line"><span class="keywordtype">size_t</span> S = 2;</div>
<div class="line"><span class="keyword">enum</span> {</div>
<div class="line">    <a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#a80079ec1e78f1ef37a99a62f707c1c8baad9d485f5bb5a893b7ac5cb705e6bcfb">TIG_LEFT</a>    = 0,</div>
<div class="line">    <a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#a80079ec1e78f1ef37a99a62f707c1c8ba68536e50ec8bc047ef2f39d4bac606d8">TIG_RIGHT</a>   = 1,</div>
<div class="line">};</div>
</div><!-- fragment --><h3>Action Space (A)</h3>
<p>Only three actions: open the door on the left, open the door on the right, or wait and listen for the breathing tiger.</p>
<div class="fragment"><div class="line"><span class="keywordtype">size_t</span> A = 3;</div>
<div class="line"><span class="keyword">enum</span> {</div>
<div class="line">    <a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#ae71f86d6929a116664a6ca3951d967e1ab29d515b21e22b0c07f9cb9aea052540">A_LISTEN</a> = 0,</div>
<div class="line">    <a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#ae71f86d6929a116664a6ca3951d967e1a97d69cfb5f15b0e627f44b8e6743aca5">A_LEFT</a>   = 1,</div>
<div class="line">    <a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#ae71f86d6929a116664a6ca3951d967e1a956ef964948f675277bf8ca0c6b9ea3f">A_RIGHT</a>  = 2,</div>
<div class="line">};</div>
</div><!-- fragment --><h3>Observation Space (O)</h3>
<p>Finally, the observations: there are also only two. When we listen, we get an observation on whether we heard the tiger behind the left door or the right one.</p>
<p>Keep in mind that such an observation does not mirror the truth! We may have misheard, and it's this uncertainty that makes this problem a POMDP (rather than an MDP).</p>
<div class="fragment"><div class="line"><span class="keywordtype">size_t</span> O = 2;</div>
<div class="line"><span class="comment">// Using same enum as states</span></div>
</div><!-- fragment --><h3>Transition Function (T)</h3>
<p>The transition function in this problem is also quite simple. If we are listening, nothing changes in the world. When we instead open a door, we discover what's behind it, and the problem is reset.</p>
<p>We can directly write a 3-dimensional transition matrix, as this should be relatively simple.</p>
<div class="fragment"><div class="line"><a class="code" href="namespaceAIToolbox.html#aa09bff0da64cdd264561fe009fa85812">AIToolbox::DumbMatrix3D</a> transitions(boost::extents[S][A][S]);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Transitions</span></div>
<div class="line"><span class="comment">// If we listen, nothing changes.</span></div>
<div class="line"><span class="keywordflow">for</span> ( <span class="keywordtype">size_t</span> s = 0; s &lt; S; ++s )</div>
<div class="line">    transitions[s][<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#ae71f86d6929a116664a6ca3951d967e1ab29d515b21e22b0c07f9cb9aea052540">A_LISTEN</a>][s] = 1.0;</div>
<div class="line"> </div>
<div class="line"><span class="comment">// If we pick a door, tiger and treasure shuffle.</span></div>
<div class="line"><span class="keywordflow">for</span> ( <span class="keywordtype">size_t</span> s = 0; s &lt; S; ++s ) {</div>
<div class="line">    <span class="keywordflow">for</span> ( <span class="keywordtype">size_t</span> s1 = 0; s1 &lt; S; ++s1 ) {</div>
<div class="line">        transitions[s][<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#ae71f86d6929a116664a6ca3951d967e1a97d69cfb5f15b0e627f44b8e6743aca5">A_LEFT</a> ][s1] = 1.0 / S;</div>
<div class="line">        transitions[s][<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#ae71f86d6929a116664a6ca3951d967e1a956ef964948f675277bf8ca0c6b9ea3f">A_RIGHT</a>][s1] = 1.0 / S;</div>
<div class="line">    }</div>
<div class="line">}</div>
</div><!-- fragment --><h3>Reward Function (R)</h3>
<p>The reward function is similar. We want to give a small penalty for listening, so that the agent won't try that forever. We'll give a decent reward for obtaining a treasure, and a great penalty for opening the door to the tiger.</p>
<div class="fragment"><div class="line"><a class="code" href="namespaceAIToolbox.html#aa09bff0da64cdd264561fe009fa85812">AIToolbox::DumbMatrix3D</a> rewards(boost::extents[S][A][S]);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Rewards</span></div>
<div class="line"><span class="comment">// Listening has a small penalty</span></div>
<div class="line"><span class="keywordflow">for</span> ( <span class="keywordtype">size_t</span> s = 0; s &lt; S; ++s )</div>
<div class="line">    <span class="keywordflow">for</span> ( <span class="keywordtype">size_t</span> s1 = 0; s1 &lt; S; ++s1 )</div>
<div class="line">        rewards[s][<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#ae71f86d6929a116664a6ca3951d967e1ab29d515b21e22b0c07f9cb9aea052540">A_LISTEN</a>][s1] = -1.0;</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Treasure has a decent reward, and tiger a bad penalty.</span></div>
<div class="line"><span class="keywordflow">for</span> ( <span class="keywordtype">size_t</span> s1 = 0; s1 &lt; S; ++s1 ) {</div>
<div class="line">    rewards[<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#a80079ec1e78f1ef37a99a62f707c1c8ba68536e50ec8bc047ef2f39d4bac606d8">TIG_RIGHT</a>][<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#ae71f86d6929a116664a6ca3951d967e1a97d69cfb5f15b0e627f44b8e6743aca5">A_LEFT</a>][s1] = 10.0;</div>
<div class="line">    rewards[<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#a80079ec1e78f1ef37a99a62f707c1c8baad9d485f5bb5a893b7ac5cb705e6bcfb">TIG_LEFT</a> ][<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#ae71f86d6929a116664a6ca3951d967e1a97d69cfb5f15b0e627f44b8e6743aca5">A_LEFT</a>][s1] = -100.0;</div>
<div class="line"> </div>
<div class="line">    rewards[<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#a80079ec1e78f1ef37a99a62f707c1c8baad9d485f5bb5a893b7ac5cb705e6bcfb">TIG_LEFT</a> ][<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#ae71f86d6929a116664a6ca3951d967e1a956ef964948f675277bf8ca0c6b9ea3f">A_RIGHT</a>][s1] = 10.0;</div>
<div class="line">    rewards[<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#a80079ec1e78f1ef37a99a62f707c1c8ba68536e50ec8bc047ef2f39d4bac606d8">TIG_RIGHT</a>][<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#ae71f86d6929a116664a6ca3951d967e1a956ef964948f675277bf8ca0c6b9ea3f">A_RIGHT</a>][s1] = -100.0;</div>
<div class="line">}</div>
</div><!-- fragment --><h3>Observation Function (W)</h3>
<p>Finally, the observation function. We want to give the agent an 85% chance of hearing the tiger correctly. So most of the time the observation will mirror the truth, but not always.</p>
<p>We also have to give the agent an observation when it opens a door; in that case we return an observation randomly (with probability <code>1.0 / O</code>), so not to give any information to the agent during a reset.</p>
<div class="fragment"><div class="line"><a class="code" href="namespaceAIToolbox.html#aa09bff0da64cdd264561fe009fa85812">AIToolbox::DumbMatrix3D</a> observations(boost::extents[S][A][O]);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Observations</span></div>
<div class="line"><span class="comment">// If we listen, we guess right 85% of the time.</span></div>
<div class="line">observations[<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#a80079ec1e78f1ef37a99a62f707c1c8baad9d485f5bb5a893b7ac5cb705e6bcfb">TIG_LEFT</a> ][<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#ae71f86d6929a116664a6ca3951d967e1ab29d515b21e22b0c07f9cb9aea052540">A_LISTEN</a>][<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#a80079ec1e78f1ef37a99a62f707c1c8baad9d485f5bb5a893b7ac5cb705e6bcfb">TIG_LEFT</a> ] = 0.85;</div>
<div class="line">observations[<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#a80079ec1e78f1ef37a99a62f707c1c8baad9d485f5bb5a893b7ac5cb705e6bcfb">TIG_LEFT</a> ][<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#ae71f86d6929a116664a6ca3951d967e1ab29d515b21e22b0c07f9cb9aea052540">A_LISTEN</a>][<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#a80079ec1e78f1ef37a99a62f707c1c8ba68536e50ec8bc047ef2f39d4bac606d8">TIG_RIGHT</a>] = 0.15;</div>
<div class="line"> </div>
<div class="line">observations[<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#a80079ec1e78f1ef37a99a62f707c1c8ba68536e50ec8bc047ef2f39d4bac606d8">TIG_RIGHT</a>][<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#ae71f86d6929a116664a6ca3951d967e1ab29d515b21e22b0c07f9cb9aea052540">A_LISTEN</a>][<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#a80079ec1e78f1ef37a99a62f707c1c8ba68536e50ec8bc047ef2f39d4bac606d8">TIG_RIGHT</a>] = 0.85;</div>
<div class="line">observations[<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#a80079ec1e78f1ef37a99a62f707c1c8ba68536e50ec8bc047ef2f39d4bac606d8">TIG_RIGHT</a>][<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#ae71f86d6929a116664a6ca3951d967e1ab29d515b21e22b0c07f9cb9aea052540">A_LISTEN</a>][<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#a80079ec1e78f1ef37a99a62f707c1c8baad9d485f5bb5a893b7ac5cb705e6bcfb">TIG_LEFT</a> ] = 0.15;</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Otherwise we get no information on the environment.</span></div>
<div class="line"><span class="keywordflow">for</span> ( <span class="keywordtype">size_t</span> s = 0; s &lt; S; ++s ) {</div>
<div class="line">    <span class="keywordflow">for</span> ( <span class="keywordtype">size_t</span> o = 0; o &lt; O; ++o ) {</div>
<div class="line">        observations[s][<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#ae71f86d6929a116664a6ca3951d967e1a97d69cfb5f15b0e627f44b8e6743aca5">A_LEFT</a> ][o] = 1.0 / O;</div>
<div class="line">        observations[s][<a class="code" href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#ae71f86d6929a116664a6ca3951d967e1a956ef964948f675277bf8ca0c6b9ea3f">A_RIGHT</a>][o] = 1.0 / O;</div>
<div class="line">    }</div>
<div class="line">}</div>
</div><!-- fragment --><h3>Discount</h3>
<p>We finally use a discount of 0.95 as it's usually done with this problem. Its meaning has not changed from the MDP case.</p>
<h2>The POMDP Model</h2>
<p>Since, differently from the MDP tutorial, we have not written functions for T, R and W, but instead we have directly created matrices, we can simply copy them into a new <code><a class="el" href="classAIToolbox_1_1POMDP_1_1Model.html" title="This class represents a Partially Observable Markov Decision Process.">AIToolbox::POMDP::Model</a></code> to start solving.</p>
<p>The default POMDP model is an extension of a given MDP (since as we said, the POMDP is a generalization of MDPs; the basic environment is still there, it's just that the agent cannot see it directly). In our case, we inherit from <code><a class="el" href="classAIToolbox_1_1MDP_1_1Model.html" title="This class represents a Markov Decision Process.">AIToolbox::MDP::Model</a></code> to keep things simple.</p>
<div class="fragment"><div class="line"><a class="code" href="classAIToolbox_1_1POMDP_1_1Model.html">AIToolbox::POMDP::Model&lt;AIToolbox::MDP::Model&gt;</a> model(O, S, A);</div>
<div class="line"> </div>
<div class="line">model.setTransitionFunction(transitions);</div>
<div class="line">model.setRewardFunction(rewards);</div>
<div class="line">model.setObservationFunction(observations);</div>
<div class="line">model.setDiscount(0.95);</div>
</div><!-- fragment --><h3>The Actual Planning Code</h3>
<p>In case of this POMDP, it's simple enough that we can solve it exactly. We use the <code><a class="el" href="classAIToolbox_1_1POMDP_1_1IncrementalPruning.html" title="This class implements the Incremental Pruning algorithm.">AIToolbox::POMDP::IncrementalPruning</a></code> algorithm, which is one of the most performant exact solvers available.</p>
<p>Here we show also some code that explains how to use the value function from the solution to obtain a policy, and how to use it. As we mentioned, policies are a bit more complex in POMDPs due to their tree-shape.</p>
<p>Please check out the documentation of <code><a class="el" href="classAIToolbox_1_1POMDP_1_1Policy.html" title="This class represents a POMDP Policy.">AIToolbox::POMDP::Policy</a></code> to better learn how to use it.</p>
<div class="fragment"><div class="line"><span class="comment">// Set the horizon. This will determine the optimality of the policy</span></div>
<div class="line"><span class="comment">// dependent on how many steps of observation/action we plan to do. 1 means</span></div>
<div class="line"><span class="comment">// we&#39;re just going to do one thing only, and we&#39;re done. 2 means we get to</span></div>
<div class="line"><span class="comment">// do a single action, observe the result, and act again. And so on.</span></div>
<div class="line"><span class="keywordtype">unsigned</span> horizon = 15;</div>
<div class="line"><span class="comment">// The 0.0 is the tolerance factor, used with high horizons. It gives a way</span></div>
<div class="line"><span class="comment">// to stop the computation if the policy has converged to something static.</span></div>
<div class="line"><a class="code" href="classAIToolbox_1_1POMDP_1_1IncrementalPruning.html">AIToolbox::POMDP::IncrementalPruning</a> solver(horizon, 0.0);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Solve the model. After this line, the problem has been completely</span></div>
<div class="line"><span class="comment">// solved. All that remains is setting up an experiment and see what</span></div>
<div class="line"><span class="comment">// happens!</span></div>
<div class="line"><span class="keyword">auto</span> solution = solver(model);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// We create a policy from the solution, in order to obtain actual actions</span></div>
<div class="line"><span class="comment">// depending on what happens in the environment.</span></div>
<div class="line"><a class="code" href="classAIToolbox_1_1POMDP_1_1Policy.html">AIToolbox::POMDP::Policy</a> policy(2, 3, 2, std::get&lt;1&gt;(solution));</div>
<div class="line"> </div>
<div class="line"><span class="comment">// We begin a simulation, we start from a uniform belief, which means that</span></div>
<div class="line"><span class="comment">// we have no idea on which side the tiger is in. We sample from the belief</span></div>
<div class="line"><span class="comment">// in order to get a &quot;real&quot; state for the world, since this code has to</span></div>
<div class="line"><span class="comment">// both emulate the environment and control the agent. The agent won&#39;t know</span></div>
<div class="line"><span class="comment">// the sampled state though, it will only have the belief to work with.</span></div>
<div class="line"><a class="code" href="namespaceAIToolbox_1_1POMDP.html#a4522c5e35483fb30b2c43c271781e8bc">AIToolbox::POMDP::Belief</a> b(2); b &lt;&lt; 0.5, 0.5;</div>
<div class="line"><span class="keyword">auto</span> s = <a class="code" href="namespaceAIToolbox.html#ad73352f6ce65aa95f76b7399cf0d2cb0">AIToolbox::sampleProbability</a>(2, b, rand);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// The first thing that happens is that we take an action, so we sample it now.</span></div>
<div class="line"><span class="keyword">auto</span> [a, ID] = policy.sampleAction(b, horizon);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// We loop for each step we have yet to do.</span></div>
<div class="line"><span class="keywordtype">double</span> totalReward = 0.0;</div>
<div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">int</span> t = horizon - 1; t &gt;= 0; --t) {</div>
<div class="line">    <span class="comment">// We advance the world one step (the agent only sees the observation</span></div>
<div class="line">    <span class="comment">// and reward).</span></div>
<div class="line">    <span class="keyword">auto</span> [s1, o, r] = model.sampleSOR(s, a);</div>
<div class="line">    <span class="comment">// We and update our total reward.</span></div>
<div class="line">    totalReward += r;</div>
<div class="line"> </div>
<div class="line">    <span class="comment">// We explicitly update the agent belief. This is necessary in some</span></div>
<div class="line">    <span class="comment">// cases depending on convergence of the solution, see below.</span></div>
<div class="line">    <span class="comment">// It is a pretty expensive operation so if performance is required it</span></div>
<div class="line">    <span class="comment">// should be avoided.</span></div>
<div class="line">    b = <a class="code" href="namespaceAIToolbox_1_1POMDP.html#a33a1c23c71b6afae16da4db1863bf68d">AIToolbox::POMDP::updateBelief</a>(model, b, a, o);</div>
<div class="line"> </div>
<div class="line">    <span class="comment">// Now we can use the observation to find out what action we should</span></div>
<div class="line">    <span class="comment">// do next.</span></div>
<div class="line">    <span class="comment">//</span></div>
<div class="line">    <span class="comment">// Depending on whether the solution converged or not, we have to use</span></div>
<div class="line">    <span class="comment">// the policy differently. Suppose that we planned for an horizon of 5,</span></div>
<div class="line">    <span class="comment">// but the solution converged after 3. Then the policy will only be</span></div>
<div class="line">    <span class="comment">// usable with horizons of 3 or less. For higher horizons, the highest</span></div>
<div class="line">    <span class="comment">// step of the policy suffices (since it converged), but it will need a</span></div>
<div class="line">    <span class="comment">// manual belief update to know what to do.</span></div>
<div class="line">    <span class="comment">//</span></div>
<div class="line">    <span class="comment">// Otherwise, the policy implicitly tracks the belief via the id it</span></div>
<div class="line">    <span class="comment">// returned from the last sampling, without the need for a belief</span></div>
<div class="line">    <span class="comment">// update. This is a consequence of the fact that POMDP policies are</span></div>
<div class="line">    <span class="comment">// computed from a piecewise linear and convex value function, so</span></div>
<div class="line">    <span class="comment">// ranges of similar beliefs actually result in needing to do the same</span></div>
<div class="line">    <span class="comment">// thing (since they are similar enough for the timesteps considered).</span></div>
<div class="line">    <span class="keywordflow">if</span> (t &gt; (<span class="keywordtype">int</span>)policy.getH())</div>
<div class="line">        std::tie(a, ID) = policy.sampleAction(b, policy.getH());</div>
<div class="line">    <span class="keywordflow">else</span></div>
<div class="line">        std::tie(a, ID) = policy.sampleAction(ID, o, t);</div>
<div class="line"> </div>
<div class="line">    <span class="comment">// Then we update the world</span></div>
<div class="line">    s = s1;</div>
<div class="line">}</div>
</div><!-- fragment --><p>The full code of this example can be found in the <code>examples/POMDP/tiger_door.cpp</code> file, and is built automatically by adding <code>-DMAKE_EXAMPLES=1</code> when running CMake.</p>
<h2>Conclusions</h2>
<p>This tutorial has given you a very brief introduction in the world of POMDPs. Given that they are so hard to solve, a lot of research has been done in approximate solvers: point-based solvers in particular, where the POMDP is solved only for a certain number of possible beliefs, have seen great success both in theory and in practical applications. <a class="el" href="namespaceAIToolbox.html">AIToolbox</a> implements some of them, for example <code><a class="el" href="classAIToolbox_1_1POMDP_1_1PBVI.html" title="This class implements the Point Based Value Iteration algorithm.">AIToolbox::POMDP::PBVI</a></code>, <code><a class="el" href="classAIToolbox_1_1POMDP_1_1PERSEUS.html" title="This class implements the PERSEUS algorithm.">AIToolbox::POMDP::PERSEUS</a></code>, or even the bound-based <code><a class="el" href="classAIToolbox_1_1POMDP_1_1GapMin.html" title="This class implements the GapMin algorithm.">AIToolbox::POMDP::GapMin</a></code>.</p>
<p>Remember to read each class' documentation, as they explain the ideas behind each algorithm, and how to use them, and feel free to check out external references to learn more about POMDPs. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<div class="ttc" id="anamespaceAIToolbox_html_aa09bff0da64cdd264561fe009fa85812"><div class="ttname"><a href="namespaceAIToolbox.html#aa09bff0da64cdd264561fe009fa85812">AIToolbox::DumbMatrix3D</a></div><div class="ttdeci">boost::multi_array&lt; double, 3 &gt; DumbMatrix3D</div><div class="ttdef"><b>Definition:</b> Types.hpp:38</div></div>
<div class="ttc" id="anamespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums_html_ae71f86d6929a116664a6ca3951d967e1ab29d515b21e22b0c07f9cb9aea052540"><div class="ttname"><a href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#ae71f86d6929a116664a6ca3951d967e1ab29d515b21e22b0c07f9cb9aea052540">AIToolbox::POMDP::TigerProblemEnums::A_LISTEN</a></div><div class="ttdeci">@ A_LISTEN</div><div class="ttdef"><b>Definition:</b> TigerProblem.hpp:40</div></div>
<div class="ttc" id="anamespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums_html_a80079ec1e78f1ef37a99a62f707c1c8ba68536e50ec8bc047ef2f39d4bac606d8"><div class="ttname"><a href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#a80079ec1e78f1ef37a99a62f707c1c8ba68536e50ec8bc047ef2f39d4bac606d8">AIToolbox::POMDP::TigerProblemEnums::TIG_RIGHT</a></div><div class="ttdeci">@ TIG_RIGHT</div><div class="ttdef"><b>Definition:</b> TigerProblem.hpp:47</div></div>
<div class="ttc" id="aclassAIToolbox_1_1POMDP_1_1Model_html"><div class="ttname"><a href="classAIToolbox_1_1POMDP_1_1Model.html">AIToolbox::POMDP::Model</a></div><div class="ttdoc">This class represents a Partially Observable Markov Decision Process.</div><div class="ttdef"><b>Definition:</b> Model.hpp:15</div></div>
<div class="ttc" id="anamespaceAIToolbox_1_1POMDP_html_a33a1c23c71b6afae16da4db1863bf68d"><div class="ttname"><a href="namespaceAIToolbox_1_1POMDP.html#a33a1c23c71b6afae16da4db1863bf68d">AIToolbox::POMDP::updateBelief</a></div><div class="ttdeci">void updateBelief(const M &amp;model, const Belief &amp;b, const size_t a, const size_t o, Belief *bRet)</div><div class="ttdoc">Creates a new belief reflecting changes after an action and observation for a particular Model.</div><div class="ttdef"><b>Definition:</b> Utils.hpp:214</div></div>
<div class="ttc" id="anamespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums_html_ae71f86d6929a116664a6ca3951d967e1a97d69cfb5f15b0e627f44b8e6743aca5"><div class="ttname"><a href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#ae71f86d6929a116664a6ca3951d967e1a97d69cfb5f15b0e627f44b8e6743aca5">AIToolbox::POMDP::TigerProblemEnums::A_LEFT</a></div><div class="ttdeci">@ A_LEFT</div><div class="ttdef"><b>Definition:</b> TigerProblem.hpp:41</div></div>
<div class="ttc" id="anamespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums_html_a80079ec1e78f1ef37a99a62f707c1c8baad9d485f5bb5a893b7ac5cb705e6bcfb"><div class="ttname"><a href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#a80079ec1e78f1ef37a99a62f707c1c8baad9d485f5bb5a893b7ac5cb705e6bcfb">AIToolbox::POMDP::TigerProblemEnums::TIG_LEFT</a></div><div class="ttdeci">@ TIG_LEFT</div><div class="ttdef"><b>Definition:</b> TigerProblem.hpp:46</div></div>
<div class="ttc" id="anamespaceAIToolbox_html_ad73352f6ce65aa95f76b7399cf0d2cb0"><div class="ttname"><a href="namespaceAIToolbox.html#ad73352f6ce65aa95f76b7399cf0d2cb0">AIToolbox::sampleProbability</a></div><div class="ttdeci">size_t sampleProbability(const size_t d, const T &amp;in, G &amp;generator)</div><div class="ttdoc">This function samples an index from a probability vector.</div><div class="ttdef"><b>Definition:</b> Probability.hpp:75</div></div>
<div class="ttc" id="aclassAIToolbox_1_1POMDP_1_1Policy_html"><div class="ttname"><a href="classAIToolbox_1_1POMDP_1_1Policy.html">AIToolbox::POMDP::Policy</a></div><div class="ttdoc">This class represents a POMDP Policy.</div><div class="ttdef"><b>Definition:</b> Policy.hpp:26</div></div>
<div class="ttc" id="aclassAIToolbox_1_1POMDP_1_1IncrementalPruning_html"><div class="ttname"><a href="classAIToolbox_1_1POMDP_1_1IncrementalPruning.html">AIToolbox::POMDP::IncrementalPruning</a></div><div class="ttdoc">This class implements the Incremental Pruning algorithm.</div><div class="ttdef"><b>Definition:</b> IncrementalPruning.hpp:44</div></div>
<div class="ttc" id="anamespaceAIToolbox_1_1POMDP_html_a4522c5e35483fb30b2c43c271781e8bc"><div class="ttname"><a href="namespaceAIToolbox_1_1POMDP.html#a4522c5e35483fb30b2c43c271781e8bc">AIToolbox::POMDP::Belief</a></div><div class="ttdeci">ProbabilityVector Belief</div><div class="ttdoc">This represents a belief, which is a probability distribution over states.</div><div class="ttdef"><b>Definition:</b> Types.hpp:12</div></div>
<div class="ttc" id="anamespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums_html_ae71f86d6929a116664a6ca3951d967e1a956ef964948f675277bf8ca0c6b9ea3f"><div class="ttname"><a href="namespaceAIToolbox_1_1POMDP_1_1TigerProblemEnums.html#ae71f86d6929a116664a6ca3951d967e1a956ef964948f675277bf8ca0c6b9ea3f">AIToolbox::POMDP::TigerProblemEnums::A_RIGHT</a></div><div class="ttdeci">@ A_RIGHT</div><div class="ttdef"><b>Definition:</b> TigerProblem.hpp:42</div></div>
<!-- HTML footer for doxygen 1.8.17-->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
</div>
</body>
</html>
