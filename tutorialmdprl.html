<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>AIToolbox: MDP RL Beginner Tutorial</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">AIToolbox
   </div>
   <div id="projectbrief">A library that offers tools for AI problem solving.</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('tutorialmdprl.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">MDP RL Beginner Tutorial </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This tutorial is meant to teach you how to learn a policy in an MDP even when the dynamics are not known a priori.</p>
<p>This tutorial's code can be found in the <code>examples/MDP/cliff.cpp</code> file, including comments and additional nodes.</p>
<h2>Reinforcement Learning </h2>
<p>While exact definitions vary depending on who is asked, here we consider reinforcement learning to be the process of learning a policy by interaction with an environment, without having previous information about its dynamics.</p>
<p>In particular, the policy we want to learn is the one that maximizes the reward the agent obtains from the environment. An additional constraint that we want to impose is that we learn in in a way to maximize the reward we obtain during the learning process. In this way, the exploration done by the agent is indirectly encouraged to figure out promising actions to perform quickly, rather than trying to understand the problem fully, which would require much more time.</p>
<p>There are many possible approaches to RL, and in this tutorial we focus on two: model-free learning, where the agent directly learns a value function to direct its actions, and model-based learning, where the agent tries to learn a model of the environment and use it to plan its next steps.</p>
<p>In the first case, we don't try to learn what the environment does, and we only focus on what the agent is doing. Some methods use the data to directly modify the policy; in this tutorial instead we use the data to update a value-function, which is then used to inform the agent's policy.</p>
<p>Otherwise, in the model-based case, we will need some infrastructure to store the perceived transitions and rewards, as we want to build a model of the environment. From this model we could plan to obtain a policy, but as our model is updated every timestep (with the new data we gain), planning every timestep gets very expensive. There are methods that can take advantage of the fact that changes are incremental, and update their policies quite quickly.</p>
<h3>MDP RL Example</h3>
<p>In this tutorial we won't create a model from scratch, and instead we will use one of the environments that are provided in the <a class="el" href="namespaceAIToolbox.html">AIToolbox</a> library. The environment is this:</p>
<blockquote class="doxtable">
<p>The agent stands in a gridworld, at the edge of a long cliff. He needs to walk towards some unknown point nearby, and it needs to figure out where it needs to go. At the same time, walking off the cliff sends the agent to its death, while resetting the trial. Can the agent figure out where to go, without falling into the cliff? </p>
</blockquote>
<p>Keep in mind that while this description is useful for us, since we are doing RL the agent itself will have no idea (initially) that it is in a gridworld, nor that the actions we are going to give it are to move around the environment. Everything will be unknown at first, and the agent will need to figure out things as it explores around.</p>
<div class="fragment"><div class="line"><span class="comment">// We make a gridworld of 12 width and 3 height, using AIToolbox provided</span></div><div class="line"><span class="comment">// class.</span></div><div class="line">GridWorld grid(12, 3);</div><div class="line"></div><div class="line"><span class="comment">// We then build a cliff problem out of it. The agent will start at the</span></div><div class="line"><span class="comment">// bottom left corner of the grid, and its target will be the bottom right</span></div><div class="line"><span class="comment">// corner. However, aside from this two corners, all the cells at the bottom</span></div><div class="line"><span class="comment">// of the grid will be marked as the cliff: going there will give a large</span></div><div class="line"><span class="comment">// penalty to the agent, and will reset its position to the bottom left corner.</span></div><div class="line"><span class="keyword">auto</span> problem = <a class="code" href="namespaceAIToolbox_1_1MDP.html#a3d48bc7cca5e75779ffc157e9be6721c">makeCliffProblem</a>(grid);</div></div><!-- fragment --><h4>Q-Learning</h4>
<p>For our model-free method, we are going to use <code><a class="el" href="classAIToolbox_1_1MDP_1_1QLearning.html" title="This class represents the QLearning algorithm. ">AIToolbox::MDP::QLearning</a></code>, which is a staple of modern RL, for its flexibility, simplicity and power.</p>
<p>QLearning uses the data we obtain from interacting with the environment in order to update a QFunction: a mapping between a state-action pair and a numerical value. These values represent how good we think taking a given action in a given state is: higher values are better.</p>
<p>From this QFunction we can then create a policy: in particular, we are going to use a <code><a class="el" href="classAIToolbox_1_1MDP_1_1QGreedyPolicy.html" title="This class models a greedy policy through a QFunction. ">AIToolbox::MDP::QGreedyPolicy</a></code>. This policy selects the action with the highest value in a certain state: it acts <em>greedily</em> with respect to the QFunction.</p>
<p>While always selecting the best action seems like a good idea, we need to remember that here we are <em>learning</em>: at first, the agent has no idea of what actions actually do!</p>
<p>So we combine this greedy policy with an <code><a class="el" href="classAIToolbox_1_1MDP_1_1EpsilonPolicy.html">AIToolbox::MDP::EpsilonPolicy</a></code>, which sometimes selects random actions to help the agent try out new things to see if they work.</p>
<div class="fragment"><div class="line"><span class="comment">// We create the QLearning method. It only needs to know the size of the</span></div><div class="line"><span class="comment">// state and action space, and the discount of the problem (to correctly update</span></div><div class="line"><span class="comment">// values).</span></div><div class="line">QLearning qlLearner(problem.getS(), problem.getA(), problem.getDiscount());</div><div class="line"></div><div class="line"><span class="comment">// We get a reference to the QFunction that QLearning is updating, and use</span></div><div class="line"><span class="comment">// it to construct a greedy policy.</span></div><div class="line">QGreedyPolicy gPolicy(qlLearner.getQFunction());</div><div class="line"></div><div class="line"><span class="comment">// The greedy policy is then augmented with some randomness, to help the</span></div><div class="line"><span class="comment">// agent explore. In this case, we are going to take random actions with</span></div><div class="line"><span class="comment">// probability 0.1 (10%). In the other cases, we will ask the greedy policy</span></div><div class="line"><span class="comment">// what to do, and return that.</span></div><div class="line">EpsilonPolicy ePolicy(gPolicy, 0.1);</div></div><!-- fragment --><p>Now, we need to write some code which will actually make the agent go around the world and try out things. Differently from the planning tutorial, where the problem would be solved in a single line of code, here we actually have to write a loop to simulate the agents going around and observing the results of its actions.</p>
<p>These observations will be passed to QLearning, which will automatically update its QFunction, and by extension the policies (since the policies keep a reference to the original QFunction, they don't have separate copies).</p>
<div class="fragment"><div class="line"><span class="comment">// Initial starting point, the bottom left corner.</span></div><div class="line"><span class="keywordtype">size_t</span> start = problem.getS() - 2;</div><div class="line"></div><div class="line"><span class="keywordtype">size_t</span> s, a;</div><div class="line"><span class="comment">// We perform 10000 episodes, which should be enough to learn this problem.</span></div><div class="line"><span class="comment">// At the start of each episode, we reset the position of the agent. Note</span></div><div class="line"><span class="comment">// that this reset is for the episode; if during the episode the agent falls</span></div><div class="line"><span class="comment">// into the cliff it will also be reset.</span></div><div class="line"><span class="keywordflow">for</span> ( <span class="keywordtype">int</span> episode = 0; episode &lt; 10000; ++episode ) {</div><div class="line">    s = start;</div><div class="line">    <span class="comment">// We limit the length of the episode to 10000 timesteps, to prevent the</span></div><div class="line">    <span class="comment">// agent roaming around indefinitely.</span></div><div class="line">    <span class="keywordflow">for</span> ( <span class="keywordtype">int</span> i = 0; i &lt; 10000; ++i ) {</div><div class="line">        <span class="comment">// Obtain an action for this state (10% random, 90% what we think is</span></div><div class="line">        <span class="comment">// best to do given the current QFunction).</span></div><div class="line">        a = ePolicy.sampleAction( s );</div><div class="line"></div><div class="line">        <span class="comment">// Sample a new state and reward from the problem</span></div><div class="line">        <span class="keyword">const</span> <span class="keyword">auto</span> [s1, rew] = problem.sampleSR( s, a );</div><div class="line"></div><div class="line">        <span class="comment">// Pass the newly collected data to QLearning, to update the</span></div><div class="line">        <span class="comment">// QFunction and improve the agent&#39;s policies.</span></div><div class="line">        qlLearner.stepUpdateQ( s, a, s1, rew );</div><div class="line"></div><div class="line">        <span class="comment">// If we reach the goal, the episode ends</span></div><div class="line">        <span class="keywordflow">if</span> ( s1 == problem.getS() - 1 ) <span class="keywordflow">break</span>;</div><div class="line"></div><div class="line">        s = s1;</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>Once we are done, the agent is ready to act optimally. The only difference is that now we would draw the actions directly from the greedy policy, rather than from the epsilon policy, to avoid taking random actions:</p>
<div class="fragment"><div class="line"><span class="comment">// Take greedy actions directly, skipping ePolicy</span></div><div class="line">a = gPolicy.sampleAction( s );</div></div><!-- fragment --><h4>Prioritized Sweeping</h4>
<p>We're now going to try a different approach, where we try to learn a model of the environment, and use that to obtain a policy.</p>
<p>First, we're going to need <code><a class="el" href="classAIToolbox_1_1MDP_1_1Experience.html" title="This class keeps track of registered events and rewards. ">AIToolbox::MDP::Experience</a></code>, which is a class that records the data we observe from the environment. It records the number of state-action to new state transitions, and for each state-action pair, the average reward obtained and its M2 value (an aggregate of the squared distance from the mean).</p>
<div class="fragment"><div class="line">Experience exp(problem.getS(), problem.getA());</div></div><!-- fragment --><p>Now, from this data, there are different ways to get a model. We're going to go with the simplest, that is to use a maximum likelihood estimator to estimate the transition and reward functions of the problem. Doing this is very simple:</p>
<div class="fragment"><div class="line">MaximumLikelihoodModel&lt;Experience&gt; learnedModel(exp, problem.getDiscount(), <span class="keyword">false</span>);</div></div><!-- fragment --><p>Every time we update our Experience with new data, we'll have to manually sync the learned model to update its transition and reward functions. This is not done automatically since syncing can be a somewhat expensive operation, so sometimes you may want to do it sporadically. Here this is not a concern though.</p>
<p>Now, this learnedModel is itself a model, so if we wanted to we could use <code><a class="el" href="classAIToolbox_1_1MDP_1_1ValueIteration.html" title="This class applies the value iteration algorithm on a Model. ">AIToolbox::MDP::ValueIteration</a></code> to solve it and obtain a policy. However, since we have not interacted with the environment yet, there is really nothing useful to solve.</p>
<p>Instead, we create our learning method, <code><a class="el" href="classAIToolbox_1_1MDP_1_1PrioritizedSweeping.html" title="This class represents the PrioritizedSweeping algorithm. ">AIToolbox::MDP::PrioritizedSweeping</a></code>, and policies:</p>
<div class="fragment"><div class="line">PrioritizedSweeping psLearner(learnedModel);</div><div class="line"></div><div class="line">QGreedyPolicy gPolicy(psLearner.getQFunction());</div><div class="line">EpsilonPolicy ePolicy(gPolicy, 0.1);</div></div><!-- fragment --><p>PrioritizedSweeping, similarly to QLearning, is an algorithm that keeps a QFunction from the input model, and updates it as new data becomes available. The difference with QLearning is that it is much more rapid with its updates, as it is able to reason about the learned model to perform multiple updates for each new data points (while QLearning only does one).</p>
<p>Additionally, note that we have created the QGreedyPolicy and EpsilonPolicy again, so that we can make the agent act and explore in the world to gather data.</p>
<p>Let's now setup the learning loop. Note that for PrioritizedSweeping we need many fewer episodes, as we are extracting more information out of each sample, and we are performing multiple updates to the QFunction per timestep, which greatly speeds up learning.</p>
<div class="fragment"><div class="line"><span class="comment">// Initial starting point, the bottom left corner.</span></div><div class="line"><span class="keywordtype">size_t</span> start = problem.getS() - 2;</div><div class="line"></div><div class="line"><span class="keywordtype">size_t</span> s, a;</div><div class="line"><span class="comment">// We perform 100 episodes, which should be enough to learn this problem.</span></div><div class="line"><span class="comment">// Note that PrioritizedSweeping needs much fewer episodes to learn</span></div><div class="line"><span class="comment">// effectively, as it is using the learned model to extract as much</span></div><div class="line"><span class="comment">// information as possible and doing many updates per timestep.</span></div><div class="line"><span class="comment">// At the start of each episode, we reset the position of the agent. Note</span></div><div class="line"><span class="comment">// that this reset is for the episode; if during the episode the agent falls</span></div><div class="line"><span class="comment">// into the cliff it will also be reset.</span></div><div class="line"><span class="keywordflow">for</span> ( <span class="keywordtype">int</span> episode = 0; episode &lt; 100; ++episode ) {</div><div class="line">    s = start;</div><div class="line">    <span class="comment">// We limit the length of the episode to 10000 timesteps, to prevent the</span></div><div class="line">    <span class="comment">// agent roaming around indefinitely.</span></div><div class="line">    <span class="keywordflow">for</span> ( <span class="keywordtype">int</span> i = 0; i &lt; 10000; ++i ) {</div><div class="line">        <span class="comment">// Obtain an action for this state (10% random, 90% what we think is</span></div><div class="line">        <span class="comment">// best to do given the current QFunction).</span></div><div class="line">        a = ePolicy.sampleAction( s );</div><div class="line"></div><div class="line">        <span class="comment">// Sample a new state and reward from the problem</span></div><div class="line">        <span class="keyword">const</span> <span class="keyword">auto</span> [s1, rew] = problem.sampleSR( s, a );</div><div class="line"></div><div class="line">        <span class="comment">// Record the new data in the Experience, so we can track it</span></div><div class="line">        exp.record(s, a, s1, rew);</div><div class="line"></div><div class="line">        <span class="comment">// Update the learned model with the data we have just got.</span></div><div class="line">        <span class="comment">// This updates both the transition and reward functions.</span></div><div class="line">        learnedModel.sync(s, a, s1);</div><div class="line"></div><div class="line">        <span class="comment">// Update the QFunction using this data.</span></div><div class="line">        psLearner.stepUpdateQ(s, a);</div><div class="line">        <span class="comment">// Finally, use PrioritizedSweeping reasoning capabilities in order</span></div><div class="line">        <span class="comment">// to perform additional updates, and learn much more rapidly that</span></div><div class="line">        <span class="comment">// QLearning.</span></div><div class="line">        psLearner.batchUpdateQ();</div><div class="line"></div><div class="line">        <span class="comment">// If we reach the goal, the episode ends</span></div><div class="line">        <span class="keywordflow">if</span> ( s1 == problem.getS() - 1 ) <span class="keywordflow">break</span>;</div><div class="line"></div><div class="line">        s = s1;</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>As you can see, we didn't need many more lines of code in order to run this model-based method.</p>
<p>The full code of this example can be found in the <code>examples/MDP/cliff.cpp</code> file, and is built automatically by adding <code>-DMAKE_EXAMPLES=1</code> when running CMake.</p>
<h3>Conclusions</h3>
<p>This tutorial should have given you the basics to start using the RL tools in <a class="el" href="namespaceAIToolbox.html">AIToolbox</a>. Most other algorithms and classes work in a similar manner to the ones described, so it should not be too difficult to read the documentation and understand how they work.</p>
<p>Remember that if the documentation is unclear or you need help you can always open an issue on GitHub! </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="tutorials.html">Tutorials</a></li>
    <li class="footer">Generated on Tue Mar 24 2020 21:16:05 for AIToolbox by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
