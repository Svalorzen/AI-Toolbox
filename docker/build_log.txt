[internal] load build definition from Dockerfile
	transferring 1580/0 0.003
[internal] load .dockerignore
	transferring 2/0 0.001
[internal] load metadata for docker.io/library/ubuntu:focal-20231211
[ 1/22] FROM docker.io/library/ubuntu:focal-20231211@sha256:f2034e7195f61334e6caff6ecf2e965f92d11e888309065da85ff50c617732b8
[ 2/22] RUN apt-get update
	Get:1 http://ports.ubuntu.com/ubuntu-ports focal InRelease [265 kB]
	Get:2 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease [114 kB]
	Get:3 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease [108 kB]
	Get:4 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease [114 kB]
	Get:5 http://ports.ubuntu.com/ubuntu-ports focal/restricted arm64 Packages [1317 B]
	Get:6 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 Packages [11.1 MB]
	Get:7 http://ports.ubuntu.com/ubuntu-ports focal/multiverse arm64 Packages [139 kB]
	Get:8 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 Packages [1234 kB]
	Get:9 http://ports.ubuntu.com/ubuntu-ports focal-updates/multiverse arm64 Packages [9395 B]
	Get:10 http://ports.ubuntu.com/ubuntu-ports focal-updates/universe arm64 Packages [1340 kB]
	Get:11 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 Packages [2846 kB]
	Get:12 http://ports.ubuntu.com/ubuntu-ports focal-updates/restricted arm64 Packages [49.3 kB]
	Get:13 http://ports.ubuntu.com/ubuntu-ports focal-backports/universe arm64 Packages [27.8 kB]
	Get:14 http://ports.ubuntu.com/ubuntu-ports focal-backports/main arm64 Packages [54.8 kB]
	Get:15 http://ports.ubuntu.com/ubuntu-ports focal-security/multiverse arm64 Packages [3258 B]
	Get:16 http://ports.ubuntu.com/ubuntu-ports focal-security/universe arm64 Packages [1043 kB]
	Get:17 http://ports.ubuntu.com/ubuntu-ports focal-security/main arm64 Packages [2460 kB]
	Get:18 http://ports.ubuntu.com/ubuntu-ports focal-security/restricted arm64 Packages [49.0 kB]
	Fetched 21.0 MB in 2s (10.1 MB/s)
	Reading package lists...
[ 3/22] RUN apt-get install -y g++-10
	Reading package lists...
	Building dependency tree...
	Reading state information...
	The following additional packages will be installed:
	binutils binutils-aarch64-linux-gnu binutils-common cpp-10 gcc-10 libasan6
	libatomic1 libbinutils libc-dev-bin libc6-dev libcc1-0 libcrypt-dev
	libctf-nobfd0 libctf0 libgcc-10-dev libgomp1 libisl22 libitm1 liblsan0
	libmpc3 libmpfr6 libstdc++-10-dev libtsan0 libubsan1 linux-libc-dev manpages
	manpages-dev
	Suggested packages:
	binutils-doc gcc-10-locales gcc-10-doc glibc-doc libstdc++-10-doc
	man-browser
	The following NEW packages will be installed:
	binutils binutils-aarch64-linux-gnu binutils-common cpp-10 g++-10 gcc-10
	libasan6 libatomic1 libbinutils libc-dev-bin libc6-dev libcc1-0 libcrypt-dev
	libctf-nobfd0 libctf0 libgcc-10-dev libgomp1 libisl22 libitm1 liblsan0
	libmpc3 libmpfr6 libstdc++-10-dev libtsan0 libubsan1 linux-libc-dev manpages
	manpages-dev
	0 upgraded, 28 newly installed, 0 to remove and 0 not upgraded.
	Need to get 51.5 MB of archives.
	After this operation, 198 MB of additional disk space will be used.
	Get:1 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 manpages all 5.05-1 [1314 kB]
	Get:2 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 binutils-common arm64 2.34-6ubuntu1.7 [207 kB]
	Get:3 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libbinutils arm64 2.34-6ubuntu1.7 [475 kB]
	Get:4 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libctf-nobfd0 arm64 2.34-6ubuntu1.7 [44.6 kB]
	Get:5 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libctf0 arm64 2.34-6ubuntu1.7 [43.2 kB]
	Get:6 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 binutils-aarch64-linux-gnu arm64 2.34-6ubuntu1.7 [1987 kB]
	Get:7 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 binutils arm64 2.34-6ubuntu1.7 [3360 B]
	Get:8 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libisl22 arm64 0.22.1-1 [537 kB]
	Get:9 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libmpfr6 arm64 4.0.2-1 [212 kB]
	Get:10 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libmpc3 arm64 1.1.0-1 [37.2 kB]
	Get:11 http://ports.ubuntu.com/ubuntu-ports focal-updates/universe arm64 cpp-10 arm64 10.5.0-1ubuntu1~20.04 [7794 kB]
	Get:12 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libcc1-0 arm64 10.5.0-1ubuntu1~20.04 [46.2 kB]
	Get:13 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libgomp1 arm64 10.5.0-1ubuntu1~20.04 [93.5 kB]
	Get:14 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libitm1 arm64 10.5.0-1ubuntu1~20.04 [24.1 kB]
	Get:15 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libatomic1 arm64 10.5.0-1ubuntu1~20.04 [9808 B]
	Get:16 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libasan6 arm64 10.5.0-1ubuntu1~20.04 [2027 kB]
	Get:17 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 liblsan0 arm64 10.5.0-1ubuntu1~20.04 [800 kB]
	Get:18 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libtsan0 arm64 10.5.0-1ubuntu1~20.04 [1972 kB]
	Get:19 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libubsan1 arm64 10.5.0-1ubuntu1~20.04 [765 kB]
	Get:20 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libgcc-10-dev arm64 10.5.0-1ubuntu1~20.04 [907 kB]
	Get:21 http://ports.ubuntu.com/ubuntu-ports focal-updates/universe arm64 gcc-10 arm64 10.5.0-1ubuntu1~20.04 [15.8 MB]
	Get:22 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libc-dev-bin arm64 2.31-0ubuntu9.14 [64.2 kB]
	Get:23 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 linux-libc-dev arm64 5.4.0-169.187 [1105 kB]
	Get:24 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libcrypt-dev arm64 1:4.4.10-10ubuntu4 [111 kB]
	Get:25 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libc6-dev arm64 2.31-0ubuntu9.14 [2069 kB]
	Get:26 http://ports.ubuntu.com/ubuntu-ports focal-updates/universe arm64 libstdc++-10-dev arm64 10.5.0-1ubuntu1~20.04 [1746 kB]
	Get:27 http://ports.ubuntu.com/ubuntu-ports focal-updates/universe arm64 g++-10 arm64 10.5.0-1ubuntu1~20.04 [8948 kB]
	Get:28 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 manpages-dev all 5.05-1 [2266 kB]
	debconf: delaying package configuration, since apt-utils is not installed
	Fetched 51.5 MB in 4s (11.8 MB/s)
	Selecting previously unselected package manpages.
	(Reading database ...(Reading database ... 5%
(Reading database ... 10%
(Reading database ... 15%
(Reading database ... 20%
(Reading database ... 25%
(Reading database ... 30%
(Reading database ... 35%
(Reading database ... 40%
(Reading database ... 45%
(Reading database ... 50%
(Reading database ... 55%
(Reading database ... 60%
(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%
(Reading database ... 4117 files and directories currently installed.)
	Preparing to unpack .../00-manpages_5.05-1_all.deb ...
	Unpacking manpages (5.05-1) ...
	Selecting previously unselected package binutils-common:arm64.
	Preparing to unpack .../01-binutils-common_2.34-6ubuntu1.7_arm64.deb ...
	Unpacking binutils-common:arm64 (2.34-6ubuntu1.7) ...
	Selecting previously unselected package libbinutils:arm64.
	Preparing to unpack .../02-libbinutils_2.34-6ubuntu1.7_arm64.deb ...
	Unpacking libbinutils:arm64 (2.34-6ubuntu1.7) ...
	Selecting previously unselected package libctf-nobfd0:arm64.
	Preparing to unpack .../03-libctf-nobfd0_2.34-6ubuntu1.7_arm64.deb ...
	Unpacking libctf-nobfd0:arm64 (2.34-6ubuntu1.7) ...
	Selecting previously unselected package libctf0:arm64.
	Preparing to unpack .../04-libctf0_2.34-6ubuntu1.7_arm64.deb ...
	Unpacking libctf0:arm64 (2.34-6ubuntu1.7) ...
	Selecting previously unselected package binutils-aarch64-linux-gnu.
	Preparing to unpack .../05-binutils-aarch64-linux-gnu_2.34-6ubuntu1.7_arm64.deb ...
	Unpacking binutils-aarch64-linux-gnu (2.34-6ubuntu1.7) ...
	Selecting previously unselected package binutils.
	Preparing to unpack .../06-binutils_2.34-6ubuntu1.7_arm64.deb ...
	Unpacking binutils (2.34-6ubuntu1.7) ...
	Selecting previously unselected package libisl22:arm64.
	Preparing to unpack .../07-libisl22_0.22.1-1_arm64.deb ...
	Unpacking libisl22:arm64 (0.22.1-1) ...
	Selecting previously unselected package libmpfr6:arm64.
	Preparing to unpack .../08-libmpfr6_4.0.2-1_arm64.deb ...
	Unpacking libmpfr6:arm64 (4.0.2-1) ...
	Selecting previously unselected package libmpc3:arm64.
	Preparing to unpack .../09-libmpc3_1.1.0-1_arm64.deb ...
	Unpacking libmpc3:arm64 (1.1.0-1) ...
	Selecting previously unselected package cpp-10.
	Preparing to unpack .../10-cpp-10_10.5.0-1ubuntu1~20.04_arm64.deb ...
	Unpacking cpp-10 (10.5.0-1ubuntu1~20.04) ...
	Selecting previously unselected package libcc1-0:arm64.
	Preparing to unpack .../11-libcc1-0_10.5.0-1ubuntu1~20.04_arm64.deb ...
	Unpacking libcc1-0:arm64 (10.5.0-1ubuntu1~20.04) ...
	Selecting previously unselected package libgomp1:arm64.
	Preparing to unpack .../12-libgomp1_10.5.0-1ubuntu1~20.04_arm64.deb ...
	Unpacking libgomp1:arm64 (10.5.0-1ubuntu1~20.04) ...
	Selecting previously unselected package libitm1:arm64.
	Preparing to unpack .../13-libitm1_10.5.0-1ubuntu1~20.04_arm64.deb ...
	Unpacking libitm1:arm64 (10.5.0-1ubuntu1~20.04) ...
	Selecting previously unselected package libatomic1:arm64.
	Preparing to unpack .../14-libatomic1_10.5.0-1ubuntu1~20.04_arm64.deb ...
	Unpacking libatomic1:arm64 (10.5.0-1ubuntu1~20.04) ...
	Selecting previously unselected package libasan6:arm64.
	Preparing to unpack .../15-libasan6_10.5.0-1ubuntu1~20.04_arm64.deb ...
	Unpacking libasan6:arm64 (10.5.0-1ubuntu1~20.04) ...
	Selecting previously unselected package liblsan0:arm64.
	Preparing to unpack .../16-liblsan0_10.5.0-1ubuntu1~20.04_arm64.deb ...
	Unpacking liblsan0:arm64 (10.5.0-1ubuntu1~20.04) ...
	Selecting previously unselected package libtsan0:arm64.
	Preparing to unpack .../17-libtsan0_10.5.0-1ubuntu1~20.04_arm64.deb ...
	Unpacking libtsan0:arm64 (10.5.0-1ubuntu1~20.04) ...
	Selecting previously unselected package libubsan1:arm64.
	Preparing to unpack .../18-libubsan1_10.5.0-1ubuntu1~20.04_arm64.deb ...
	Unpacking libubsan1:arm64 (10.5.0-1ubuntu1~20.04) ...
	Selecting previously unselected package libgcc-10-dev:arm64.
	Preparing to unpack .../19-libgcc-10-dev_10.5.0-1ubuntu1~20.04_arm64.deb ...
	Unpacking libgcc-10-dev:arm64 (10.5.0-1ubuntu1~20.04) ...
	Selecting previously unselected package gcc-10.
	Preparing to unpack .../20-gcc-10_10.5.0-1ubuntu1~20.04_arm64.deb ...
	Unpacking gcc-10 (10.5.0-1ubuntu1~20.04) ...
	Selecting previously unselected package libc-dev-bin.
	Preparing to unpack .../21-libc-dev-bin_2.31-0ubuntu9.14_arm64.deb ...
	Unpacking libc-dev-bin (2.31-0ubuntu9.14) ...
	Selecting previously unselected package linux-libc-dev:arm64.
	Preparing to unpack .../22-linux-libc-dev_5.4.0-169.187_arm64.deb ...
	Unpacking linux-libc-dev:arm64 (5.4.0-169.187) ...
	Selecting previously unselected package libcrypt-dev:arm64.
	Preparing to unpack .../23-libcrypt-dev_1%3a4.4.10-10ubuntu4_arm64.deb ...
	Unpacking libcrypt-dev:arm64 (1:4.4.10-10ubuntu4) ...
	Selecting previously unselected package libc6-dev:arm64.
	Preparing to unpack .../24-libc6-dev_2.31-0ubuntu9.14_arm64.deb ...
	Unpacking libc6-dev:arm64 (2.31-0ubuntu9.14) ...
	Selecting previously unselected package libstdc++-10-dev:arm64.
	Preparing to unpack .../25-libstdc++-10-dev_10.5.0-1ubuntu1~20.04_arm64.deb ...
	Unpacking libstdc++-10-dev:arm64 (10.5.0-1ubuntu1~20.04) ...
	Selecting previously unselected package g++-10.
	Preparing to unpack .../26-g++-10_10.5.0-1ubuntu1~20.04_arm64.deb ...
	Unpacking g++-10 (10.5.0-1ubuntu1~20.04) ...
	Selecting previously unselected package manpages-dev.
	Preparing to unpack .../27-manpages-dev_5.05-1_all.deb ...
	Unpacking manpages-dev (5.05-1) ...
	Setting up manpages (5.05-1) ...
	Setting up binutils-common:arm64 (2.34-6ubuntu1.7) ...
	Setting up linux-libc-dev:arm64 (5.4.0-169.187) ...
	Setting up libctf-nobfd0:arm64 (2.34-6ubuntu1.7) ...
	Setting up libgomp1:arm64 (10.5.0-1ubuntu1~20.04) ...
	Setting up libasan6:arm64 (10.5.0-1ubuntu1~20.04) ...
	Setting up libmpfr6:arm64 (4.0.2-1) ...
	Setting up libmpc3:arm64 (1.1.0-1) ...
	Setting up libatomic1:arm64 (10.5.0-1ubuntu1~20.04) ...
	Setting up libubsan1:arm64 (10.5.0-1ubuntu1~20.04) ...
	Setting up libcrypt-dev:arm64 (1:4.4.10-10ubuntu4) ...
	Setting up libisl22:arm64 (0.22.1-1) ...
	Setting up libbinutils:arm64 (2.34-6ubuntu1.7) ...
	Setting up libc-dev-bin (2.31-0ubuntu9.14) ...
	Setting up libcc1-0:arm64 (10.5.0-1ubuntu1~20.04) ...
	Setting up liblsan0:arm64 (10.5.0-1ubuntu1~20.04) ...
	Setting up cpp-10 (10.5.0-1ubuntu1~20.04) ...
	Setting up libitm1:arm64 (10.5.0-1ubuntu1~20.04) ...
	Setting up libtsan0:arm64 (10.5.0-1ubuntu1~20.04) ...
	Setting up libctf0:arm64 (2.34-6ubuntu1.7) ...
	Setting up manpages-dev (5.05-1) ...
	Setting up libgcc-10-dev:arm64 (10.5.0-1ubuntu1~20.04) ...
	Setting up binutils-aarch64-linux-gnu (2.34-6ubuntu1.7) ...
	Setting up binutils (2.34-6ubuntu1.7) ...
	Setting up gcc-10 (10.5.0-1ubuntu1~20.04) ...
	Setting up libc6-dev:arm64 (2.31-0ubuntu9.14) ...
	Setting up libstdc++-10-dev:arm64 (10.5.0-1ubuntu1~20.04) ...
	Setting up g++-10 (10.5.0-1ubuntu1~20.04) ...
	Processing triggers for libc-bin (2.31-0ubuntu9.14) ...
[ 4/22] RUN apt-get install -y cmake
	Reading package lists...
	Building dependency tree...
	Reading state information...
	The following additional packages will be installed:
	ca-certificates cmake-data cpp cpp-9 gcc gcc-9 gcc-9-base krb5-locales
	libarchive13 libasan5 libasn1-8-heimdal libbrotli1 libcurl4 libexpat1
	libgcc-9-dev libgssapi-krb5-2 libgssapi3-heimdal libhcrypto4-heimdal
	libheimbase1-heimdal libheimntlm0-heimdal libhx509-5-heimdal libicu66
	libjsoncpp1 libk5crypto3 libkeyutils1 libkrb5-26-heimdal libkrb5-3
	libkrb5support0 libldap-2.4-2 libldap-common libnghttp2-14 libpsl5 librhash0
	libroken18-heimdal librtmp1 libsasl2-2 libsasl2-modules libsasl2-modules-db
	libsqlite3-0 libssh-4 libssl1.1 libuv1 libwind0-heimdal libxml2 make openssl
	publicsuffix tzdata
	Suggested packages:
	cmake-doc ninja-build cpp-doc gcc-9-locales gcc-multilib autoconf automake
	libtool flex bison gdb gcc-doc gcc-9-doc lrzip krb5-doc krb5-user
	libsasl2-modules-gssapi-mit | libsasl2-modules-gssapi-heimdal
	libsasl2-modules-ldap libsasl2-modules-otp libsasl2-modules-sql make-doc
	The following NEW packages will be installed:
	ca-certificates cmake cmake-data cpp cpp-9 gcc gcc-9 gcc-9-base krb5-locales
	libarchive13 libasan5 libasn1-8-heimdal libbrotli1 libcurl4 libexpat1
	libgcc-9-dev libgssapi-krb5-2 libgssapi3-heimdal libhcrypto4-heimdal
	libheimbase1-heimdal libheimntlm0-heimdal libhx509-5-heimdal libicu66
	libjsoncpp1 libk5crypto3 libkeyutils1 libkrb5-26-heimdal libkrb5-3
	libkrb5support0 libldap-2.4-2 libldap-common libnghttp2-14 libpsl5 librhash0
	libroken18-heimdal librtmp1 libsasl2-2 libsasl2-modules libsasl2-modules-db
	libsqlite3-0 libssh-4 libssl1.1 libuv1 libwind0-heimdal libxml2 make openssl
	publicsuffix tzdata
	0 upgraded, 49 newly installed, 0 to remove and 0 not upgraded.
	Need to get 36.0 MB of archives.
	After this operation, 151 MB of additional disk space will be used.
	Get:1 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libssl1.1 arm64 1.1.1f-1ubuntu2.20 [1157 kB]
	Get:2 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 openssl arm64 1.1.1f-1ubuntu2.20 [599 kB]
	Get:3 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 ca-certificates all 20230311ubuntu0.20.04.1 [152 kB]
	Get:4 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libexpat1 arm64 2.2.9-1ubuntu0.6 [62.5 kB]
	Get:5 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 tzdata all 2023c-0ubuntu0.20.04.2 [301 kB]
	Get:6 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libicu66 arm64 66.1-2ubuntu2.1 [8360 kB]
	Get:7 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libsqlite3-0 arm64 3.31.1-4ubuntu0.6 [507 kB]
	Get:8 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libxml2 arm64 2.9.10+dfsg-5ubuntu0.20.04.6 [570 kB]
	Get:9 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 krb5-locales all 1.17-6ubuntu4.4 [11.5 kB]
	Get:10 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libkrb5support0 arm64 1.17-6ubuntu4.4 [30.5 kB]
	Get:11 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libk5crypto3 arm64 1.17-6ubuntu4.4 [80.4 kB]
	Get:12 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libkeyutils1 arm64 1.6-6ubuntu1.1 [10.1 kB]
	Get:13 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libkrb5-3 arm64 1.17-6ubuntu4.4 [312 kB]
	Get:14 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libgssapi-krb5-2 arm64 1.17-6ubuntu4.4 [113 kB]
	Get:15 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libpsl5 arm64 0.21.0-1ubuntu1 [51.3 kB]
	Get:16 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libuv1 arm64 1.34.2-1ubuntu1.3 [75.1 kB]
	Get:17 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 publicsuffix all 20200303.0012-1 [111 kB]
	Get:18 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 cmake-data all 3.16.3-1ubuntu1.20.04.1 [1613 kB]
	Get:19 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libarchive13 arm64 3.4.0-2ubuntu1.2 [304 kB]
	Get:20 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libbrotli1 arm64 1.0.7-6ubuntu0.1 [257 kB]
	Get:21 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libroken18-heimdal arm64 7.7.0+dfsg-1ubuntu1.4 [40.1 kB]
	Get:22 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libasn1-8-heimdal arm64 7.7.0+dfsg-1ubuntu1.4 [150 kB]
	Get:23 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libheimbase1-heimdal arm64 7.7.0+dfsg-1ubuntu1.4 [28.7 kB]
	Get:24 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libhcrypto4-heimdal arm64 7.7.0+dfsg-1ubuntu1.4 [84.7 kB]
	Get:25 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libwind0-heimdal arm64 7.7.0+dfsg-1ubuntu1.4 [47.5 kB]
	Get:26 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libhx509-5-heimdal arm64 7.7.0+dfsg-1ubuntu1.4 [98.9 kB]
	Get:27 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libkrb5-26-heimdal arm64 7.7.0+dfsg-1ubuntu1.4 [192 kB]
	Get:28 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libheimntlm0-heimdal arm64 7.7.0+dfsg-1ubuntu1.4 [14.7 kB]
	Get:29 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libgssapi3-heimdal arm64 7.7.0+dfsg-1ubuntu1.4 [88.4 kB]
	Get:30 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libsasl2-modules-db arm64 2.1.27+dfsg-2ubuntu0.1 [14.9 kB]
	Get:31 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libsasl2-2 arm64 2.1.27+dfsg-2ubuntu0.1 [48.4 kB]
	Get:32 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libldap-common all 2.4.49+dfsg-2ubuntu1.9 [16.6 kB]
	Get:33 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libldap-2.4-2 arm64 2.4.49+dfsg-2ubuntu1.9 [145 kB]
	Get:34 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libnghttp2-14 arm64 1.40.0-1ubuntu0.2 [75.9 kB]
	Get:35 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 librtmp1 arm64 2.4+20151223.gitfa8646d.1-2build1 [53.3 kB]
	Get:36 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libssh-4 arm64 0.9.3-2ubuntu2.4 [160 kB]
	Get:37 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libcurl4 arm64 7.68.0-1ubuntu2.21 [216 kB]
	Get:38 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libjsoncpp1 arm64 1.7.4-3.1ubuntu2 [69.8 kB]
	Get:39 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 librhash0 arm64 1.3.9-1 [111 kB]
	Get:40 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 cmake arm64 3.16.3-1ubuntu1.20.04.1 [3112 kB]
	Get:41 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 gcc-9-base arm64 9.4.0-1ubuntu1~20.04.2 [18.9 kB]
	Get:42 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 cpp-9 arm64 9.4.0-1ubuntu1~20.04.2 [5985 kB]
	Get:43 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 cpp arm64 4:9.3.0-1ubuntu2 [27.6 kB]
	Get:44 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libasan5 arm64 9.4.0-1ubuntu1~20.04.2 [2690 kB]
	Get:45 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libgcc-9-dev arm64 9.4.0-1ubuntu1~20.04.2 [920 kB]
	Get:46 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 gcc-9 arm64 9.4.0-1ubuntu1~20.04.2 [6736 kB]
	Get:47 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 gcc arm64 4:9.3.0-1ubuntu2 [5228 B]
	Get:48 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libsasl2-modules arm64 2.1.27+dfsg-2ubuntu0.1 [46.1 kB]
	Get:49 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 make arm64 4.2.1-1.2 [154 kB]
	debconf: delaying package configuration, since apt-utils is not installed
	Fetched 36.0 MB in 4s (10.2 MB/s)
	Selecting previously unselected package libssl1.1:arm64.
	(Reading database ...(Reading database ... 5%
(Reading database ... 10%
(Reading database ... 15%
(Reading database ... 20%
(Reading database ... 25%
(Reading database ... 30%
(Reading database ... 35%
(Reading database ... 40%
(Reading database ... 45%
(Reading database ... 50%
(Reading database ... 55%
(Reading database ... 60%
(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%
(Reading database ... 9441 files and directories currently installed.)
	Preparing to unpack .../00-libssl1.1_1.1.1f-1ubuntu2.20_arm64.deb ...
	Unpacking libssl1.1:arm64 (1.1.1f-1ubuntu2.20) ...
	Selecting previously unselected package openssl.
	Preparing to unpack .../01-openssl_1.1.1f-1ubuntu2.20_arm64.deb ...
	Unpacking openssl (1.1.1f-1ubuntu2.20) ...
	Selecting previously unselected package ca-certificates.
	Preparing to unpack .../02-ca-certificates_20230311ubuntu0.20.04.1_all.deb ...
	Unpacking ca-certificates (20230311ubuntu0.20.04.1) ...
	Selecting previously unselected package libexpat1:arm64.
	Preparing to unpack .../03-libexpat1_2.2.9-1ubuntu0.6_arm64.deb ...
	Unpacking libexpat1:arm64 (2.2.9-1ubuntu0.6) ...
	Selecting previously unselected package tzdata.
	Preparing to unpack .../04-tzdata_2023c-0ubuntu0.20.04.2_all.deb ...
	Unpacking tzdata (2023c-0ubuntu0.20.04.2) ...
	Selecting previously unselected package libicu66:arm64.
	Preparing to unpack .../05-libicu66_66.1-2ubuntu2.1_arm64.deb ...
	Unpacking libicu66:arm64 (66.1-2ubuntu2.1) ...
	Selecting previously unselected package libsqlite3-0:arm64.
	Preparing to unpack .../06-libsqlite3-0_3.31.1-4ubuntu0.6_arm64.deb ...
	Unpacking libsqlite3-0:arm64 (3.31.1-4ubuntu0.6) ...
	Selecting previously unselected package libxml2:arm64.
	Preparing to unpack .../07-libxml2_2.9.10+dfsg-5ubuntu0.20.04.6_arm64.deb ...
	Unpacking libxml2:arm64 (2.9.10+dfsg-5ubuntu0.20.04.6) ...
	Selecting previously unselected package krb5-locales.
	Preparing to unpack .../08-krb5-locales_1.17-6ubuntu4.4_all.deb ...
	Unpacking krb5-locales (1.17-6ubuntu4.4) ...
	Selecting previously unselected package libkrb5support0:arm64.
	Preparing to unpack .../09-libkrb5support0_1.17-6ubuntu4.4_arm64.deb ...
	Unpacking libkrb5support0:arm64 (1.17-6ubuntu4.4) ...
	Selecting previously unselected package libk5crypto3:arm64.
	Preparing to unpack .../10-libk5crypto3_1.17-6ubuntu4.4_arm64.deb ...
	Unpacking libk5crypto3:arm64 (1.17-6ubuntu4.4) ...
	Selecting previously unselected package libkeyutils1:arm64.
	Preparing to unpack .../11-libkeyutils1_1.6-6ubuntu1.1_arm64.deb ...
	Unpacking libkeyutils1:arm64 (1.6-6ubuntu1.1) ...
	Selecting previously unselected package libkrb5-3:arm64.
	Preparing to unpack .../12-libkrb5-3_1.17-6ubuntu4.4_arm64.deb ...
	Unpacking libkrb5-3:arm64 (1.17-6ubuntu4.4) ...
	Selecting previously unselected package libgssapi-krb5-2:arm64.
	Preparing to unpack .../13-libgssapi-krb5-2_1.17-6ubuntu4.4_arm64.deb ...
	Unpacking libgssapi-krb5-2:arm64 (1.17-6ubuntu4.4) ...
	Selecting previously unselected package libpsl5:arm64.
	Preparing to unpack .../14-libpsl5_0.21.0-1ubuntu1_arm64.deb ...
	Unpacking libpsl5:arm64 (0.21.0-1ubuntu1) ...
	Selecting previously unselected package libuv1:arm64.
	Preparing to unpack .../15-libuv1_1.34.2-1ubuntu1.3_arm64.deb ...
	Unpacking libuv1:arm64 (1.34.2-1ubuntu1.3) ...
	Selecting previously unselected package publicsuffix.
	Preparing to unpack .../16-publicsuffix_20200303.0012-1_all.deb ...
	Unpacking publicsuffix (20200303.0012-1) ...
	Selecting previously unselected package cmake-data.
	Preparing to unpack .../17-cmake-data_3.16.3-1ubuntu1.20.04.1_all.deb ...
	Unpacking cmake-data (3.16.3-1ubuntu1.20.04.1) ...
	Selecting previously unselected package libarchive13:arm64.
	Preparing to unpack .../18-libarchive13_3.4.0-2ubuntu1.2_arm64.deb ...
	Unpacking libarchive13:arm64 (3.4.0-2ubuntu1.2) ...
	Selecting previously unselected package libbrotli1:arm64.
	Preparing to unpack .../19-libbrotli1_1.0.7-6ubuntu0.1_arm64.deb ...
	Unpacking libbrotli1:arm64 (1.0.7-6ubuntu0.1) ...
	Selecting previously unselected package libroken18-heimdal:arm64.
	Preparing to unpack .../20-libroken18-heimdal_7.7.0+dfsg-1ubuntu1.4_arm64.deb ...
	Unpacking libroken18-heimdal:arm64 (7.7.0+dfsg-1ubuntu1.4) ...
	Selecting previously unselected package libasn1-8-heimdal:arm64.
	Preparing to unpack .../21-libasn1-8-heimdal_7.7.0+dfsg-1ubuntu1.4_arm64.deb ...
	Unpacking libasn1-8-heimdal:arm64 (7.7.0+dfsg-1ubuntu1.4) ...
	Selecting previously unselected package libheimbase1-heimdal:arm64.
	Preparing to unpack .../22-libheimbase1-heimdal_7.7.0+dfsg-1ubuntu1.4_arm64.deb ...
	Unpacking libheimbase1-heimdal:arm64 (7.7.0+dfsg-1ubuntu1.4) ...
	Selecting previously unselected package libhcrypto4-heimdal:arm64.
	Preparing to unpack .../23-libhcrypto4-heimdal_7.7.0+dfsg-1ubuntu1.4_arm64.deb ...
	Unpacking libhcrypto4-heimdal:arm64 (7.7.0+dfsg-1ubuntu1.4) ...
	Selecting previously unselected package libwind0-heimdal:arm64.
	Preparing to unpack .../24-libwind0-heimdal_7.7.0+dfsg-1ubuntu1.4_arm64.deb ...
	Unpacking libwind0-heimdal:arm64 (7.7.0+dfsg-1ubuntu1.4) ...
	Selecting previously unselected package libhx509-5-heimdal:arm64.
	Preparing to unpack .../25-libhx509-5-heimdal_7.7.0+dfsg-1ubuntu1.4_arm64.deb ...
	Unpacking libhx509-5-heimdal:arm64 (7.7.0+dfsg-1ubuntu1.4) ...
	Selecting previously unselected package libkrb5-26-heimdal:arm64.
	Preparing to unpack .../26-libkrb5-26-heimdal_7.7.0+dfsg-1ubuntu1.4_arm64.deb ...
	Unpacking libkrb5-26-heimdal:arm64 (7.7.0+dfsg-1ubuntu1.4) ...
	Selecting previously unselected package libheimntlm0-heimdal:arm64.
	Preparing to unpack .../27-libheimntlm0-heimdal_7.7.0+dfsg-1ubuntu1.4_arm64.deb ...
	Unpacking libheimntlm0-heimdal:arm64 (7.7.0+dfsg-1ubuntu1.4) ...
	Selecting previously unselected package libgssapi3-heimdal:arm64.
	Preparing to unpack .../28-libgssapi3-heimdal_7.7.0+dfsg-1ubuntu1.4_arm64.deb ...
	Unpacking libgssapi3-heimdal:arm64 (7.7.0+dfsg-1ubuntu1.4) ...
	Selecting previously unselected package libsasl2-modules-db:arm64.
	Preparing to unpack .../29-libsasl2-modules-db_2.1.27+dfsg-2ubuntu0.1_arm64.deb ...
	Unpacking libsasl2-modules-db:arm64 (2.1.27+dfsg-2ubuntu0.1) ...
	Selecting previously unselected package libsasl2-2:arm64.
	Preparing to unpack .../30-libsasl2-2_2.1.27+dfsg-2ubuntu0.1_arm64.deb ...
	Unpacking libsasl2-2:arm64 (2.1.27+dfsg-2ubuntu0.1) ...
	Selecting previously unselected package libldap-common.
	Preparing to unpack .../31-libldap-common_2.4.49+dfsg-2ubuntu1.9_all.deb ...
	Unpacking libldap-common (2.4.49+dfsg-2ubuntu1.9) ...
	Selecting previously unselected package libldap-2.4-2:arm64.
	Preparing to unpack .../32-libldap-2.4-2_2.4.49+dfsg-2ubuntu1.9_arm64.deb ...
	Unpacking libldap-2.4-2:arm64 (2.4.49+dfsg-2ubuntu1.9) ...
	Selecting previously unselected package libnghttp2-14:arm64.
	Preparing to unpack .../33-libnghttp2-14_1.40.0-1ubuntu0.2_arm64.deb ...
	Unpacking libnghttp2-14:arm64 (1.40.0-1ubuntu0.2) ...
	Selecting previously unselected package librtmp1:arm64.
	Preparing to unpack .../34-librtmp1_2.4+20151223.gitfa8646d.1-2build1_arm64.deb ...
	Unpacking librtmp1:arm64 (2.4+20151223.gitfa8646d.1-2build1) ...
	Selecting previously unselected package libssh-4:arm64.
	Preparing to unpack .../35-libssh-4_0.9.3-2ubuntu2.4_arm64.deb ...
	Unpacking libssh-4:arm64 (0.9.3-2ubuntu2.4) ...
	Selecting previously unselected package libcurl4:arm64.
	Preparing to unpack .../36-libcurl4_7.68.0-1ubuntu2.21_arm64.deb ...
	Unpacking libcurl4:arm64 (7.68.0-1ubuntu2.21) ...
	Selecting previously unselected package libjsoncpp1:arm64.
	Preparing to unpack .../37-libjsoncpp1_1.7.4-3.1ubuntu2_arm64.deb ...
	Unpacking libjsoncpp1:arm64 (1.7.4-3.1ubuntu2) ...
	Selecting previously unselected package librhash0:arm64.
	Preparing to unpack .../38-librhash0_1.3.9-1_arm64.deb ...
	Unpacking librhash0:arm64 (1.3.9-1) ...
	Selecting previously unselected package cmake.
	Preparing to unpack .../39-cmake_3.16.3-1ubuntu1.20.04.1_arm64.deb ...
	Unpacking cmake (3.16.3-1ubuntu1.20.04.1) ...
	Selecting previously unselected package gcc-9-base:arm64.
	Preparing to unpack .../40-gcc-9-base_9.4.0-1ubuntu1~20.04.2_arm64.deb ...
	Unpacking gcc-9-base:arm64 (9.4.0-1ubuntu1~20.04.2) ...
	Selecting previously unselected package cpp-9.
	Preparing to unpack .../41-cpp-9_9.4.0-1ubuntu1~20.04.2_arm64.deb ...
	Unpacking cpp-9 (9.4.0-1ubuntu1~20.04.2) ...
	Selecting previously unselected package cpp.
	Preparing to unpack .../42-cpp_4%3a9.3.0-1ubuntu2_arm64.deb ...
	Unpacking cpp (4:9.3.0-1ubuntu2) ...
	Selecting previously unselected package libasan5:arm64.
	Preparing to unpack .../43-libasan5_9.4.0-1ubuntu1~20.04.2_arm64.deb ...
	Unpacking libasan5:arm64 (9.4.0-1ubuntu1~20.04.2) ...
	Selecting previously unselected package libgcc-9-dev:arm64.
	Preparing to unpack .../44-libgcc-9-dev_9.4.0-1ubuntu1~20.04.2_arm64.deb ...
	Unpacking libgcc-9-dev:arm64 (9.4.0-1ubuntu1~20.04.2) ...
	Selecting previously unselected package gcc-9.
	Preparing to unpack .../45-gcc-9_9.4.0-1ubuntu1~20.04.2_arm64.deb ...
	Unpacking gcc-9 (9.4.0-1ubuntu1~20.04.2) ...
	Selecting previously unselected package gcc.
	Preparing to unpack .../46-gcc_4%3a9.3.0-1ubuntu2_arm64.deb ...
	Unpacking gcc (4:9.3.0-1ubuntu2) ...
	Selecting previously unselected package libsasl2-modules:arm64.
	Preparing to unpack .../47-libsasl2-modules_2.1.27+dfsg-2ubuntu0.1_arm64.deb ...
	Unpacking libsasl2-modules:arm64 (2.1.27+dfsg-2ubuntu0.1) ...
	Selecting previously unselected package make.
	Preparing to unpack .../48-make_4.2.1-1.2_arm64.deb ...
	Unpacking make (4.2.1-1.2) ...
	Setting up libexpat1:arm64 (2.2.9-1ubuntu0.6) ...
	Setting up libkeyutils1:arm64 (1.6-6ubuntu1.1) ...
	Setting up libpsl5:arm64 (0.21.0-1ubuntu1) ...
	Setting up libssl1.1:arm64 (1.1.1f-1ubuntu2.20) ...
	debconf: unable to initialize frontend: Dialog
	debconf: (TERM is not set, so the dialog frontend is not usable.)
	debconf: falling back to frontend: Readline
	debconf: unable to initialize frontend: Readline
	debconf: (Can't locate Term/ReadLine.pm in @INC (you may need to install the Term::ReadLine module) (@INC contains: /etc/perl /usr/local/lib/aarch64-linux-gnu/perl/5.30.0 /usr/local/share/perl/5.30.0 /usr/lib/aarch64-linux-gnu/perl5/5.30 /usr/share/perl5 /usr/lib/aarch64-linux-gnu/perl/5.30 /usr/share/perl/5.30 /usr/local/lib/site_perl /usr/lib/aarch64-linux-gnu/perl-base) at /usr/share/perl5/Debconf/FrontEnd/Readline.pm line 7.)
	debconf: falling back to frontend: Teletype
	Setting up libbrotli1:arm64 (1.0.7-6ubuntu0.1) ...
	Setting up libsqlite3-0:arm64 (3.31.1-4ubuntu0.6) ...
	Setting up libsasl2-modules:arm64 (2.1.27+dfsg-2ubuntu0.1) ...
	Setting up libnghttp2-14:arm64 (1.40.0-1ubuntu0.2) ...
	Setting up krb5-locales (1.17-6ubuntu4.4) ...
	Setting up libldap-common (2.4.49+dfsg-2ubuntu1.9) ...
	Setting up libkrb5support0:arm64 (1.17-6ubuntu4.4) ...
	Setting up libsasl2-modules-db:arm64 (2.1.27+dfsg-2ubuntu0.1) ...
	Setting up tzdata (2023c-0ubuntu0.20.04.2) ...
	debconf: unable to initialize frontend: Dialog
	debconf: (TERM is not set, so the dialog frontend is not usable.)
	debconf: falling back to frontend: Readline
	debconf: unable to initialize frontend: Readline
	debconf: (Can't locate Term/ReadLine.pm in @INC (you may need to install the Term::ReadLine module) (@INC contains: /etc/perl /usr/local/lib/aarch64-linux-gnu/perl/5.30.0 /usr/local/share/perl/5.30.0 /usr/lib/aarch64-linux-gnu/perl5/5.30 /usr/share/perl5 /usr/lib/aarch64-linux-gnu/perl/5.30 /usr/share/perl/5.30 /usr/local/lib/site_perl /usr/lib/aarch64-linux-gnu/perl-base) at /usr/share/perl5/Debconf/FrontEnd/Readline.pm line 7.)
	debconf: falling back to frontend: Teletype
	Configuring tzdata
	------------------
	
	Please select the geographic area in which you live. Subsequent configuration
	questions will narrow this down by presenting a list of cities, representing
	the time zones in which they are located.
	
	1. Africa      4. Australia  7. Atlantic  10. Pacific  13. Etc
	2. America     5. Arctic     8. Europe    11. SystemV
	3. Antarctica  6. Asia       9. Indian    12. US
	Geographic area:
	Use of uninitialized value $_[1] in join or string at /usr/share/perl5/Debconf/DbDriver/Stack.pm line 111.
	
	Current default time zone: '/UTC'
	Local time is now:      Fri Jan  5 23:58:26 UTC 2024.
	Universal Time is now:  Fri Jan  5 23:58:26 UTC 2024.
	Run 'dpkg-reconfigure tzdata' if you wish to change it.
	
	Use of uninitialized value $val in substitution (s///) at /usr/share/perl5/Debconf/Format/822.pm line 83, <GEN6> line 4.
	Use of uninitialized value $val in concatenation (.) or string at /usr/share/perl5/Debconf/Format/822.pm line 84, <GEN6> line 4.
	Setting up libuv1:arm64 (1.34.2-1ubuntu1.3) ...
	Setting up make (4.2.1-1.2) ...
	Setting up librtmp1:arm64 (2.4+20151223.gitfa8646d.1-2build1) ...
	Setting up libk5crypto3:arm64 (1.17-6ubuntu4.4) ...
	Setting up libsasl2-2:arm64 (2.1.27+dfsg-2ubuntu0.1) ...
	Setting up libroken18-heimdal:arm64 (7.7.0+dfsg-1ubuntu1.4) ...
	Setting up librhash0:arm64 (1.3.9-1) ...
	Setting up cmake-data (3.16.3-1ubuntu1.20.04.1) ...
	Setting up libkrb5-3:arm64 (1.17-6ubuntu4.4) ...
	Setting up openssl (1.1.1f-1ubuntu2.20) ...
	Setting up publicsuffix (20200303.0012-1) ...
	Setting up libjsoncpp1:arm64 (1.7.4-3.1ubuntu2) ...
	Setting up libheimbase1-heimdal:arm64 (7.7.0+dfsg-1ubuntu1.4) ...
	Setting up gcc-9-base:arm64 (9.4.0-1ubuntu1~20.04.2) ...
	Setting up libicu66:arm64 (66.1-2ubuntu2.1) ...
	Setting up libasn1-8-heimdal:arm64 (7.7.0+dfsg-1ubuntu1.4) ...
	Setting up libhcrypto4-heimdal:arm64 (7.7.0+dfsg-1ubuntu1.4) ...
	Setting up ca-certificates (20230311ubuntu0.20.04.1) ...
	debconf: unable to initialize frontend: Dialog
	debconf: (TERM is not set, so the dialog frontend is not usable.)
	debconf: falling back to frontend: Readline
	debconf: unable to initialize frontend: Readline
	debconf: (Can't locate Term/ReadLine.pm in @INC (you may need to install the Term::ReadLine module) (@INC contains: /etc/perl /usr/local/lib/aarch64-linux-gnu/perl/5.30.0 /usr/local/share/perl/5.30.0 /usr/lib/aarch64-linux-gnu/perl5/5.30 /usr/share/perl5 /usr/lib/aarch64-linux-gnu/perl/5.30 /usr/share/perl/5.30 /usr/local/lib/site_perl /usr/lib/aarch64-linux-gnu/perl-base) at /usr/share/perl5/Debconf/FrontEnd/Readline.pm line 7.)
	debconf: falling back to frontend: Teletype
	Updating certificates in /etc/ssl/certs...
	137 added, 0 removed; done.
	Setting up libasan5:arm64 (9.4.0-1ubuntu1~20.04.2) ...
	Setting up libwind0-heimdal:arm64 (7.7.0+dfsg-1ubuntu1.4) ...
	Setting up libgssapi-krb5-2:arm64 (1.17-6ubuntu4.4) ...
	Setting up libssh-4:arm64 (0.9.3-2ubuntu2.4) ...
	Setting up cpp-9 (9.4.0-1ubuntu1~20.04.2) ...
	Setting up libxml2:arm64 (2.9.10+dfsg-5ubuntu0.20.04.6) ...
	Setting up libarchive13:arm64 (3.4.0-2ubuntu1.2) ...
	Setting up libhx509-5-heimdal:arm64 (7.7.0+dfsg-1ubuntu1.4) ...
	Setting up libgcc-9-dev:arm64 (9.4.0-1ubuntu1~20.04.2) ...
	Setting up cpp (4:9.3.0-1ubuntu2) ...
	Setting up libkrb5-26-heimdal:arm64 (7.7.0+dfsg-1ubuntu1.4) ...
	Setting up gcc-9 (9.4.0-1ubuntu1~20.04.2) ...
	Setting up libheimntlm0-heimdal:arm64 (7.7.0+dfsg-1ubuntu1.4) ...
	Setting up gcc (4:9.3.0-1ubuntu2) ...
	Setting up libgssapi3-heimdal:arm64 (7.7.0+dfsg-1ubuntu1.4) ...
	Setting up libldap-2.4-2:arm64 (2.4.49+dfsg-2ubuntu1.9) ...
	Setting up libcurl4:arm64 (7.68.0-1ubuntu2.21) ...
	Setting up cmake (3.16.3-1ubuntu1.20.04.1) ...
	Processing triggers for libc-bin (2.31-0ubuntu9.14) ...
	Processing triggers for ca-certificates (20230311ubuntu0.20.04.1) ...
	Updating certificates in /etc/ssl/certs...
	0 added, 0 removed; done.
	Running hooks in /etc/ca-certificates/update.d...
	done.
[ 5/22] RUN apt-get install -y liblpsolve55-dev
	Reading package lists...
	Building dependency tree...
	Reading state information...
	The following additional packages will be installed:
	libamd2 libblas-dev libblas3 libbtf1 libcamd2 libccolamd2 libcholmod3
	libcolamd2 libcxsparse3 libgfortran5 libgraphblas3 libklu1 liblapack-dev
	liblapack3 libldl2 libmetis5 libmongoose2 librbio2 libspqr2
	libsuitesparse-dev libsuitesparseconfig5 libumfpack5
	Suggested packages:
	liblapack-doc
	The following NEW packages will be installed:
	libamd2 libblas-dev libblas3 libbtf1 libcamd2 libccolamd2 libcholmod3
	libcolamd2 libcxsparse3 libgfortran5 libgraphblas3 libklu1 liblapack-dev
	liblapack3 libldl2 liblpsolve55-dev libmetis5 libmongoose2 librbio2 libspqr2
	libsuitesparse-dev libsuitesparseconfig5 libumfpack5
	0 upgraded, 23 newly installed, 0 to remove and 0 not upgraded.
	Need to get 11.8 MB of archives.
	After this operation, 132 MB of additional disk space will be used.
	Get:1 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libsuitesparseconfig5 arm64 1:5.7.1+dfsg-2 [9444 B]
	Get:2 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libamd2 arm64 1:5.7.1+dfsg-2 [17.3 kB]
	Get:3 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libblas3 arm64 3.9.0-1build1 [89.1 kB]
	Get:4 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libblas-dev arm64 3.9.0-1build1 [92.7 kB]
	Get:5 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libbtf1 arm64 1:5.7.1+dfsg-2 [10.9 kB]
	Get:6 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libcamd2 arm64 1:5.7.1+dfsg-2 [18.2 kB]
	Get:7 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libccolamd2 arm64 1:5.7.1+dfsg-2 [20.8 kB]
	Get:8 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libcolamd2 arm64 1:5.7.1+dfsg-2 [15.9 kB]
	Get:9 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libgfortran5 arm64 10.5.0-1ubuntu1~20.04 [346 kB]
	Get:10 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 liblapack3 arm64 3.9.0-1build1 [1529 kB]
	Get:11 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libmetis5 arm64 5.1.0.dfsg-5 [140 kB]
	Get:12 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libcholmod3 arm64 1:5.7.1+dfsg-2 [262 kB]
	Get:13 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libcxsparse3 arm64 1:5.7.1+dfsg-2 [57.4 kB]
	Get:14 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libgraphblas3 arm64 1:5.7.1+dfsg-2 [2650 kB]
	Get:15 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libklu1 arm64 1:5.7.1+dfsg-2 [58.9 kB]
	Get:16 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 liblapack-dev arm64 3.9.0-1build1 [2763 kB]
	Get:17 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libldl2 arm64 1:5.7.1+dfsg-2 [10.5 kB]
	Get:18 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libmongoose2 arm64 1:5.7.1+dfsg-2 [27.9 kB]
	Get:19 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libumfpack5 arm64 1:5.7.1+dfsg-2 [181 kB]
	Get:20 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 librbio2 arm64 1:5.7.1+dfsg-2 [22.2 kB]
	Get:21 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libspqr2 arm64 1:5.7.1+dfsg-2 [57.9 kB]
	Get:22 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libsuitesparse-dev arm64 1:5.7.1+dfsg-2 [3083 kB]
	Get:23 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 liblpsolve55-dev arm64 5.5.0.15-4build1 [332 kB]
	debconf: delaying package configuration, since apt-utils is not installed
	Fetched 11.8 MB in 1s (8082 kB/s)
	Selecting previously unselected package libsuitesparseconfig5:arm64.
	(Reading database ...(Reading database ... 5%
(Reading database ... 10%
(Reading database ... 15%
(Reading database ... 20%
(Reading database ... 25%
(Reading database ... 30%
(Reading database ... 35%
(Reading database ... 40%
(Reading database ... 45%
(Reading database ... 50%
(Reading database ... 55%
(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%
(Reading database ... 14934 files and directories currently installed.)
	Preparing to unpack .../00-libsuitesparseconfig5_1%3a5.7.1+dfsg-2_arm64.deb ...
	Unpacking libsuitesparseconfig5:arm64 (1:5.7.1+dfsg-2) ...
	Selecting previously unselected package libamd2:arm64.
	Preparing to unpack .../01-libamd2_1%3a5.7.1+dfsg-2_arm64.deb ...
	Unpacking libamd2:arm64 (1:5.7.1+dfsg-2) ...
	Selecting previously unselected package libblas3:arm64.
	Preparing to unpack .../02-libblas3_3.9.0-1build1_arm64.deb ...
	Unpacking libblas3:arm64 (3.9.0-1build1) ...
	Selecting previously unselected package libblas-dev:arm64.
	Preparing to unpack .../03-libblas-dev_3.9.0-1build1_arm64.deb ...
	Unpacking libblas-dev:arm64 (3.9.0-1build1) ...
	Selecting previously unselected package libbtf1:arm64.
	Preparing to unpack .../04-libbtf1_1%3a5.7.1+dfsg-2_arm64.deb ...
	Unpacking libbtf1:arm64 (1:5.7.1+dfsg-2) ...
	Selecting previously unselected package libcamd2:arm64.
	Preparing to unpack .../05-libcamd2_1%3a5.7.1+dfsg-2_arm64.deb ...
	Unpacking libcamd2:arm64 (1:5.7.1+dfsg-2) ...
	Selecting previously unselected package libccolamd2:arm64.
	Preparing to unpack .../06-libccolamd2_1%3a5.7.1+dfsg-2_arm64.deb ...
	Unpacking libccolamd2:arm64 (1:5.7.1+dfsg-2) ...
	Selecting previously unselected package libcolamd2:arm64.
	Preparing to unpack .../07-libcolamd2_1%3a5.7.1+dfsg-2_arm64.deb ...
	Unpacking libcolamd2:arm64 (1:5.7.1+dfsg-2) ...
	Selecting previously unselected package libgfortran5:arm64.
	Preparing to unpack .../08-libgfortran5_10.5.0-1ubuntu1~20.04_arm64.deb ...
	Unpacking libgfortran5:arm64 (10.5.0-1ubuntu1~20.04) ...
	Selecting previously unselected package liblapack3:arm64.
	Preparing to unpack .../09-liblapack3_3.9.0-1build1_arm64.deb ...
	Unpacking liblapack3:arm64 (3.9.0-1build1) ...
	Selecting previously unselected package libmetis5:arm64.
	Preparing to unpack .../10-libmetis5_5.1.0.dfsg-5_arm64.deb ...
	Unpacking libmetis5:arm64 (5.1.0.dfsg-5) ...
	Selecting previously unselected package libcholmod3:arm64.
	Preparing to unpack .../11-libcholmod3_1%3a5.7.1+dfsg-2_arm64.deb ...
	Unpacking libcholmod3:arm64 (1:5.7.1+dfsg-2) ...
	Selecting previously unselected package libcxsparse3:arm64.
	Preparing to unpack .../12-libcxsparse3_1%3a5.7.1+dfsg-2_arm64.deb ...
	Unpacking libcxsparse3:arm64 (1:5.7.1+dfsg-2) ...
	Selecting previously unselected package libgraphblas3:arm64.
	Preparing to unpack .../13-libgraphblas3_1%3a5.7.1+dfsg-2_arm64.deb ...
	Unpacking libgraphblas3:arm64 (1:5.7.1+dfsg-2) ...
	Selecting previously unselected package libklu1:arm64.
	Preparing to unpack .../14-libklu1_1%3a5.7.1+dfsg-2_arm64.deb ...
	Unpacking libklu1:arm64 (1:5.7.1+dfsg-2) ...
	Selecting previously unselected package liblapack-dev:arm64.
	Preparing to unpack .../15-liblapack-dev_3.9.0-1build1_arm64.deb ...
	Unpacking liblapack-dev:arm64 (3.9.0-1build1) ...
	Selecting previously unselected package libldl2:arm64.
	Preparing to unpack .../16-libldl2_1%3a5.7.1+dfsg-2_arm64.deb ...
	Unpacking libldl2:arm64 (1:5.7.1+dfsg-2) ...
	Selecting previously unselected package libmongoose2:arm64.
	Preparing to unpack .../17-libmongoose2_1%3a5.7.1+dfsg-2_arm64.deb ...
	Unpacking libmongoose2:arm64 (1:5.7.1+dfsg-2) ...
	Selecting previously unselected package libumfpack5:arm64.
	Preparing to unpack .../18-libumfpack5_1%3a5.7.1+dfsg-2_arm64.deb ...
	Unpacking libumfpack5:arm64 (1:5.7.1+dfsg-2) ...
	Selecting previously unselected package librbio2:arm64.
	Preparing to unpack .../19-librbio2_1%3a5.7.1+dfsg-2_arm64.deb ...
	Unpacking librbio2:arm64 (1:5.7.1+dfsg-2) ...
	Selecting previously unselected package libspqr2:arm64.
	Preparing to unpack .../20-libspqr2_1%3a5.7.1+dfsg-2_arm64.deb ...
	Unpacking libspqr2:arm64 (1:5.7.1+dfsg-2) ...
	Selecting previously unselected package libsuitesparse-dev:arm64.
	Preparing to unpack .../21-libsuitesparse-dev_1%3a5.7.1+dfsg-2_arm64.deb ...
	Unpacking libsuitesparse-dev:arm64 (1:5.7.1+dfsg-2) ...
	Selecting previously unselected package liblpsolve55-dev.
	Preparing to unpack .../22-liblpsolve55-dev_5.5.0.15-4build1_arm64.deb ...
	Unpacking liblpsolve55-dev (5.5.0.15-4build1) ...
	Setting up libgraphblas3:arm64 (1:5.7.1+dfsg-2) ...
	Setting up libldl2:arm64 (1:5.7.1+dfsg-2) ...
	Setting up libmetis5:arm64 (5.1.0.dfsg-5) ...
	Setting up libbtf1:arm64 (1:5.7.1+dfsg-2) ...
	Setting up libblas3:arm64 (3.9.0-1build1) ...
	update-alternatives: using /usr/lib/aarch64-linux-gnu/blas/libblas.so.3 to provide /usr/lib/aarch64-linux-gnu/libblas.so.3 (libblas.so.3-aarch64-linux-gnu) in auto mode
	Setting up libgfortran5:arm64 (10.5.0-1ubuntu1~20.04) ...
	Setting up libcxsparse3:arm64 (1:5.7.1+dfsg-2) ...
	Setting up libsuitesparseconfig5:arm64 (1:5.7.1+dfsg-2) ...
	Setting up libblas-dev:arm64 (3.9.0-1build1) ...
	update-alternatives: using /usr/lib/aarch64-linux-gnu/blas/libblas.so to provide /usr/lib/aarch64-linux-gnu/libblas.so (libblas.so-aarch64-linux-gnu) in auto mode
	Setting up librbio2:arm64 (1:5.7.1+dfsg-2) ...
	Setting up libamd2:arm64 (1:5.7.1+dfsg-2) ...
	Setting up liblapack3:arm64 (3.9.0-1build1) ...
	update-alternatives: using /usr/lib/aarch64-linux-gnu/lapack/liblapack.so.3 to provide /usr/lib/aarch64-linux-gnu/liblapack.so.3 (liblapack.so.3-aarch64-linux-gnu) in auto mode
	Setting up libcolamd2:arm64 (1:5.7.1+dfsg-2) ...
	Setting up libcamd2:arm64 (1:5.7.1+dfsg-2) ...
	Setting up libmongoose2:arm64 (1:5.7.1+dfsg-2) ...
	Setting up liblapack-dev:arm64 (3.9.0-1build1) ...
	update-alternatives: using /usr/lib/aarch64-linux-gnu/lapack/liblapack.so to provide /usr/lib/aarch64-linux-gnu/liblapack.so (liblapack.so-aarch64-linux-gnu) in auto mode
	Setting up libklu1:arm64 (1:5.7.1+dfsg-2) ...
	Setting up libccolamd2:arm64 (1:5.7.1+dfsg-2) ...
	Setting up libcholmod3:arm64 (1:5.7.1+dfsg-2) ...
	Setting up libspqr2:arm64 (1:5.7.1+dfsg-2) ...
	Setting up libumfpack5:arm64 (1:5.7.1+dfsg-2) ...
	Setting up libsuitesparse-dev:arm64 (1:5.7.1+dfsg-2) ...
	Setting up liblpsolve55-dev (5.5.0.15-4build1) ...
	Processing triggers for libc-bin (2.31-0ubuntu9.14) ...
[ 6/22] RUN apt-get install -y lp-solve
	Reading package lists...
	Building dependency tree...
	Reading state information...
	The following NEW packages will be installed:
	lp-solve
	0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.
	Need to get 236 kB of archives.
	After this operation, 595 kB of additional disk space will be used.
	Get:1 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 lp-solve arm64 5.5.0.15-4build1 [236 kB]
	debconf: delaying package configuration, since apt-utils is not installed
	Fetched 236 kB in 0s (940 kB/s)
	Selecting previously unselected package lp-solve.
	(Reading database ...(Reading database ... 5%
(Reading database ... 10%
(Reading database ... 15%
(Reading database ... 20%
(Reading database ... 25%
(Reading database ... 30%
(Reading database ... 35%
(Reading database ... 40%
(Reading database ... 45%
(Reading database ... 50%
(Reading database ... 55%
(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%
(Reading database ... 15202 files and directories currently installed.)
	Preparing to unpack .../lp-solve_5.5.0.15-4build1_arm64.deb ...
	Unpacking lp-solve (5.5.0.15-4build1) ...
	Setting up lp-solve (5.5.0.15-4build1) ...
[ 7/22] RUN apt-get install -y libeigen3-dev
	Reading package lists...
	Building dependency tree...
	Reading state information...
	The following additional packages will be installed:
	libdpkg-perl libfile-fcntllock-perl libgdbm-compat4 libgdbm6 libglib2.0-0
	libglib2.0-data liblocale-gettext-perl libperl5.30 netbase perl
	perl-modules-5.30 pkg-config shared-mime-info xdg-user-dirs xz-utils
	Suggested packages:
	debian-keyring gnupg | gnupg2 patch git bzr libeigen3-doc libmpfrc++-dev
	gdbm-l10n perl-doc libterm-readline-gnu-perl | libterm-readline-perl-perl
	libb-debug-perl liblocale-codes-perl dpkg-dev
	The following NEW packages will be installed:
	libdpkg-perl libeigen3-dev libfile-fcntllock-perl libgdbm-compat4 libgdbm6
	libglib2.0-0 libglib2.0-data liblocale-gettext-perl libperl5.30 netbase perl
	perl-modules-5.30 pkg-config shared-mime-info xdg-user-dirs xz-utils
	0 upgraded, 16 newly installed, 0 to remove and 0 not upgraded.
	Need to get 9674 kB of archives.
	After this operation, 63.4 MB of additional disk space will be used.
	Get:1 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 liblocale-gettext-perl arm64 1.07-4 [16.7 kB]
	Get:2 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 perl-modules-5.30 all 5.30.0-9ubuntu0.5 [2739 kB]
	Get:3 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libgdbm6 arm64 1.18.1-5 [26.4 kB]
	Get:4 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libgdbm-compat4 arm64 1.18.1-5 [6040 B]
	Get:5 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libperl5.30 arm64 5.30.0-9ubuntu0.5 [3763 kB]
	Get:6 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 perl arm64 5.30.0-9ubuntu0.5 [224 kB]
	Get:7 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libglib2.0-0 arm64 2.64.6-1~ubuntu20.04.6 [1199 kB]
	Get:8 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libglib2.0-data all 2.64.6-1~ubuntu20.04.6 [6032 B]
	Get:9 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 netbase all 6.1 [13.1 kB]
	Get:10 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 shared-mime-info arm64 1.15-1 [429 kB]
	Get:11 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 xdg-user-dirs arm64 0.17-2ubuntu1 [47.6 kB]
	Get:12 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 xz-utils arm64 5.2.4-1ubuntu1.1 [81.4 kB]
	Get:13 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libdpkg-perl all 1.19.7ubuntu3.2 [231 kB]
	Get:14 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libfile-fcntllock-perl arm64 0.22-3build4 [33.0 kB]
	Get:15 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 pkg-config arm64 0.29.1-0ubuntu4 [43.8 kB]
	Get:16 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libeigen3-dev all 3.3.7-2 [815 kB]
	debconf: delaying package configuration, since apt-utils is not installed
	Fetched 9674 kB in 1s (8123 kB/s)
	Selecting previously unselected package liblocale-gettext-perl.
	(Reading database ...(Reading database ... 5%
(Reading database ... 10%
(Reading database ... 15%
(Reading database ... 20%
(Reading database ... 25%
(Reading database ... 30%
(Reading database ... 35%
(Reading database ... 40%
(Reading database ... 45%
(Reading database ... 50%
(Reading database ... 55%
(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%
(Reading database ... 15213 files and directories currently installed.)
	Preparing to unpack .../00-liblocale-gettext-perl_1.07-4_arm64.deb ...
	Unpacking liblocale-gettext-perl (1.07-4) ...
	Selecting previously unselected package perl-modules-5.30.
	Preparing to unpack .../01-perl-modules-5.30_5.30.0-9ubuntu0.5_all.deb ...
	Unpacking perl-modules-5.30 (5.30.0-9ubuntu0.5) ...
	Selecting previously unselected package libgdbm6:arm64.
	Preparing to unpack .../02-libgdbm6_1.18.1-5_arm64.deb ...
	Unpacking libgdbm6:arm64 (1.18.1-5) ...
	Selecting previously unselected package libgdbm-compat4:arm64.
	Preparing to unpack .../03-libgdbm-compat4_1.18.1-5_arm64.deb ...
	Unpacking libgdbm-compat4:arm64 (1.18.1-5) ...
	Selecting previously unselected package libperl5.30:arm64.
	Preparing to unpack .../04-libperl5.30_5.30.0-9ubuntu0.5_arm64.deb ...
	Unpacking libperl5.30:arm64 (5.30.0-9ubuntu0.5) ...
	Selecting previously unselected package perl.
	Preparing to unpack .../05-perl_5.30.0-9ubuntu0.5_arm64.deb ...
	Unpacking perl (5.30.0-9ubuntu0.5) ...
	Selecting previously unselected package libglib2.0-0:arm64.
	Preparing to unpack .../06-libglib2.0-0_2.64.6-1~ubuntu20.04.6_arm64.deb ...
	Unpacking libglib2.0-0:arm64 (2.64.6-1~ubuntu20.04.6) ...
	Selecting previously unselected package libglib2.0-data.
	Preparing to unpack .../07-libglib2.0-data_2.64.6-1~ubuntu20.04.6_all.deb ...
	Unpacking libglib2.0-data (2.64.6-1~ubuntu20.04.6) ...
	Selecting previously unselected package netbase.
	Preparing to unpack .../08-netbase_6.1_all.deb ...
	Unpacking netbase (6.1) ...
	Selecting previously unselected package shared-mime-info.
	Preparing to unpack .../09-shared-mime-info_1.15-1_arm64.deb ...
	Unpacking shared-mime-info (1.15-1) ...
	Selecting previously unselected package xdg-user-dirs.
	Preparing to unpack .../10-xdg-user-dirs_0.17-2ubuntu1_arm64.deb ...
	Unpacking xdg-user-dirs (0.17-2ubuntu1) ...
	Selecting previously unselected package xz-utils.
	Preparing to unpack .../11-xz-utils_5.2.4-1ubuntu1.1_arm64.deb ...
	Unpacking xz-utils (5.2.4-1ubuntu1.1) ...
	Selecting previously unselected package libdpkg-perl.
	Preparing to unpack .../12-libdpkg-perl_1.19.7ubuntu3.2_all.deb ...
	Unpacking libdpkg-perl (1.19.7ubuntu3.2) ...
	Selecting previously unselected package libfile-fcntllock-perl.
	Preparing to unpack .../13-libfile-fcntllock-perl_0.22-3build4_arm64.deb ...
	Unpacking libfile-fcntllock-perl (0.22-3build4) ...
	Selecting previously unselected package pkg-config.
	Preparing to unpack .../14-pkg-config_0.29.1-0ubuntu4_arm64.deb ...
	Unpacking pkg-config (0.29.1-0ubuntu4) ...
	Selecting previously unselected package libeigen3-dev.
	Preparing to unpack .../15-libeigen3-dev_3.3.7-2_all.deb ...
	Unpacking libeigen3-dev (3.3.7-2) ...
	Setting up perl-modules-5.30 (5.30.0-9ubuntu0.5) ...
	Setting up xdg-user-dirs (0.17-2ubuntu1) ...
	Setting up libglib2.0-0:arm64 (2.64.6-1~ubuntu20.04.6) ...
	No schema files found: doing nothing.
	Setting up libglib2.0-data (2.64.6-1~ubuntu20.04.6) ...
	Setting up xz-utils (5.2.4-1ubuntu1.1) ...
	update-alternatives: using /usr/bin/xz to provide /usr/bin/lzma (lzma) in auto mode
	update-alternatives: warning: skip creation of /usr/share/man/man1/lzma.1.gz because associated file /usr/share/man/man1/xz.1.gz (of link group lzma) doesn't exist
	update-alternatives: warning: skip creation of /usr/share/man/man1/unlzma.1.gz because associated file /usr/share/man/man1/unxz.1.gz (of link group lzma) doesn't exist
	update-alternatives: warning: skip creation of /usr/share/man/man1/lzcat.1.gz because associated file /usr/share/man/man1/xzcat.1.gz (of link group lzma) doesn't exist
	update-alternatives: warning: skip creation of /usr/share/man/man1/lzmore.1.gz because associated file /usr/share/man/man1/xzmore.1.gz (of link group lzma) doesn't exist
	update-alternatives: warning: skip creation of /usr/share/man/man1/lzless.1.gz because associated file /usr/share/man/man1/xzless.1.gz (of link group lzma) doesn't exist
	update-alternatives: warning: skip creation of /usr/share/man/man1/lzdiff.1.gz because associated file /usr/share/man/man1/xzdiff.1.gz (of link group lzma) doesn't exist
	update-alternatives: warning: skip creation of /usr/share/man/man1/lzcmp.1.gz because associated file /usr/share/man/man1/xzcmp.1.gz (of link group lzma) doesn't exist
	update-alternatives: warning: skip creation of /usr/share/man/man1/lzgrep.1.gz because associated file /usr/share/man/man1/xzgrep.1.gz (of link group lzma) doesn't exist
	update-alternatives: warning: skip creation of /usr/share/man/man1/lzegrep.1.gz because associated file /usr/share/man/man1/xzegrep.1.gz (of link group lzma) doesn't exist
	update-alternatives: warning: skip creation of /usr/share/man/man1/lzfgrep.1.gz because associated file /usr/share/man/man1/xzfgrep.1.gz (of link group lzma) doesn't exist
	Setting up shared-mime-info (1.15-1) ...
	Setting up netbase (6.1) ...
	Setting up liblocale-gettext-perl (1.07-4) ...
	Setting up libgdbm6:arm64 (1.18.1-5) ...
	Setting up libgdbm-compat4:arm64 (1.18.1-5) ...
	Setting up libperl5.30:arm64 (5.30.0-9ubuntu0.5) ...
	Setting up perl (5.30.0-9ubuntu0.5) ...
	Setting up libdpkg-perl (1.19.7ubuntu3.2) ...
	Setting up libfile-fcntllock-perl (0.22-3build4) ...
	Setting up pkg-config (0.29.1-0ubuntu4) ...
	Setting up libeigen3-dev (3.3.7-2) ...
	Processing triggers for libc-bin (2.31-0ubuntu9.14) ...
[ 8/22] RUN apt-get install -y libboost-all-dev
	Reading package lists...
	Building dependency tree...
	Reading state information...
	The following additional packages will be installed:
	autoconf automake autotools-dev cpp-8 file gcc-8 gcc-8-base gfortran
	gfortran-8 gfortran-9 ibverbs-providers icu-devtools libboost-atomic-dev
	libboost-atomic1.71-dev libboost-atomic1.71.0 libboost-chrono-dev
	libboost-chrono1.71-dev libboost-chrono1.71.0 libboost-container-dev
	libboost-container1.71-dev libboost-container1.71.0 libboost-context-dev
	libboost-context1.71-dev libboost-context1.71.0 libboost-coroutine-dev
	libboost-coroutine1.71-dev libboost-coroutine1.71.0 libboost-date-time-dev
	libboost-date-time1.71-dev libboost-date-time1.71.0 libboost-dev
	libboost-exception-dev libboost-exception1.71-dev libboost-fiber-dev
	libboost-fiber1.71-dev libboost-fiber1.71.0 libboost-filesystem-dev
	libboost-filesystem1.71-dev libboost-filesystem1.71.0 libboost-graph-dev
	libboost-graph-parallel-dev libboost-graph-parallel1.71-dev
	libboost-graph-parallel1.71.0 libboost-graph1.71-dev libboost-graph1.71.0
	libboost-iostreams-dev libboost-iostreams1.71-dev libboost-iostreams1.71.0
	libboost-locale-dev libboost-locale1.71-dev libboost-locale1.71.0
	libboost-log-dev libboost-log1.71-dev libboost-log1.71.0 libboost-math-dev
	libboost-math1.71-dev libboost-math1.71.0 libboost-mpi-dev
	libboost-mpi-python-dev libboost-mpi-python1.71-dev
	libboost-mpi-python1.71.0 libboost-mpi1.71-dev libboost-mpi1.71.0
	libboost-numpy-dev libboost-numpy1.71-dev libboost-numpy1.71.0
	libboost-program-options-dev libboost-program-options1.71-dev
	libboost-program-options1.71.0 libboost-python-dev libboost-python1.71-dev
	libboost-python1.71.0 libboost-random-dev libboost-random1.71-dev
	libboost-random1.71.0 libboost-regex-dev libboost-regex1.71-dev
	libboost-regex1.71.0 libboost-serialization-dev
	libboost-serialization1.71-dev libboost-serialization1.71.0
	libboost-stacktrace-dev libboost-stacktrace1.71-dev
	libboost-stacktrace1.71.0 libboost-system-dev libboost-system1.71-dev
	libboost-system1.71.0 libboost-test-dev libboost-test1.71-dev
	libboost-test1.71.0 libboost-thread-dev libboost-thread1.71-dev
	libboost-thread1.71.0 libboost-timer-dev libboost-timer1.71-dev
	libboost-timer1.71.0 libboost-tools-dev libboost-type-erasure-dev
	libboost-type-erasure1.71-dev libboost-type-erasure1.71.0 libboost-wave-dev
	libboost-wave1.71-dev libboost-wave1.71.0 libboost1.71-dev
	libboost1.71-tools-dev libbsd0 libcaf-openmpi-3 libcbor0.6 libcoarrays-dev
	libcoarrays-openmpi-dev libedit2 libevent-2.1-7 libevent-core-2.1-7
	libevent-dev libevent-extra-2.1-7 libevent-openssl-2.1-7
	libevent-pthreads-2.1-7 libexpat1-dev libfido2-1 libgcc-8-dev
	libgfortran-8-dev libgfortran-9-dev libhwloc-dev libhwloc-plugins libhwloc15
	libibverbs-dev libibverbs1 libicu-dev libltdl-dev libltdl7 libmagic-mgc
	libmagic1 libmpdec2 libnl-3-200 libnl-3-dev libnl-route-3-200
	libnl-route-3-dev libnuma-dev libnuma1 libopenmpi-dev libopenmpi3
	libpciaccess0 libpmix2 libpython3-dev libpython3-stdlib libpython3.8
	libpython3.8-dev libpython3.8-minimal libpython3.8-stdlib libreadline8
	libsigsegv2 libtool libx11-6 libx11-data libxau6 libxcb1 libxdmcp6 libxext6
	libxmuu1 libxnvctrl0 m4 mime-support mpi-default-bin mpi-default-dev
	ocl-icd-libopencl1 openmpi-bin openmpi-common openssh-client python3
	python3-dev python3-distutils python3-lib2to3 python3-minimal python3.8
	python3.8-dev python3.8-minimal readline-common xauth zlib1g-dev
	Suggested packages:
	autoconf-archive gnu-standards autoconf-doc gettext gcc-8-locales gcc-8-doc
	gfortran-doc gfortran-8-doc gfortran-9-doc libboost-doc graphviz
	libboost1.71-doc gccxml libboost-contract1.71-dev libmpfrc++-dev libntl-dev
	xsltproc doxygen docbook-xml docbook-xsl default-jdk fop icu-doc libtool-doc
	openmpi-doc pciutils gcj-jdk m4-doc opencl-icd keychain libpam-ssh
	monkeysphere ssh-askpass python3-doc python3-tk python3-venv python3.8-venv
	python3.8-doc binfmt-support readline-doc
	The following NEW packages will be installed:
	autoconf automake autotools-dev cpp-8 file gcc-8 gcc-8-base gfortran
	gfortran-8 gfortran-9 ibverbs-providers icu-devtools libboost-all-dev
	libboost-atomic-dev libboost-atomic1.71-dev libboost-atomic1.71.0
	libboost-chrono-dev libboost-chrono1.71-dev libboost-chrono1.71.0
	libboost-container-dev libboost-container1.71-dev libboost-container1.71.0
	libboost-context-dev libboost-context1.71-dev libboost-context1.71.0
	libboost-coroutine-dev libboost-coroutine1.71-dev libboost-coroutine1.71.0
	libboost-date-time-dev libboost-date-time1.71-dev libboost-date-time1.71.0
	libboost-dev libboost-exception-dev libboost-exception1.71-dev
	libboost-fiber-dev libboost-fiber1.71-dev libboost-fiber1.71.0
	libboost-filesystem-dev libboost-filesystem1.71-dev
	libboost-filesystem1.71.0 libboost-graph-dev libboost-graph-parallel-dev
	libboost-graph-parallel1.71-dev libboost-graph-parallel1.71.0
	libboost-graph1.71-dev libboost-graph1.71.0 libboost-iostreams-dev
	libboost-iostreams1.71-dev libboost-iostreams1.71.0 libboost-locale-dev
	libboost-locale1.71-dev libboost-locale1.71.0 libboost-log-dev
	libboost-log1.71-dev libboost-log1.71.0 libboost-math-dev
	libboost-math1.71-dev libboost-math1.71.0 libboost-mpi-dev
	libboost-mpi-python-dev libboost-mpi-python1.71-dev
	libboost-mpi-python1.71.0 libboost-mpi1.71-dev libboost-mpi1.71.0
	libboost-numpy-dev libboost-numpy1.71-dev libboost-numpy1.71.0
	libboost-program-options-dev libboost-program-options1.71-dev
	libboost-program-options1.71.0 libboost-python-dev libboost-python1.71-dev
	libboost-python1.71.0 libboost-random-dev libboost-random1.71-dev
	libboost-random1.71.0 libboost-regex-dev libboost-regex1.71-dev
	libboost-regex1.71.0 libboost-serialization-dev
	libboost-serialization1.71-dev libboost-serialization1.71.0
	libboost-stacktrace-dev libboost-stacktrace1.71-dev
	libboost-stacktrace1.71.0 libboost-system-dev libboost-system1.71-dev
	libboost-system1.71.0 libboost-test-dev libboost-test1.71-dev
	libboost-test1.71.0 libboost-thread-dev libboost-thread1.71-dev
	libboost-thread1.71.0 libboost-timer-dev libboost-timer1.71-dev
	libboost-timer1.71.0 libboost-tools-dev libboost-type-erasure-dev
	libboost-type-erasure1.71-dev libboost-type-erasure1.71.0 libboost-wave-dev
	libboost-wave1.71-dev libboost-wave1.71.0 libboost1.71-dev
	libboost1.71-tools-dev libbsd0 libcaf-openmpi-3 libcbor0.6 libcoarrays-dev
	libcoarrays-openmpi-dev libedit2 libevent-2.1-7 libevent-core-2.1-7
	libevent-dev libevent-extra-2.1-7 libevent-openssl-2.1-7
	libevent-pthreads-2.1-7 libexpat1-dev libfido2-1 libgcc-8-dev
	libgfortran-8-dev libgfortran-9-dev libhwloc-dev libhwloc-plugins libhwloc15
	libibverbs-dev libibverbs1 libicu-dev libltdl-dev libltdl7 libmagic-mgc
	libmagic1 libmpdec2 libnl-3-200 libnl-3-dev libnl-route-3-200
	libnl-route-3-dev libnuma-dev libnuma1 libopenmpi-dev libopenmpi3
	libpciaccess0 libpmix2 libpython3-dev libpython3-stdlib libpython3.8
	libpython3.8-dev libpython3.8-minimal libpython3.8-stdlib libreadline8
	libsigsegv2 libtool libx11-6 libx11-data libxau6 libxcb1 libxdmcp6 libxext6
	libxmuu1 libxnvctrl0 m4 mime-support mpi-default-bin mpi-default-dev
	ocl-icd-libopencl1 openmpi-bin openmpi-common openssh-client python3
	python3-dev python3-distutils python3-lib2to3 python3-minimal python3.8
	python3.8-dev python3.8-minimal readline-common xauth zlib1g-dev
	0 upgraded, 180 newly installed, 0 to remove and 0 not upgraded.
	Need to get 89.3 MB of archives.
	After this operation, 560 MB of additional disk space will be used.
	Get:1 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libpython3.8-minimal arm64 3.8.10-0ubuntu1~20.04.9 [714 kB]
	Get:2 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 python3.8-minimal arm64 3.8.10-0ubuntu1~20.04.9 [1831 kB]
	Get:3 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 python3-minimal arm64 3.8.2-0ubuntu2 [23.6 kB]
	Get:4 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 mime-support all 3.64ubuntu1 [30.6 kB]
	Get:5 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libmpdec2 arm64 2.4.2-3 [79.6 kB]
	Get:6 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 readline-common all 8.0-4 [53.5 kB]
	Get:7 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libreadline8 arm64 8.0-4 [123 kB]
	Get:8 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libpython3.8-stdlib arm64 3.8.10-0ubuntu1~20.04.9 [1649 kB]
	Get:9 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 python3.8 arm64 3.8.10-0ubuntu1~20.04.9 [387 kB]
	Get:10 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libpython3-stdlib arm64 3.8.2-0ubuntu2 [7068 B]
	Get:11 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 python3 arm64 3.8.2-0ubuntu2 [47.6 kB]
	Get:12 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libmagic-mgc arm64 1:5.38-4 [218 kB]
	Get:13 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libmagic1 arm64 1:5.38-4 [71.7 kB]
	Get:14 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 file arm64 1:5.38-4 [23.3 kB]
	Get:15 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libbsd0 arm64 0.10.0-1 [43.7 kB]
	Get:16 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libcbor0.6 arm64 0.6.0-0ubuntu1 [20.5 kB]
	Get:17 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libedit2 arm64 3.1-20191231-1 [82.7 kB]
	Get:18 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libfido2-1 arm64 1.3.1-1ubuntu2 [45.1 kB]
	Get:19 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libnuma1 arm64 2.0.12-1 [20.5 kB]
	Get:20 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libxau6 arm64 1:1.0.9-0ubuntu1 [7356 B]
	Get:21 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libxdmcp6 arm64 1:1.1.3-0ubuntu1 [10.3 kB]
	Get:22 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libxcb1 arm64 1.14-2 [43.0 kB]
	Get:23 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libx11-data all 2:1.6.9-2ubuntu1.6 [114 kB]
	Get:24 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libx11-6 arm64 2:1.6.9-2ubuntu1.6 [554 kB]
	Get:25 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libxext6 arm64 2:1.3.4-0ubuntu1 [27.7 kB]
	Get:26 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libxmuu1 arm64 2:1.1.3-0ubuntu1 [9772 B]
	Get:27 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 openssh-client arm64 1:8.2p1-4ubuntu0.11 [636 kB]
	Get:28 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 xauth arm64 1:1.1-0ubuntu1 [24.1 kB]
	Get:29 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libsigsegv2 arm64 2.12-2 [13.3 kB]
	Get:30 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 m4 arm64 1.4.18-4 [194 kB]
	Get:31 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 autoconf all 2.69-11.1 [321 kB]
	Get:32 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 autotools-dev all 20180224.1 [39.6 kB]
	Get:33 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 automake all 1:1.16.1-4ubuntu6 [522 kB]
	Get:34 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 gcc-8-base arm64 8.4.0-3ubuntu2 [18.8 kB]
	Get:35 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 cpp-8 arm64 8.4.0-3ubuntu2 [7412 kB]
	Get:36 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libgcc-8-dev arm64 8.4.0-3ubuntu2 [859 kB]
	Get:37 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 gcc-8 arm64 8.4.0-3ubuntu2 [8255 kB]
	Get:38 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libgfortran-9-dev arm64 9.4.0-1ubuntu1~20.04.2 [390 kB]
	Get:39 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 gfortran-9 arm64 9.4.0-1ubuntu1~20.04.2 [6395 kB]
	Get:40 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 gfortran arm64 4:9.3.0-1ubuntu2 [1360 B]
	Get:41 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libgfortran-8-dev arm64 8.4.0-3ubuntu2 [358 kB]
	Get:42 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 gfortran-8 arm64 8.4.0-3ubuntu2 [7889 kB]
	Get:43 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libnl-3-200 arm64 3.4.0-1ubuntu0.1 [51.2 kB]
	Get:44 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libnl-route-3-200 arm64 3.4.0-1ubuntu0.1 [137 kB]
	Get:45 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libibverbs1 arm64 28.0-1ubuntu1 [50.7 kB]
	Get:46 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 ibverbs-providers arm64 28.0-1ubuntu1 [217 kB]
	Get:47 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 icu-devtools arm64 66.1-2ubuntu2.1 [176 kB]
	Get:48 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost1.71-dev arm64 1.71.0-6ubuntu6 [9068 kB]
	Get:49 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost-dev arm64 1.71.0.0ubuntu2 [3596 B]
	Get:50 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost1.71-tools-dev arm64 1.71.0-6ubuntu6 [1305 kB]
	Get:51 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-tools-dev arm64 1.71.0.0ubuntu2 [3560 B]
	Get:52 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost-atomic1.71.0 arm64 1.71.0-6ubuntu6 [206 kB]
	Get:53 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost-atomic1.71-dev arm64 1.71.0-6ubuntu6 [205 kB]
	Get:54 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-atomic-dev arm64 1.71.0.0ubuntu2 [3704 B]
	Get:55 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost-chrono1.71.0 arm64 1.71.0-6ubuntu6 [216 kB]
	Get:56 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost-chrono1.71-dev arm64 1.71.0-6ubuntu6 [225 kB]
	Get:57 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-chrono-dev arm64 1.71.0.0ubuntu2 [4016 B]
	Get:58 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-container1.71.0 arm64 1.71.0-6ubuntu6 [233 kB]
	Get:59 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-container1.71-dev arm64 1.71.0-6ubuntu6 [237 kB]
	Get:60 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-container-dev arm64 1.71.0.0ubuntu2 [3868 B]
	Get:61 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost-date-time1.71.0 arm64 1.71.0-6ubuntu6 [218 kB]
	Get:62 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost-serialization1.71.0 arm64 1.71.0-6ubuntu6 [292 kB]
	Get:63 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost-serialization1.71-dev arm64 1.71.0-6ubuntu6 [341 kB]
	Get:64 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost-date-time1.71-dev arm64 1.71.0-6ubuntu6 [229 kB]
	Get:65 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost-system1.71.0 arm64 1.71.0-6ubuntu6 [205 kB]
	Get:66 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost-system1.71-dev arm64 1.71.0-6ubuntu6 [205 kB]
	Get:67 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost-thread1.71.0 arm64 1.71.0-6ubuntu6 [245 kB]
	Get:68 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost-thread1.71-dev arm64 1.71.0-6ubuntu6 [258 kB]
	Get:69 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost-context1.71.0 arm64 1.71.0-6ubuntu6 [207 kB]
	Get:70 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-context1.71-dev arm64 1.71.0-6ubuntu6 [208 kB]
	Get:71 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-context-dev arm64 1.71.0.0ubuntu2 [3584 B]
	Get:72 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-coroutine1.71.0 arm64 1.71.0-6ubuntu6 [218 kB]
	Get:73 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-coroutine1.71-dev arm64 1.71.0-6ubuntu6 [228 kB]
	Get:74 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-coroutine-dev arm64 1.71.0.0ubuntu2 [3660 B]
	Get:75 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-date-time-dev arm64 1.71.0.0ubuntu2 [3396 B]
	Get:76 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-exception1.71-dev arm64 1.71.0-6ubuntu6 [203 kB]
	Get:77 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-exception-dev arm64 1.71.0.0ubuntu2 [3384 B]
	Get:78 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost-filesystem1.71.0 arm64 1.71.0-6ubuntu6 [238 kB]
	Get:79 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost-filesystem1.71-dev arm64 1.71.0-6ubuntu6 [257 kB]
	Get:80 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-fiber1.71.0 arm64 1.71.0-6ubuntu6 [225 kB]
	Get:81 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-fiber1.71-dev arm64 1.71.0-6ubuntu6 [237 kB]
	Get:82 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-fiber-dev arm64 1.71.0.0ubuntu2 [3824 B]
	Get:83 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost-filesystem-dev arm64 1.71.0.0ubuntu2 [3420 B]
	Get:84 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-regex1.71.0 arm64 1.71.0-6ubuntu6 [437 kB]
	Get:85 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-graph1.71.0 arm64 1.71.0-6ubuntu6 [288 kB]
	Get:86 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libicu-dev arm64 66.1-2ubuntu2.1 [9320 kB]
	Get:87 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-regex1.71-dev arm64 1.71.0-6ubuntu6 [533 kB]
	Get:88 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-test1.71.0 arm64 1.71.0-6ubuntu6 [407 kB]
	Get:89 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-test1.71-dev arm64 1.71.0-6ubuntu6 [502 kB]
	Get:90 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-graph1.71-dev arm64 1.71.0-6ubuntu6 [315 kB]
	Get:91 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-graph-dev arm64 1.71.0.0ubuntu2 [3500 B]
	Get:92 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libevent-2.1-7 arm64 2.1.11-stable-1 [125 kB]
	Get:93 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libevent-core-2.1-7 arm64 2.1.11-stable-1 [82.2 kB]
	Get:94 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libevent-pthreads-2.1-7 arm64 2.1.11-stable-1 [7292 B]
	Get:95 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libltdl7 arm64 2.4.6-14 [37.5 kB]
	Get:96 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libhwloc15 arm64 2.1.0+dfsg-4 [116 kB]
	Get:97 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libpciaccess0 arm64 0.16-0ubuntu1 [17.1 kB]
	Get:98 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libxnvctrl0 arm64 470.57.01-0ubuntu0.20.04.3 [10.6 kB]
	Get:99 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 ocl-icd-libopencl1 arm64 2.2.11-1ubuntu1 [29.2 kB]
	Get:100 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libhwloc-plugins arm64 2.1.0+dfsg-4 [13.8 kB]
	Get:101 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libpmix2 arm64 3.1.5-1 [404 kB]
	Get:102 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libopenmpi3 arm64 4.0.3-0ubuntu1 [1811 kB]
	Get:103 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-mpi1.71.0 arm64 1.71.0-6ubuntu6 [253 kB]
	Get:104 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-graph-parallel1.71.0 arm64 1.71.0-6ubuntu6 [262 kB]
	Get:105 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-graph-parallel1.71-dev arm64 1.71.0-6ubuntu6 [271 kB]
	Get:106 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-graph-parallel-dev arm64 1.71.0.0ubuntu2 [3520 B]
	Get:107 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost-iostreams1.71.0 arm64 1.71.0-6ubuntu6 [234 kB]
	Get:108 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-iostreams1.71-dev arm64 1.71.0-6ubuntu6 [247 kB]
	Get:109 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-iostreams-dev arm64 1.71.0.0ubuntu2 [3380 B]
	Get:110 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost-locale1.71.0 arm64 1.71.0-6ubuntu6 [400 kB]
	Get:111 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-locale1.71-dev arm64 1.71.0-6ubuntu6 [536 kB]
	Get:112 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-locale-dev arm64 1.71.0.0ubuntu2 [3708 B]
	Get:113 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-log1.71.0 arm64 1.71.0-6ubuntu6 [566 kB]
	Get:114 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-log1.71-dev arm64 1.71.0-6ubuntu6 [824 kB]
	Get:115 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-log-dev arm64 1.71.0.0ubuntu2 [3592 B]
	Get:116 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-math1.71.0 arm64 1.71.0-6ubuntu6 [444 kB]
	Get:117 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-math1.71-dev arm64 1.71.0-6ubuntu6 [676 kB]
	Get:118 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-math-dev arm64 1.71.0.0ubuntu2 [3580 B]
	Get:119 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 openmpi-common all 4.0.3-0ubuntu1 [151 kB]
	Get:120 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libnl-3-dev arm64 3.4.0-1ubuntu0.1 [93.2 kB]
	Get:121 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libnl-route-3-dev arm64 3.4.0-1ubuntu0.1 [164 kB]
	Get:122 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libibverbs-dev arm64 28.0-1ubuntu1 [439 kB]
	Get:123 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libnuma-dev arm64 2.0.12-1 [33.3 kB]
	Get:124 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libltdl-dev arm64 2.4.6-14 [162 kB]
	Get:125 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libhwloc-dev arm64 2.1.0+dfsg-4 [191 kB]
	Get:126 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libevent-extra-2.1-7 arm64 2.1.11-stable-1 [54.2 kB]
	Get:127 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libevent-openssl-2.1-7 arm64 2.1.11-stable-1 [13.2 kB]
	Get:128 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libevent-dev arm64 2.1.11-stable-1 [258 kB]
	Get:129 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 openmpi-bin arm64 4.0.3-0ubuntu1 [66.6 kB]
	Get:130 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libopenmpi-dev arm64 4.0.3-0ubuntu1 [798 kB]
	Get:131 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 mpi-default-dev arm64 1.13 [3748 B]
	Get:132 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-mpi1.71-dev arm64 1.71.0-6ubuntu6 [387 kB]
	Get:133 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-mpi-dev arm64 1.71.0.0ubuntu2 [3480 B]
	Get:134 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost-python1.71.0 arm64 1.71.0-6ubuntu6 [269 kB]
	Get:135 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 mpi-default-bin arm64 1.13 [2968 B]
	Get:136 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-mpi-python1.71.0 arm64 1.71.0-6ubuntu6 [335 kB]
	Get:137 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-mpi-python1.71-dev arm64 1.71.0-6ubuntu6 [224 kB]
	Get:138 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-mpi-python-dev arm64 1.71.0.0ubuntu2 [3512 B]
	Get:139 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-numpy1.71.0 arm64 1.71.0-6ubuntu6 [213 kB]
	Get:140 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-numpy1.71-dev arm64 1.71.0-6ubuntu6 [218 kB]
	Get:141 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-numpy-dev arm64 1.71.0.0ubuntu2 [3424 B]
	Get:142 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost-program-options1.71.0 arm64 1.71.0-6ubuntu6 [326 kB]
	Get:143 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost-program-options1.71-dev arm64 1.71.0-6ubuntu6 [372 kB]
	Get:144 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost-program-options-dev arm64 1.71.0.0ubuntu2 [3408 B]
	Get:145 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libpython3.8 arm64 3.8.10-0ubuntu1~20.04.9 [1491 kB]
	Get:146 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libexpat1-dev arm64 2.2.9-1ubuntu0.6 [104 kB]
	Get:147 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libpython3.8-dev arm64 3.8.10-0ubuntu1~20.04.9 [3758 kB]
	Get:148 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libpython3-dev arm64 3.8.2-0ubuntu2 [7236 B]
	Get:149 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 zlib1g-dev arm64 1:1.2.11.dfsg-2ubuntu1.5 [154 kB]
	Get:150 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 python3.8-dev arm64 3.8.10-0ubuntu1~20.04.9 [514 kB]
	Get:151 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 python3-lib2to3 all 3.8.10-0ubuntu1~20.04 [76.3 kB]
	Get:152 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 python3-distutils all 3.8.10-0ubuntu1~20.04 [141 kB]
	Get:153 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 python3-dev arm64 3.8.2-0ubuntu2 [1212 B]
	Get:154 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-python1.71-dev arm64 1.71.0-6ubuntu6 [294 kB]
	Get:155 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-python-dev arm64 1.71.0.0ubuntu2 [3688 B]
	Get:156 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-random1.71.0 arm64 1.71.0-6ubuntu6 [215 kB]
	Get:157 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-random1.71-dev arm64 1.71.0-6ubuntu6 [218 kB]
	Get:158 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-random-dev arm64 1.71.0.0ubuntu2 [3388 B]
	Get:159 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-regex-dev arm64 1.71.0.0ubuntu2 [3656 B]
	Get:160 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-serialization-dev arm64 1.71.0.0ubuntu2 [3612 B]
	Get:161 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-stacktrace1.71.0 arm64 1.71.0-6ubuntu6 [236 kB]
	Get:162 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-stacktrace1.71-dev arm64 1.71.0-6ubuntu6 [217 kB]
	Get:163 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-stacktrace-dev arm64 1.71.0.0ubuntu2 [3392 B]
	Get:164 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost-system-dev arm64 1.71.0.0ubuntu2 [3536 B]
	Get:165 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-test-dev arm64 1.71.0.0ubuntu2 [3428 B]
	Get:166 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libboost-thread-dev arm64 1.71.0.0ubuntu2 [3416 B]
	Get:167 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-timer1.71.0 arm64 1.71.0-6ubuntu6 [212 kB]
	Get:168 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-timer1.71-dev arm64 1.71.0-6ubuntu6 [216 kB]
	Get:169 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-timer-dev arm64 1.71.0.0ubuntu2 [3516 B]
	Get:170 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-type-erasure1.71.0 arm64 1.71.0-6ubuntu6 [221 kB]
	Get:171 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-type-erasure1.71-dev arm64 1.71.0-6ubuntu6 [226 kB]
	Get:172 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-type-erasure-dev arm64 1.71.0.0ubuntu2 [3488 B]
	Get:173 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-wave1.71.0 arm64 1.71.0-6ubuntu6 [370 kB]
	Get:174 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-wave1.71-dev arm64 1.71.0-6ubuntu6 [450 kB]
	Get:175 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-wave-dev arm64 1.71.0.0ubuntu2 [3420 B]
	Get:176 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libboost-all-dev arm64 1.71.0.0ubuntu2 [2468 B]
	Get:177 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libcaf-openmpi-3 arm64 2.8.0-1 [31.5 kB]
	Get:178 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libcoarrays-dev arm64 2.8.0-1 [28.2 kB]
	Get:179 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 libcoarrays-openmpi-dev arm64 2.8.0-1 [31.5 kB]
	Get:180 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libtool all 2.4.6-14 [161 kB]
	debconf: delaying package configuration, since apt-utils is not installed
	Fetched 89.3 MB in 10s (9006 kB/s)
	Selecting previously unselected package libpython3.8-minimal:arm64.
	(Reading database ...(Reading database ... 5%
(Reading database ... 10%
(Reading database ... 15%
(Reading database ... 20%
(Reading database ... 25%
(Reading database ... 30%
(Reading database ... 35%
(Reading database ... 40%
(Reading database ... 45%
(Reading database ... 50%
(Reading database ... 55%
(Reading database ... 60%
(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%
(Reading database ... 18303 files and directories currently installed.)
	Preparing to unpack .../libpython3.8-minimal_3.8.10-0ubuntu1~20.04.9_arm64.deb ...
	Unpacking libpython3.8-minimal:arm64 (3.8.10-0ubuntu1~20.04.9) ...
	Selecting previously unselected package python3.8-minimal.
	Preparing to unpack .../python3.8-minimal_3.8.10-0ubuntu1~20.04.9_arm64.deb ...
	Unpacking python3.8-minimal (3.8.10-0ubuntu1~20.04.9) ...
	Setting up libpython3.8-minimal:arm64 (3.8.10-0ubuntu1~20.04.9) ...
	Setting up python3.8-minimal (3.8.10-0ubuntu1~20.04.9) ...
	Selecting previously unselected package python3-minimal.
	(Reading database ...(Reading database ... 5%
(Reading database ... 10%
(Reading database ... 15%
(Reading database ... 20%
(Reading database ... 25%
(Reading database ... 30%
(Reading database ... 35%
(Reading database ... 40%
(Reading database ... 45%
(Reading database ... 50%
(Reading database ... 55%
(Reading database ... 60%
(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%
(Reading database ... 18586 files and directories currently installed.)
	Preparing to unpack .../0-python3-minimal_3.8.2-0ubuntu2_arm64.deb ...
	Unpacking python3-minimal (3.8.2-0ubuntu2) ...
	Selecting previously unselected package mime-support.
	Preparing to unpack .../1-mime-support_3.64ubuntu1_all.deb ...
	Unpacking mime-support (3.64ubuntu1) ...
	Selecting previously unselected package libmpdec2:arm64.
	Preparing to unpack .../2-libmpdec2_2.4.2-3_arm64.deb ...
	Unpacking libmpdec2:arm64 (2.4.2-3) ...
	Selecting previously unselected package readline-common.
	Preparing to unpack .../3-readline-common_8.0-4_all.deb ...
	Unpacking readline-common (8.0-4) ...
	Selecting previously unselected package libreadline8:arm64.
	Preparing to unpack .../4-libreadline8_8.0-4_arm64.deb ...
	Unpacking libreadline8:arm64 (8.0-4) ...
	Selecting previously unselected package libpython3.8-stdlib:arm64.
	Preparing to unpack .../5-libpython3.8-stdlib_3.8.10-0ubuntu1~20.04.9_arm64.deb ...
	Unpacking libpython3.8-stdlib:arm64 (3.8.10-0ubuntu1~20.04.9) ...
	Selecting previously unselected package python3.8.
	Preparing to unpack .../6-python3.8_3.8.10-0ubuntu1~20.04.9_arm64.deb ...
	Unpacking python3.8 (3.8.10-0ubuntu1~20.04.9) ...
	Selecting previously unselected package libpython3-stdlib:arm64.
	Preparing to unpack .../7-libpython3-stdlib_3.8.2-0ubuntu2_arm64.deb ...
	Unpacking libpython3-stdlib:arm64 (3.8.2-0ubuntu2) ...
	Setting up python3-minimal (3.8.2-0ubuntu2) ...
	Selecting previously unselected package python3.
	(Reading database ...(Reading database ... 5%
(Reading database ... 10%
(Reading database ... 15%
(Reading database ... 20%
(Reading database ... 25%
(Reading database ... 30%
(Reading database ... 35%
(Reading database ... 40%
(Reading database ... 45%
(Reading database ... 50%
(Reading database ... 55%
(Reading database ... 60%
(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%
(Reading database ... 19010 files and directories currently installed.)
	Preparing to unpack .../000-python3_3.8.2-0ubuntu2_arm64.deb ...
	Unpacking python3 (3.8.2-0ubuntu2) ...
	Selecting previously unselected package libmagic-mgc.
	Preparing to unpack .../001-libmagic-mgc_1%3a5.38-4_arm64.deb ...
	Unpacking libmagic-mgc (1:5.38-4) ...
	Selecting previously unselected package libmagic1:arm64.
	Preparing to unpack .../002-libmagic1_1%3a5.38-4_arm64.deb ...
	Unpacking libmagic1:arm64 (1:5.38-4) ...
	Selecting previously unselected package file.
	Preparing to unpack .../003-file_1%3a5.38-4_arm64.deb ...
	Unpacking file (1:5.38-4) ...
	Selecting previously unselected package libbsd0:arm64.
	Preparing to unpack .../004-libbsd0_0.10.0-1_arm64.deb ...
	Unpacking libbsd0:arm64 (0.10.0-1) ...
	Selecting previously unselected package libcbor0.6:arm64.
	Preparing to unpack .../005-libcbor0.6_0.6.0-0ubuntu1_arm64.deb ...
	Unpacking libcbor0.6:arm64 (0.6.0-0ubuntu1) ...
	Selecting previously unselected package libedit2:arm64.
	Preparing to unpack .../006-libedit2_3.1-20191231-1_arm64.deb ...
	Unpacking libedit2:arm64 (3.1-20191231-1) ...
	Selecting previously unselected package libfido2-1:arm64.
	Preparing to unpack .../007-libfido2-1_1.3.1-1ubuntu2_arm64.deb ...
	Unpacking libfido2-1:arm64 (1.3.1-1ubuntu2) ...
	Selecting previously unselected package libnuma1:arm64.
	Preparing to unpack .../008-libnuma1_2.0.12-1_arm64.deb ...
	Unpacking libnuma1:arm64 (2.0.12-1) ...
	Selecting previously unselected package libxau6:arm64.
	Preparing to unpack .../009-libxau6_1%3a1.0.9-0ubuntu1_arm64.deb ...
	Unpacking libxau6:arm64 (1:1.0.9-0ubuntu1) ...
	Selecting previously unselected package libxdmcp6:arm64.
	Preparing to unpack .../010-libxdmcp6_1%3a1.1.3-0ubuntu1_arm64.deb ...
	Unpacking libxdmcp6:arm64 (1:1.1.3-0ubuntu1) ...
	Selecting previously unselected package libxcb1:arm64.
	Preparing to unpack .../011-libxcb1_1.14-2_arm64.deb ...
	Unpacking libxcb1:arm64 (1.14-2) ...
	Selecting previously unselected package libx11-data.
	Preparing to unpack .../012-libx11-data_2%3a1.6.9-2ubuntu1.6_all.deb ...
	Unpacking libx11-data (2:1.6.9-2ubuntu1.6) ...
	Selecting previously unselected package libx11-6:arm64.
	Preparing to unpack .../013-libx11-6_2%3a1.6.9-2ubuntu1.6_arm64.deb ...
	Unpacking libx11-6:arm64 (2:1.6.9-2ubuntu1.6) ...
	Selecting previously unselected package libxext6:arm64.
	Preparing to unpack .../014-libxext6_2%3a1.3.4-0ubuntu1_arm64.deb ...
	Unpacking libxext6:arm64 (2:1.3.4-0ubuntu1) ...
	Selecting previously unselected package libxmuu1:arm64.
	Preparing to unpack .../015-libxmuu1_2%3a1.1.3-0ubuntu1_arm64.deb ...
	Unpacking libxmuu1:arm64 (2:1.1.3-0ubuntu1) ...
	Selecting previously unselected package openssh-client.
	Preparing to unpack .../016-openssh-client_1%3a8.2p1-4ubuntu0.11_arm64.deb ...
	Unpacking openssh-client (1:8.2p1-4ubuntu0.11) ...
	Selecting previously unselected package xauth.
	Preparing to unpack .../017-xauth_1%3a1.1-0ubuntu1_arm64.deb ...
	Unpacking xauth (1:1.1-0ubuntu1) ...
	Selecting previously unselected package libsigsegv2:arm64.
	Preparing to unpack .../018-libsigsegv2_2.12-2_arm64.deb ...
	Unpacking libsigsegv2:arm64 (2.12-2) ...
	Selecting previously unselected package m4.
	Preparing to unpack .../019-m4_1.4.18-4_arm64.deb ...
	Unpacking m4 (1.4.18-4) ...
	Selecting previously unselected package autoconf.
	Preparing to unpack .../020-autoconf_2.69-11.1_all.deb ...
	Unpacking autoconf (2.69-11.1) ...
	Selecting previously unselected package autotools-dev.
	Preparing to unpack .../021-autotools-dev_20180224.1_all.deb ...
	Unpacking autotools-dev (20180224.1) ...
	Selecting previously unselected package automake.
	Preparing to unpack .../022-automake_1%3a1.16.1-4ubuntu6_all.deb ...
	Unpacking automake (1:1.16.1-4ubuntu6) ...
	Selecting previously unselected package gcc-8-base:arm64.
	Preparing to unpack .../023-gcc-8-base_8.4.0-3ubuntu2_arm64.deb ...
	Unpacking gcc-8-base:arm64 (8.4.0-3ubuntu2) ...
	Selecting previously unselected package cpp-8.
	Preparing to unpack .../024-cpp-8_8.4.0-3ubuntu2_arm64.deb ...
	Unpacking cpp-8 (8.4.0-3ubuntu2) ...
	Selecting previously unselected package libgcc-8-dev:arm64.
	Preparing to unpack .../025-libgcc-8-dev_8.4.0-3ubuntu2_arm64.deb ...
	Unpacking libgcc-8-dev:arm64 (8.4.0-3ubuntu2) ...
	Selecting previously unselected package gcc-8.
	Preparing to unpack .../026-gcc-8_8.4.0-3ubuntu2_arm64.deb ...
	Unpacking gcc-8 (8.4.0-3ubuntu2) ...
	Selecting previously unselected package libgfortran-9-dev:arm64.
	Preparing to unpack .../027-libgfortran-9-dev_9.4.0-1ubuntu1~20.04.2_arm64.deb ...
	Unpacking libgfortran-9-dev:arm64 (9.4.0-1ubuntu1~20.04.2) ...
	Selecting previously unselected package gfortran-9.
	Preparing to unpack .../028-gfortran-9_9.4.0-1ubuntu1~20.04.2_arm64.deb ...
	Unpacking gfortran-9 (9.4.0-1ubuntu1~20.04.2) ...
	Selecting previously unselected package gfortran.
	Preparing to unpack .../029-gfortran_4%3a9.3.0-1ubuntu2_arm64.deb ...
	Unpacking gfortran (4:9.3.0-1ubuntu2) ...
	Selecting previously unselected package libgfortran-8-dev:arm64.
	Preparing to unpack .../030-libgfortran-8-dev_8.4.0-3ubuntu2_arm64.deb ...
	Unpacking libgfortran-8-dev:arm64 (8.4.0-3ubuntu2) ...
	Selecting previously unselected package gfortran-8.
	Preparing to unpack .../031-gfortran-8_8.4.0-3ubuntu2_arm64.deb ...
	Unpacking gfortran-8 (8.4.0-3ubuntu2) ...
	Selecting previously unselected package libnl-3-200:arm64.
	Preparing to unpack .../032-libnl-3-200_3.4.0-1ubuntu0.1_arm64.deb ...
	Unpacking libnl-3-200:arm64 (3.4.0-1ubuntu0.1) ...
	Selecting previously unselected package libnl-route-3-200:arm64.
	Preparing to unpack .../033-libnl-route-3-200_3.4.0-1ubuntu0.1_arm64.deb ...
	Unpacking libnl-route-3-200:arm64 (3.4.0-1ubuntu0.1) ...
	Selecting previously unselected package libibverbs1:arm64.
	Preparing to unpack .../034-libibverbs1_28.0-1ubuntu1_arm64.deb ...
	Unpacking libibverbs1:arm64 (28.0-1ubuntu1) ...
	Selecting previously unselected package ibverbs-providers:arm64.
	Preparing to unpack .../035-ibverbs-providers_28.0-1ubuntu1_arm64.deb ...
	Unpacking ibverbs-providers:arm64 (28.0-1ubuntu1) ...
	Selecting previously unselected package icu-devtools.
	Preparing to unpack .../036-icu-devtools_66.1-2ubuntu2.1_arm64.deb ...
	Unpacking icu-devtools (66.1-2ubuntu2.1) ...
	Selecting previously unselected package libboost1.71-dev:arm64.
	Preparing to unpack .../037-libboost1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-dev:arm64.
	Preparing to unpack .../038-libboost-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-dev:arm64 (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost1.71-tools-dev.
	Preparing to unpack .../039-libboost1.71-tools-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost1.71-tools-dev (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-tools-dev.
	Preparing to unpack .../040-libboost-tools-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-tools-dev (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-atomic1.71.0:arm64.
	Preparing to unpack .../041-libboost-atomic1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-atomic1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-atomic1.71-dev:arm64.
	Preparing to unpack .../042-libboost-atomic1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-atomic1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-atomic-dev:arm64.
	Preparing to unpack .../043-libboost-atomic-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-atomic-dev:arm64 (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-chrono1.71.0:arm64.
	Preparing to unpack .../044-libboost-chrono1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-chrono1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-chrono1.71-dev:arm64.
	Preparing to unpack .../045-libboost-chrono1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-chrono1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-chrono-dev:arm64.
	Preparing to unpack .../046-libboost-chrono-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-chrono-dev:arm64 (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-container1.71.0:arm64.
	Preparing to unpack .../047-libboost-container1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-container1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-container1.71-dev:arm64.
	Preparing to unpack .../048-libboost-container1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-container1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-container-dev:arm64.
	Preparing to unpack .../049-libboost-container-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-container-dev:arm64 (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-date-time1.71.0:arm64.
	Preparing to unpack .../050-libboost-date-time1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-date-time1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-serialization1.71.0:arm64.
	Preparing to unpack .../051-libboost-serialization1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-serialization1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-serialization1.71-dev:arm64.
	Preparing to unpack .../052-libboost-serialization1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-serialization1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-date-time1.71-dev:arm64.
	Preparing to unpack .../053-libboost-date-time1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-date-time1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-system1.71.0:arm64.
	Preparing to unpack .../054-libboost-system1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-system1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-system1.71-dev:arm64.
	Preparing to unpack .../055-libboost-system1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-system1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-thread1.71.0:arm64.
	Preparing to unpack .../056-libboost-thread1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-thread1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-thread1.71-dev:arm64.
	Preparing to unpack .../057-libboost-thread1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-thread1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-context1.71.0:arm64.
	Preparing to unpack .../058-libboost-context1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-context1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-context1.71-dev:arm64.
	Preparing to unpack .../059-libboost-context1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-context1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-context-dev:arm64.
	Preparing to unpack .../060-libboost-context-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-context-dev:arm64 (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-coroutine1.71.0:arm64.
	Preparing to unpack .../061-libboost-coroutine1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-coroutine1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-coroutine1.71-dev:arm64.
	Preparing to unpack .../062-libboost-coroutine1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-coroutine1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-coroutine-dev:arm64.
	Preparing to unpack .../063-libboost-coroutine-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-coroutine-dev:arm64 (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-date-time-dev:arm64.
	Preparing to unpack .../064-libboost-date-time-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-date-time-dev:arm64 (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-exception1.71-dev:arm64.
	Preparing to unpack .../065-libboost-exception1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-exception1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-exception-dev:arm64.
	Preparing to unpack .../066-libboost-exception-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-exception-dev:arm64 (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-filesystem1.71.0:arm64.
	Preparing to unpack .../067-libboost-filesystem1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-filesystem1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-filesystem1.71-dev:arm64.
	Preparing to unpack .../068-libboost-filesystem1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-filesystem1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-fiber1.71.0:arm64.
	Preparing to unpack .../069-libboost-fiber1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-fiber1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-fiber1.71-dev:arm64.
	Preparing to unpack .../070-libboost-fiber1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-fiber1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-fiber-dev:arm64.
	Preparing to unpack .../071-libboost-fiber-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-fiber-dev:arm64 (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-filesystem-dev:arm64.
	Preparing to unpack .../072-libboost-filesystem-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-filesystem-dev:arm64 (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-regex1.71.0:arm64.
	Preparing to unpack .../073-libboost-regex1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-regex1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-graph1.71.0:arm64.
	Preparing to unpack .../074-libboost-graph1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-graph1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libicu-dev:arm64.
	Preparing to unpack .../075-libicu-dev_66.1-2ubuntu2.1_arm64.deb ...
	Unpacking libicu-dev:arm64 (66.1-2ubuntu2.1) ...
	Selecting previously unselected package libboost-regex1.71-dev:arm64.
	Preparing to unpack .../076-libboost-regex1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-regex1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-test1.71.0:arm64.
	Preparing to unpack .../077-libboost-test1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-test1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-test1.71-dev:arm64.
	Preparing to unpack .../078-libboost-test1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-test1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-graph1.71-dev:arm64.
	Preparing to unpack .../079-libboost-graph1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-graph1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-graph-dev:arm64.
	Preparing to unpack .../080-libboost-graph-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-graph-dev:arm64 (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libevent-2.1-7:arm64.
	Preparing to unpack .../081-libevent-2.1-7_2.1.11-stable-1_arm64.deb ...
	Unpacking libevent-2.1-7:arm64 (2.1.11-stable-1) ...
	Selecting previously unselected package libevent-core-2.1-7:arm64.
	Preparing to unpack .../082-libevent-core-2.1-7_2.1.11-stable-1_arm64.deb ...
	Unpacking libevent-core-2.1-7:arm64 (2.1.11-stable-1) ...
	Selecting previously unselected package libevent-pthreads-2.1-7:arm64.
	Preparing to unpack .../083-libevent-pthreads-2.1-7_2.1.11-stable-1_arm64.deb ...
	Unpacking libevent-pthreads-2.1-7:arm64 (2.1.11-stable-1) ...
	Selecting previously unselected package libltdl7:arm64.
	Preparing to unpack .../084-libltdl7_2.4.6-14_arm64.deb ...
	Unpacking libltdl7:arm64 (2.4.6-14) ...
	Selecting previously unselected package libhwloc15:arm64.
	Preparing to unpack .../085-libhwloc15_2.1.0+dfsg-4_arm64.deb ...
	Unpacking libhwloc15:arm64 (2.1.0+dfsg-4) ...
	Selecting previously unselected package libpciaccess0:arm64.
	Preparing to unpack .../086-libpciaccess0_0.16-0ubuntu1_arm64.deb ...
	Unpacking libpciaccess0:arm64 (0.16-0ubuntu1) ...
	Selecting previously unselected package libxnvctrl0:arm64.
	Preparing to unpack .../087-libxnvctrl0_470.57.01-0ubuntu0.20.04.3_arm64.deb ...
	Unpacking libxnvctrl0:arm64 (470.57.01-0ubuntu0.20.04.3) ...
	Selecting previously unselected package ocl-icd-libopencl1:arm64.
	Preparing to unpack .../088-ocl-icd-libopencl1_2.2.11-1ubuntu1_arm64.deb ...
	Unpacking ocl-icd-libopencl1:arm64 (2.2.11-1ubuntu1) ...
	Selecting previously unselected package libhwloc-plugins:arm64.
	Preparing to unpack .../089-libhwloc-plugins_2.1.0+dfsg-4_arm64.deb ...
	Unpacking libhwloc-plugins:arm64 (2.1.0+dfsg-4) ...
	Selecting previously unselected package libpmix2:arm64.
	Preparing to unpack .../090-libpmix2_3.1.5-1_arm64.deb ...
	Unpacking libpmix2:arm64 (3.1.5-1) ...
	Selecting previously unselected package libopenmpi3:arm64.
	Preparing to unpack .../091-libopenmpi3_4.0.3-0ubuntu1_arm64.deb ...
	Unpacking libopenmpi3:arm64 (4.0.3-0ubuntu1) ...
	Selecting previously unselected package libboost-mpi1.71.0.
	Preparing to unpack .../092-libboost-mpi1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-mpi1.71.0 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-graph-parallel1.71.0.
	Preparing to unpack .../093-libboost-graph-parallel1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-graph-parallel1.71.0 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-graph-parallel1.71-dev.
	Preparing to unpack .../094-libboost-graph-parallel1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-graph-parallel1.71-dev (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-graph-parallel-dev.
	Preparing to unpack .../095-libboost-graph-parallel-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-graph-parallel-dev (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-iostreams1.71.0:arm64.
	Preparing to unpack .../096-libboost-iostreams1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-iostreams1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-iostreams1.71-dev:arm64.
	Preparing to unpack .../097-libboost-iostreams1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-iostreams1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-iostreams-dev:arm64.
	Preparing to unpack .../098-libboost-iostreams-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-iostreams-dev:arm64 (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-locale1.71.0:arm64.
	Preparing to unpack .../099-libboost-locale1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-locale1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-locale1.71-dev:arm64.
	Preparing to unpack .../100-libboost-locale1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-locale1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-locale-dev:arm64.
	Preparing to unpack .../101-libboost-locale-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-locale-dev:arm64 (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-log1.71.0.
	Preparing to unpack .../102-libboost-log1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-log1.71.0 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-log1.71-dev.
	Preparing to unpack .../103-libboost-log1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-log1.71-dev (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-log-dev.
	Preparing to unpack .../104-libboost-log-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-log-dev (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-math1.71.0:arm64.
	Preparing to unpack .../105-libboost-math1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-math1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-math1.71-dev:arm64.
	Preparing to unpack .../106-libboost-math1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-math1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-math-dev:arm64.
	Preparing to unpack .../107-libboost-math-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-math-dev:arm64 (1.71.0.0ubuntu2) ...
	Selecting previously unselected package openmpi-common.
	Preparing to unpack .../108-openmpi-common_4.0.3-0ubuntu1_all.deb ...
	Unpacking openmpi-common (4.0.3-0ubuntu1) ...
	Selecting previously unselected package libnl-3-dev:arm64.
	Preparing to unpack .../109-libnl-3-dev_3.4.0-1ubuntu0.1_arm64.deb ...
	Unpacking libnl-3-dev:arm64 (3.4.0-1ubuntu0.1) ...
	Selecting previously unselected package libnl-route-3-dev:arm64.
	Preparing to unpack .../110-libnl-route-3-dev_3.4.0-1ubuntu0.1_arm64.deb ...
	Unpacking libnl-route-3-dev:arm64 (3.4.0-1ubuntu0.1) ...
	Selecting previously unselected package libibverbs-dev:arm64.
	Preparing to unpack .../111-libibverbs-dev_28.0-1ubuntu1_arm64.deb ...
	Unpacking libibverbs-dev:arm64 (28.0-1ubuntu1) ...
	Selecting previously unselected package libnuma-dev:arm64.
	Preparing to unpack .../112-libnuma-dev_2.0.12-1_arm64.deb ...
	Unpacking libnuma-dev:arm64 (2.0.12-1) ...
	Selecting previously unselected package libltdl-dev:arm64.
	Preparing to unpack .../113-libltdl-dev_2.4.6-14_arm64.deb ...
	Unpacking libltdl-dev:arm64 (2.4.6-14) ...
	Selecting previously unselected package libhwloc-dev:arm64.
	Preparing to unpack .../114-libhwloc-dev_2.1.0+dfsg-4_arm64.deb ...
	Unpacking libhwloc-dev:arm64 (2.1.0+dfsg-4) ...
	Selecting previously unselected package libevent-extra-2.1-7:arm64.
	Preparing to unpack .../115-libevent-extra-2.1-7_2.1.11-stable-1_arm64.deb ...
	Unpacking libevent-extra-2.1-7:arm64 (2.1.11-stable-1) ...
	Selecting previously unselected package libevent-openssl-2.1-7:arm64.
	Preparing to unpack .../116-libevent-openssl-2.1-7_2.1.11-stable-1_arm64.deb ...
	Unpacking libevent-openssl-2.1-7:arm64 (2.1.11-stable-1) ...
	Selecting previously unselected package libevent-dev.
	Preparing to unpack .../117-libevent-dev_2.1.11-stable-1_arm64.deb ...
	Unpacking libevent-dev (2.1.11-stable-1) ...
	Selecting previously unselected package openmpi-bin.
	Preparing to unpack .../118-openmpi-bin_4.0.3-0ubuntu1_arm64.deb ...
	Unpacking openmpi-bin (4.0.3-0ubuntu1) ...
	Selecting previously unselected package libopenmpi-dev:arm64.
	Preparing to unpack .../119-libopenmpi-dev_4.0.3-0ubuntu1_arm64.deb ...
	Unpacking libopenmpi-dev:arm64 (4.0.3-0ubuntu1) ...
	Selecting previously unselected package mpi-default-dev.
	Preparing to unpack .../120-mpi-default-dev_1.13_arm64.deb ...
	Unpacking mpi-default-dev (1.13) ...
	Selecting previously unselected package libboost-mpi1.71-dev.
	Preparing to unpack .../121-libboost-mpi1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-mpi1.71-dev (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-mpi-dev.
	Preparing to unpack .../122-libboost-mpi-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-mpi-dev (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-python1.71.0.
	Preparing to unpack .../123-libboost-python1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-python1.71.0 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package mpi-default-bin.
	Preparing to unpack .../124-mpi-default-bin_1.13_arm64.deb ...
	Unpacking mpi-default-bin (1.13) ...
	Selecting previously unselected package libboost-mpi-python1.71.0.
	Preparing to unpack .../125-libboost-mpi-python1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-mpi-python1.71.0 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-mpi-python1.71-dev.
	Preparing to unpack .../126-libboost-mpi-python1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-mpi-python1.71-dev (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-mpi-python-dev.
	Preparing to unpack .../127-libboost-mpi-python-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-mpi-python-dev (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-numpy1.71.0.
	Preparing to unpack .../128-libboost-numpy1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-numpy1.71.0 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-numpy1.71-dev.
	Preparing to unpack .../129-libboost-numpy1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-numpy1.71-dev (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-numpy-dev.
	Preparing to unpack .../130-libboost-numpy-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-numpy-dev (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-program-options1.71.0:arm64.
	Preparing to unpack .../131-libboost-program-options1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-program-options1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-program-options1.71-dev:arm64.
	Preparing to unpack .../132-libboost-program-options1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-program-options1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-program-options-dev:arm64.
	Preparing to unpack .../133-libboost-program-options-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-program-options-dev:arm64 (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libpython3.8:arm64.
	Preparing to unpack .../134-libpython3.8_3.8.10-0ubuntu1~20.04.9_arm64.deb ...
	Unpacking libpython3.8:arm64 (3.8.10-0ubuntu1~20.04.9) ...
	Selecting previously unselected package libexpat1-dev:arm64.
	Preparing to unpack .../135-libexpat1-dev_2.2.9-1ubuntu0.6_arm64.deb ...
	Unpacking libexpat1-dev:arm64 (2.2.9-1ubuntu0.6) ...
	Selecting previously unselected package libpython3.8-dev:arm64.
	Preparing to unpack .../136-libpython3.8-dev_3.8.10-0ubuntu1~20.04.9_arm64.deb ...
	Unpacking libpython3.8-dev:arm64 (3.8.10-0ubuntu1~20.04.9) ...
	Selecting previously unselected package libpython3-dev:arm64.
	Preparing to unpack .../137-libpython3-dev_3.8.2-0ubuntu2_arm64.deb ...
	Unpacking libpython3-dev:arm64 (3.8.2-0ubuntu2) ...
	Selecting previously unselected package zlib1g-dev:arm64.
	Preparing to unpack .../138-zlib1g-dev_1%3a1.2.11.dfsg-2ubuntu1.5_arm64.deb ...
	Unpacking zlib1g-dev:arm64 (1:1.2.11.dfsg-2ubuntu1.5) ...
	Selecting previously unselected package python3.8-dev.
	Preparing to unpack .../139-python3.8-dev_3.8.10-0ubuntu1~20.04.9_arm64.deb ...
	Unpacking python3.8-dev (3.8.10-0ubuntu1~20.04.9) ...
	Selecting previously unselected package python3-lib2to3.
	Preparing to unpack .../140-python3-lib2to3_3.8.10-0ubuntu1~20.04_all.deb ...
	Unpacking python3-lib2to3 (3.8.10-0ubuntu1~20.04) ...
	Selecting previously unselected package python3-distutils.
	Preparing to unpack .../141-python3-distutils_3.8.10-0ubuntu1~20.04_all.deb ...
	Unpacking python3-distutils (3.8.10-0ubuntu1~20.04) ...
	Selecting previously unselected package python3-dev.
	Preparing to unpack .../142-python3-dev_3.8.2-0ubuntu2_arm64.deb ...
	Unpacking python3-dev (3.8.2-0ubuntu2) ...
	Selecting previously unselected package libboost-python1.71-dev.
	Preparing to unpack .../143-libboost-python1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-python1.71-dev (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-python-dev.
	Preparing to unpack .../144-libboost-python-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-python-dev (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-random1.71.0:arm64.
	Preparing to unpack .../145-libboost-random1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-random1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-random1.71-dev:arm64.
	Preparing to unpack .../146-libboost-random1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-random1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-random-dev:arm64.
	Preparing to unpack .../147-libboost-random-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-random-dev:arm64 (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-regex-dev:arm64.
	Preparing to unpack .../148-libboost-regex-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-regex-dev:arm64 (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-serialization-dev:arm64.
	Preparing to unpack .../149-libboost-serialization-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-serialization-dev:arm64 (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-stacktrace1.71.0:arm64.
	Preparing to unpack .../150-libboost-stacktrace1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-stacktrace1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-stacktrace1.71-dev:arm64.
	Preparing to unpack .../151-libboost-stacktrace1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-stacktrace1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-stacktrace-dev:arm64.
	Preparing to unpack .../152-libboost-stacktrace-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-stacktrace-dev:arm64 (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-system-dev:arm64.
	Preparing to unpack .../153-libboost-system-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-system-dev:arm64 (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-test-dev:arm64.
	Preparing to unpack .../154-libboost-test-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-test-dev:arm64 (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-thread-dev:arm64.
	Preparing to unpack .../155-libboost-thread-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-thread-dev:arm64 (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-timer1.71.0:arm64.
	Preparing to unpack .../156-libboost-timer1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-timer1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-timer1.71-dev:arm64.
	Preparing to unpack .../157-libboost-timer1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-timer1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-timer-dev:arm64.
	Preparing to unpack .../158-libboost-timer-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-timer-dev:arm64 (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-type-erasure1.71.0:arm64.
	Preparing to unpack .../159-libboost-type-erasure1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-type-erasure1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-type-erasure1.71-dev:arm64.
	Preparing to unpack .../160-libboost-type-erasure1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-type-erasure1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-type-erasure-dev:arm64.
	Preparing to unpack .../161-libboost-type-erasure-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-type-erasure-dev:arm64 (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-wave1.71.0:arm64.
	Preparing to unpack .../162-libboost-wave1.71.0_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-wave1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-wave1.71-dev:arm64.
	Preparing to unpack .../163-libboost-wave1.71-dev_1.71.0-6ubuntu6_arm64.deb ...
	Unpacking libboost-wave1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Selecting previously unselected package libboost-wave-dev:arm64.
	Preparing to unpack .../164-libboost-wave-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-wave-dev:arm64 (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libboost-all-dev.
	Preparing to unpack .../165-libboost-all-dev_1.71.0.0ubuntu2_arm64.deb ...
	Unpacking libboost-all-dev (1.71.0.0ubuntu2) ...
	Selecting previously unselected package libcaf-openmpi-3:arm64.
	Preparing to unpack .../166-libcaf-openmpi-3_2.8.0-1_arm64.deb ...
	Unpacking libcaf-openmpi-3:arm64 (2.8.0-1) ...
	Selecting previously unselected package libcoarrays-dev:arm64.
	Preparing to unpack .../167-libcoarrays-dev_2.8.0-1_arm64.deb ...
	Unpacking libcoarrays-dev:arm64 (2.8.0-1) ...
	Selecting previously unselected package libcoarrays-openmpi-dev:arm64.
	Preparing to unpack .../168-libcoarrays-openmpi-dev_2.8.0-1_arm64.deb ...
	Unpacking libcoarrays-openmpi-dev:arm64 (2.8.0-1) ...
	Selecting previously unselected package libtool.
	Preparing to unpack .../169-libtool_2.4.6-14_all.deb ...
	Unpacking libtool (2.4.6-14) ...
	Setting up libboost1.71-tools-dev (1.71.0-6ubuntu6) ...
	Setting up libboost-stacktrace1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Setting up libpciaccess0:arm64 (0.16-0ubuntu1) ...
	Setting up libxau6:arm64 (1:1.0.9-0ubuntu1) ...
	Setting up mime-support (3.64ubuntu1) ...
	Setting up libmagic-mgc (1:5.38-4) ...
	Setting up libboost-container1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-chrono1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Setting up libmagic1:arm64 (1:5.38-4) ...
	Setting up libgfortran-9-dev:arm64 (9.4.0-1ubuntu1~20.04.2) ...
	Setting up libboost-container1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-tools-dev (1.71.0.0ubuntu2) ...
	Setting up file (1:5.38-4) ...
	Setting up libcbor0.6:arm64 (0.6.0-0ubuntu1) ...
	Setting up libboost-filesystem1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-iostreams1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-python1.71.0 (1.71.0-6ubuntu6) ...
	Setting up libboost-date-time1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-container-dev:arm64 (1.71.0.0ubuntu2) ...
	Setting up libboost-numpy1.71.0 (1.71.0-6ubuntu6) ...
	Setting up autotools-dev (20180224.1) ...
	Setting up libboost-math1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Setting up libexpat1-dev:arm64 (2.2.9-1ubuntu0.6) ...
	Setting up libx11-data (2:1.6.9-2ubuntu1.6) ...
	Setting up libboost-atomic1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Setting up libsigsegv2:arm64 (2.12-2) ...
	Setting up libevent-core-2.1-7:arm64 (2.1.11-stable-1) ...
	Setting up libevent-2.1-7:arm64 (2.1.11-stable-1) ...
	Setting up gcc-8-base:arm64 (8.4.0-3ubuntu2) ...
	Setting up icu-devtools (66.1-2ubuntu2.1) ...
	Setting up libboost-regex1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-math1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Setting up libltdl7:arm64 (2.4.6-14) ...
	Setting up libgcc-8-dev:arm64 (8.4.0-3ubuntu2) ...
	Setting up zlib1g-dev:arm64 (1:1.2.11.dfsg-2ubuntu1.5) ...
	Setting up libnuma1:arm64 (2.0.12-1) ...
	Setting up libboost-timer1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Setting up ocl-icd-libopencl1:arm64 (2.2.11-1ubuntu1) ...
	Setting up libboost-test1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Setting up libnl-3-200:arm64 (3.4.0-1ubuntu0.1) ...
	Setting up libboost-thread1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Setting up openmpi-common (4.0.3-0ubuntu1) ...
	Setting up libboost-context1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-serialization1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-random1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-system1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-atomic1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-dev:arm64 (1.71.0.0ubuntu2) ...
	Setting up libboost-fiber1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-locale1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Setting up libmpdec2:arm64 (2.4.2-3) ...
	Setting up libboost-graph1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Setting up cpp-8 (8.4.0-3ubuntu2) ...
	Setting up libfido2-1:arm64 (1.3.1-1ubuntu2) ...
	Setting up libboost-stacktrace1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-wave1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Setting up libbsd0:arm64 (0.10.0-1) ...
	Setting up readline-common (8.0-4) ...
	Setting up libicu-dev:arm64 (66.1-2ubuntu2.1) ...
	Setting up libevent-pthreads-2.1-7:arm64 (2.1.11-stable-1) ...
	Setting up libboost-program-options1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-program-options1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Setting up libgfortran-8-dev:arm64 (8.4.0-3ubuntu2) ...
	Setting up libboost-chrono1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Setting up libxdmcp6:arm64 (1:1.1.3-0ubuntu1) ...
	Setting up libevent-extra-2.1-7:arm64 (2.1.11-stable-1) ...
	Setting up libxcb1:arm64 (1.14-2) ...
	Setting up libboost-exception1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Setting up gcc-8 (8.4.0-3ubuntu2) ...
	Setting up gfortran-9 (9.4.0-1ubuntu1~20.04.2) ...
	Setting up libboost-log1.71.0 (1.71.0-6ubuntu6) ...
	Setting up libtool (2.4.6-14) ...
	Setting up libboost-chrono-dev:arm64 (1.71.0.0ubuntu2) ...
	Setting up libboost-numpy1.71-dev (1.71.0-6ubuntu6) ...
	Setting up gfortran-8 (8.4.0-3ubuntu2) ...
	Setting up libboost-math-dev:arm64 (1.71.0.0ubuntu2) ...
	Setting up libedit2:arm64 (3.1-20191231-1) ...
	Setting up libreadline8:arm64 (8.0-4) ...
	Setting up libevent-openssl-2.1-7:arm64 (2.1.11-stable-1) ...
	Setting up m4 (1.4.18-4) ...
	Setting up libboost-exception-dev:arm64 (1.71.0.0ubuntu2) ...
	Setting up gfortran (4:9.3.0-1ubuntu2) ...
	update-alternatives: using /usr/bin/gfortran to provide /usr/bin/f95 (f95) in auto mode
	update-alternatives: warning: skip creation of /usr/share/man/man1/f95.1.gz because associated file /usr/share/man/man1/gfortran.1.gz (of link group f95) doesn't exist
	update-alternatives: using /usr/bin/gfortran to provide /usr/bin/f77 (f77) in auto mode
	update-alternatives: warning: skip creation of /usr/share/man/man1/f77.1.gz because associated file /usr/share/man/man1/gfortran.1.gz (of link group f77) doesn't exist
	Setting up libnuma-dev:arm64 (2.0.12-1) ...
	Setting up libboost-system1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Setting up libnl-route-3-200:arm64 (3.4.0-1ubuntu0.1) ...
	Setting up libboost-random1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-coroutine1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-timer1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-type-erasure1.71.0:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-test1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-program-options-dev:arm64 (1.71.0.0ubuntu2) ...
	Setting up libboost-filesystem1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-numpy-dev (1.71.0.0ubuntu2) ...
	Setting up libevent-dev (2.1.11-stable-1) ...
	Setting up libhwloc15:arm64 (2.1.0+dfsg-4) ...
	Setting up libboost-regex1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-serialization1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-serialization-dev:arm64 (1.71.0.0ubuntu2) ...
	Setting up autoconf (2.69-11.1) ...
	Setting up libboost-atomic-dev:arm64 (1.71.0.0ubuntu2) ...
	Setting up libboost-date-time1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Setting up libnl-3-dev:arm64 (3.4.0-1ubuntu0.1) ...
	Setting up libboost-stacktrace-dev:arm64 (1.71.0.0ubuntu2) ...
	Setting up libx11-6:arm64 (2:1.6.9-2ubuntu1.6) ...
	Setting up libpython3.8-stdlib:arm64 (3.8.10-0ubuntu1~20.04.9) ...
	Setting up python3.8 (3.8.10-0ubuntu1~20.04.9) ...
	Setting up libboost-regex-dev:arm64 (1.71.0.0ubuntu2) ...
	Setting up libboost-timer-dev:arm64 (1.71.0.0ubuntu2) ...
	Setting up libxmuu1:arm64 (2:1.1.3-0ubuntu1) ...
	Setting up libboost-filesystem-dev:arm64 (1.71.0.0ubuntu2) ...
	Setting up libboost-iostreams1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Setting up libpython3-stdlib:arm64 (3.8.2-0ubuntu2) ...
	Setting up libboost-graph1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Setting up automake (1:1.16.1-4ubuntu6) ...
	update-alternatives: using /usr/bin/automake-1.16 to provide /usr/bin/automake (automake) in auto mode
	update-alternatives: warning: skip creation of /usr/share/man/man1/automake.1.gz because associated file /usr/share/man/man1/automake-1.16.1.gz (of link group automake) doesn't exist
	update-alternatives: warning: skip creation of /usr/share/man/man1/aclocal.1.gz because associated file /usr/share/man/man1/aclocal-1.16.1.gz (of link group automake) doesn't exist
	Setting up libboost-graph-dev:arm64 (1.71.0.0ubuntu2) ...
	Setting up libibverbs1:arm64 (28.0-1ubuntu1) ...
	Setting up ibverbs-providers:arm64 (28.0-1ubuntu1) ...
	Setting up libcoarrays-dev:arm64 (2.8.0-1) ...
	Setting up openssh-client (1:8.2p1-4ubuntu0.11) ...
	Setting up libboost-thread1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-system-dev:arm64 (1.71.0.0ubuntu2) ...
	Setting up libboost-context1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-wave1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Setting up libxext6:arm64 (2:1.3.4-0ubuntu1) ...
	Setting up libboost-random-dev:arm64 (1.71.0.0ubuntu2) ...
	Setting up python3 (3.8.2-0ubuntu2) ...
	running python rtupdate hooks for python3.8...
	running python post-rtupdate hooks for python3.8...
	Setting up libboost-wave-dev:arm64 (1.71.0.0ubuntu2) ...
	Setting up libboost-context-dev:arm64 (1.71.0.0ubuntu2) ...
	Setting up libboost-iostreams-dev:arm64 (1.71.0.0ubuntu2) ...
	Setting up libboost-fiber1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Setting up libxnvctrl0:arm64 (470.57.01-0ubuntu0.20.04.3) ...
	Setting up libboost-test-dev:arm64 (1.71.0.0ubuntu2) ...
	Setting up libnl-route-3-dev:arm64 (3.4.0-1ubuntu0.1) ...
	Setting up libltdl-dev:arm64 (2.4.6-14) ...
	Setting up libpython3.8:arm64 (3.8.10-0ubuntu1~20.04.9) ...
	Setting up libboost-date-time-dev:arm64 (1.71.0.0ubuntu2) ...
	Setting up xauth (1:1.1-0ubuntu1) ...
	Setting up libboost-type-erasure1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-log1.71-dev (1.71.0-6ubuntu6) ...
	Setting up libboost-coroutine1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Setting up libhwloc-dev:arm64 (2.1.0+dfsg-4) ...
	Setting up libboost-coroutine-dev:arm64 (1.71.0.0ubuntu2) ...
	Setting up python3-lib2to3 (3.8.10-0ubuntu1~20.04) ...
	Setting up libboost-log-dev (1.71.0.0ubuntu2) ...
	Setting up libboost-thread-dev:arm64 (1.71.0.0ubuntu2) ...
	Setting up libboost-fiber-dev:arm64 (1.71.0.0ubuntu2) ...
	Setting up python3-distutils (3.8.10-0ubuntu1~20.04) ...
	Setting up libboost-locale1.71-dev:arm64 (1.71.0-6ubuntu6) ...
	Setting up libboost-locale-dev:arm64 (1.71.0.0ubuntu2) ...
	Setting up libibverbs-dev:arm64 (28.0-1ubuntu1) ...
	Setting up libhwloc-plugins:arm64 (2.1.0+dfsg-4) ...
	Setting up libboost-type-erasure-dev:arm64 (1.71.0.0ubuntu2) ...
	Setting up libpython3.8-dev:arm64 (3.8.10-0ubuntu1~20.04.9) ...
	Setting up python3.8-dev (3.8.10-0ubuntu1~20.04.9) ...
	Setting up libpython3-dev:arm64 (3.8.2-0ubuntu2) ...
	Setting up libpmix2:arm64 (3.1.5-1) ...
	Setting up libopenmpi3:arm64 (4.0.3-0ubuntu1) ...
	Setting up libboost-mpi1.71.0 (1.71.0-6ubuntu6) ...
	Setting up libcaf-openmpi-3:arm64 (2.8.0-1) ...
	Setting up python3-dev (3.8.2-0ubuntu2) ...
	Setting up libboost-graph-parallel1.71.0 (1.71.0-6ubuntu6) ...
	Setting up openmpi-bin (4.0.3-0ubuntu1) ...
	update-alternatives: using /usr/bin/mpirun.openmpi to provide /usr/bin/mpirun (mpirun) in auto mode
	update-alternatives: warning: skip creation of /usr/share/man/man1/mpirun.1.gz because associated file /usr/share/man/man1/mpirun.openmpi.1.gz (of link group mpirun) doesn't exist
	update-alternatives: warning: skip creation of /usr/share/man/man1/mpiexec.1.gz because associated file /usr/share/man/man1/mpiexec.openmpi.1.gz (of link group mpirun) doesn't exist
	update-alternatives: using /usr/bin/mpicc.openmpi to provide /usr/bin/mpicc (mpi) in auto mode
	update-alternatives: warning: skip creation of /usr/share/man/man1/mpicc.1.gz because associated file /usr/share/man/man1/mpicc.openmpi.1.gz (of link group mpi) doesn't exist
	update-alternatives: warning: skip creation of /usr/share/man/man1/mpic++.1.gz because associated file /usr/share/man/man1/mpic++.openmpi.1.gz (of link group mpi) doesn't exist
	update-alternatives: warning: skip creation of /usr/share/man/man1/mpicxx.1.gz because associated file /usr/share/man/man1/mpicxx.openmpi.1.gz (of link group mpi) doesn't exist
	update-alternatives: warning: skip creation of /usr/share/man/man1/mpiCC.1.gz because associated file /usr/share/man/man1/mpiCC.openmpi.1.gz (of link group mpi) doesn't exist
	update-alternatives: warning: skip creation of /usr/share/man/man1/mpif77.1.gz because associated file /usr/share/man/man1/mpif77.openmpi.1.gz (of link group mpi) doesn't exist
	update-alternatives: warning: skip creation of /usr/share/man/man1/mpif90.1.gz because associated file /usr/share/man/man1/mpif90.openmpi.1.gz (of link group mpi) doesn't exist
	update-alternatives: warning: skip creation of /usr/share/man/man1/mpifort.1.gz because associated file /usr/share/man/man1/mpifort.openmpi.1.gz (of link group mpi) doesn't exist
	Setting up libboost-python1.71-dev (1.71.0-6ubuntu6) ...
	Setting up mpi-default-bin (1.13) ...
	Setting up libcoarrays-openmpi-dev:arm64 (2.8.0-1) ...
	Setting up libboost-python-dev (1.71.0.0ubuntu2) ...
	Setting up libboost-graph-parallel1.71-dev (1.71.0-6ubuntu6) ...
	Setting up libopenmpi-dev:arm64 (4.0.3-0ubuntu1) ...
	update-alternatives: using /usr/lib/aarch64-linux-gnu/openmpi/include to provide /usr/include/aarch64-linux-gnu/mpi (mpi-aarch64-linux-gnu) in auto mode
	Setting up libboost-mpi-python1.71.0 (1.71.0-6ubuntu6) ...
	Setting up libboost-graph-parallel-dev (1.71.0.0ubuntu2) ...
	Setting up mpi-default-dev (1.13) ...
	Setting up libboost-mpi1.71-dev (1.71.0-6ubuntu6) ...
	Setting up libboost-mpi-python1.71-dev (1.71.0-6ubuntu6) ...
	Setting up libboost-mpi-python-dev (1.71.0.0ubuntu2) ...
	Setting up libboost-mpi-dev (1.71.0.0ubuntu2) ...
	Setting up libboost-all-dev (1.71.0.0ubuntu2) ...
	Processing triggers for libc-bin (2.31-0ubuntu9.14) ...
[ 9/22] RUN apt-get install -y git
	Reading package lists...
	Building dependency tree...
	Reading state information...
	The following additional packages will be installed:
	git-man less libcurl3-gnutls liberror-perl patch
	Suggested packages:
	gettext-base git-daemon-run | git-daemon-sysvinit git-doc git-el git-email
	git-gui gitk gitweb git-cvs git-mediawiki git-svn ed diffutils-doc
	The following NEW packages will be installed:
	git git-man less libcurl3-gnutls liberror-perl patch
	0 upgraded, 6 newly installed, 0 to remove and 0 not upgraded.
	Need to get 5781 kB of archives.
	After this operation, 39.4 MB of additional disk space will be used.
	Get:1 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 less arm64 551-1ubuntu0.1 [119 kB]
	Get:2 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libcurl3-gnutls arm64 7.68.0-1ubuntu2.21 [214 kB]
	Get:3 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 liberror-perl all 0.17029-1 [26.5 kB]
	Get:4 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 git-man all 1:2.25.1-1ubuntu3.11 [887 kB]
	Get:5 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 git arm64 1:2.25.1-1ubuntu3.11 [4437 kB]
	Get:6 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 patch arm64 2.7.6-6 [98.1 kB]
	debconf: delaying package configuration, since apt-utils is not installed
	Fetched 5781 kB in 1s (6417 kB/s)
	Selecting previously unselected package less.
	(Reading database ...(Reading database ... 5%
(Reading database ... 10%
(Reading database ... 15%
(Reading database ... 20%
(Reading database ... 25%
(Reading database ... 30%
(Reading database ... 35%
(Reading database ... 40%
(Reading database ... 45%
(Reading database ... 50%
(Reading database ... 55%
(Reading database ... 60%
(Reading database ... 65%
(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%
(Reading database ... 38613 files and directories currently installed.)
	Preparing to unpack .../0-less_551-1ubuntu0.1_arm64.deb ...
	Unpacking less (551-1ubuntu0.1) ...
	Selecting previously unselected package libcurl3-gnutls:arm64.
	Preparing to unpack .../1-libcurl3-gnutls_7.68.0-1ubuntu2.21_arm64.deb ...
	Unpacking libcurl3-gnutls:arm64 (7.68.0-1ubuntu2.21) ...
	Selecting previously unselected package liberror-perl.
	Preparing to unpack .../2-liberror-perl_0.17029-1_all.deb ...
	Unpacking liberror-perl (0.17029-1) ...
	Selecting previously unselected package git-man.
	Preparing to unpack .../3-git-man_1%3a2.25.1-1ubuntu3.11_all.deb ...
	Unpacking git-man (1:2.25.1-1ubuntu3.11) ...
	Selecting previously unselected package git.
	Preparing to unpack .../4-git_1%3a2.25.1-1ubuntu3.11_arm64.deb ...
	Unpacking git (1:2.25.1-1ubuntu3.11) ...
	Selecting previously unselected package patch.
	Preparing to unpack .../5-patch_2.7.6-6_arm64.deb ...
	Unpacking patch (2.7.6-6) ...
	Setting up less (551-1ubuntu0.1) ...
	Setting up libcurl3-gnutls:arm64 (7.68.0-1ubuntu2.21) ...
	Setting up liberror-perl (0.17029-1) ...
	Setting up patch (2.7.6-6) ...
	Setting up git-man (1:2.25.1-1ubuntu3.11) ...
	Setting up git (1:2.25.1-1ubuntu3.11) ...
	Processing triggers for libc-bin (2.31-0ubuntu9.14) ...
	Processing triggers for mime-support (3.64ubuntu1) ...
[10/22] RUN apt-get install -y rsync
	Reading package lists...
	Building dependency tree...
	Reading state information...
	The following additional packages will be installed:
	libpopt0
	Suggested packages:
	openssh-server
	The following NEW packages will be installed:
	libpopt0 rsync
	0 upgraded, 2 newly installed, 0 to remove and 0 not upgraded.
	Need to get 332 kB of archives.
	After this operation, 825 kB of additional disk space will be used.
	Get:1 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libpopt0 arm64 1.16-14 [25.1 kB]
	Get:2 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 rsync arm64 3.1.3-8ubuntu0.7 [307 kB]
	debconf: delaying package configuration, since apt-utils is not installed
	Fetched 332 kB in 0s (1546 kB/s)
	Selecting previously unselected package libpopt0:arm64.
	(Reading database ...(Reading database ... 5%
(Reading database ... 10%
(Reading database ... 15%
(Reading database ... 20%
(Reading database ... 25%
(Reading database ... 30%
(Reading database ... 35%
(Reading database ... 40%
(Reading database ... 45%
(Reading database ... 50%
(Reading database ... 55%
(Reading database ... 60%
(Reading database ... 65%
(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%
(Reading database ... 39583 files and directories currently installed.)
	Preparing to unpack .../libpopt0_1.16-14_arm64.deb ...
	Unpacking libpopt0:arm64 (1.16-14) ...
	Selecting previously unselected package rsync.
	Preparing to unpack .../rsync_3.1.3-8ubuntu0.7_arm64.deb ...
	Unpacking rsync (3.1.3-8ubuntu0.7) ...
	Setting up libpopt0:arm64 (1.16-14) ...
	Setting up rsync (3.1.3-8ubuntu0.7) ...
	invoke-rc.d: could not determine current runlevel
	invoke-rc.d: policy-rc.d denied execution of start.
	Processing triggers for libc-bin (2.31-0ubuntu9.14) ...
[11/22] RUN apt-get install -y python3-pip
	Reading package lists...
	Building dependency tree...
	Reading state information...
	The following additional packages will be installed:
	build-essential dirmngr dpkg-dev fakeroot g++ g++-9 gnupg gnupg-l10n
	gnupg-utils gpg gpg-agent gpg-wks-client gpg-wks-server gpgconf gpgsm
	libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl
	libassuan0 libfakeroot libksba8 libnpth0 libstdc++-9-dev pinentry-curses
	python-pip-whl python3-pkg-resources python3-setuptools python3-wheel
	Suggested packages:
	dbus-user-session libpam-systemd pinentry-gnome3 tor debian-keyring
	gcc-9-doc parcimonie xloadimage scdaemon libstdc++-9-doc pinentry-doc
	python-setuptools-doc
	The following NEW packages will be installed:
	build-essential dirmngr dpkg-dev fakeroot g++ g++-9 gnupg gnupg-l10n
	gnupg-utils gpg gpg-agent gpg-wks-client gpg-wks-server gpgconf gpgsm
	libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl
	libassuan0 libfakeroot libksba8 libnpth0 libstdc++-9-dev pinentry-curses
	python-pip-whl python3-pip python3-pkg-resources python3-setuptools
	python3-wheel
	0 upgraded, 29 newly installed, 0 to remove and 0 not upgraded.
	Need to get 14.3 MB of archives.
	After this operation, 55.0 MB of additional disk space will be used.
	Get:1 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 python3-pkg-resources all 45.2.0-1ubuntu0.1 [130 kB]
	Get:2 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libstdc++-9-dev arm64 9.4.0-1ubuntu1~20.04.2 [1687 kB]
	Get:3 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 g++-9 arm64 9.4.0-1ubuntu1~20.04.2 [6843 kB]
	Get:4 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 g++ arm64 4:9.3.0-1ubuntu2 [1592 B]
	Get:5 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 dpkg-dev all 1.19.7ubuntu3.2 [679 kB]
	Get:6 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 build-essential arm64 12.8ubuntu1.1 [4664 B]
	Get:7 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libassuan0 arm64 2.5.3-7ubuntu2 [33.1 kB]
	Get:8 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 gpgconf arm64 2.2.19-3ubuntu2.2 [117 kB]
	Get:9 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 libksba8 arm64 1.3.5-2ubuntu0.20.04.2 [89.1 kB]
	Get:10 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libnpth0 arm64 1.6-1 [7440 B]
	Get:11 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 dirmngr arm64 2.2.19-3ubuntu2.2 [311 kB]
	Get:12 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libfakeroot arm64 1.24-1 [26.0 kB]
	Get:13 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 fakeroot arm64 1.24-1 [61.9 kB]
	Get:14 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 gnupg-l10n all 2.2.19-3ubuntu2.2 [51.7 kB]
	Get:15 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 gnupg-utils arm64 2.2.19-3ubuntu2.2 [441 kB]
	Get:16 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 gpg arm64 2.2.19-3ubuntu2.2 [442 kB]
	Get:17 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 pinentry-curses arm64 1.1.0-3build1 [34.3 kB]
	Get:18 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 gpg-agent arm64 2.2.19-3ubuntu2.2 [216 kB]
	Get:19 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 gpg-wks-client arm64 2.2.19-3ubuntu2.2 [89.4 kB]
	Get:20 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 gpg-wks-server arm64 2.2.19-3ubuntu2.2 [83.1 kB]
	Get:21 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 gpgsm arm64 2.2.19-3ubuntu2.2 [198 kB]
	Get:22 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 gnupg all 2.2.19-3ubuntu2.2 [259 kB]
	Get:23 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libalgorithm-diff-perl all 1.19.03-2 [46.6 kB]
	Get:24 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libalgorithm-diff-xs-perl arm64 0.04-6 [11.1 kB]
	Get:25 http://ports.ubuntu.com/ubuntu-ports focal/main arm64 libalgorithm-merge-perl all 0.08-3 [12.0 kB]
	Get:26 http://ports.ubuntu.com/ubuntu-ports focal-updates/universe arm64 python-pip-whl all 20.0.2-5ubuntu1.10 [1805 kB]
	Get:27 http://ports.ubuntu.com/ubuntu-ports focal-updates/main arm64 python3-setuptools all 45.2.0-1ubuntu0.1 [330 kB]
	Get:28 http://ports.ubuntu.com/ubuntu-ports focal-updates/universe arm64 python3-wheel all 0.34.2-1ubuntu0.1 [23.9 kB]
	Get:29 http://ports.ubuntu.com/ubuntu-ports focal-updates/universe arm64 python3-pip all 20.0.2-5ubuntu1.10 [231 kB]
	debconf: delaying package configuration, since apt-utils is not installed
	Fetched 14.3 MB in 2s (8639 kB/s)
	Selecting previously unselected package python3-pkg-resources.
	(Reading database ...(Reading database ... 5%
(Reading database ... 10%
(Reading database ... 15%
(Reading database ... 20%
(Reading database ... 25%
(Reading database ... 30%
(Reading database ... 35%
(Reading database ... 40%
(Reading database ... 45%
(Reading database ... 50%
(Reading database ... 55%
(Reading database ... 60%
(Reading database ... 65%
(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%
(Reading database ... 39619 files and directories currently installed.)
	Preparing to unpack .../00-python3-pkg-resources_45.2.0-1ubuntu0.1_all.deb ...
	Unpacking python3-pkg-resources (45.2.0-1ubuntu0.1) ...
	Selecting previously unselected package libstdc++-9-dev:arm64.
	Preparing to unpack .../01-libstdc++-9-dev_9.4.0-1ubuntu1~20.04.2_arm64.deb ...
	Unpacking libstdc++-9-dev:arm64 (9.4.0-1ubuntu1~20.04.2) ...
	Selecting previously unselected package g++-9.
	Preparing to unpack .../02-g++-9_9.4.0-1ubuntu1~20.04.2_arm64.deb ...
	Unpacking g++-9 (9.4.0-1ubuntu1~20.04.2) ...
	Selecting previously unselected package g++.
	Preparing to unpack .../03-g++_4%3a9.3.0-1ubuntu2_arm64.deb ...
	Unpacking g++ (4:9.3.0-1ubuntu2) ...
	Selecting previously unselected package dpkg-dev.
	Preparing to unpack .../04-dpkg-dev_1.19.7ubuntu3.2_all.deb ...
	Unpacking dpkg-dev (1.19.7ubuntu3.2) ...
	Selecting previously unselected package build-essential.
	Preparing to unpack .../05-build-essential_12.8ubuntu1.1_arm64.deb ...
	Unpacking build-essential (12.8ubuntu1.1) ...
	Selecting previously unselected package libassuan0:arm64.
	Preparing to unpack .../06-libassuan0_2.5.3-7ubuntu2_arm64.deb ...
	Unpacking libassuan0:arm64 (2.5.3-7ubuntu2) ...
	Selecting previously unselected package gpgconf.
	Preparing to unpack .../07-gpgconf_2.2.19-3ubuntu2.2_arm64.deb ...
	Unpacking gpgconf (2.2.19-3ubuntu2.2) ...
	Selecting previously unselected package libksba8:arm64.
	Preparing to unpack .../08-libksba8_1.3.5-2ubuntu0.20.04.2_arm64.deb ...
	Unpacking libksba8:arm64 (1.3.5-2ubuntu0.20.04.2) ...
	Selecting previously unselected package libnpth0:arm64.
	Preparing to unpack .../09-libnpth0_1.6-1_arm64.deb ...
	Unpacking libnpth0:arm64 (1.6-1) ...
	Selecting previously unselected package dirmngr.
	Preparing to unpack .../10-dirmngr_2.2.19-3ubuntu2.2_arm64.deb ...
	Unpacking dirmngr (2.2.19-3ubuntu2.2) ...
	Selecting previously unselected package libfakeroot:arm64.
	Preparing to unpack .../11-libfakeroot_1.24-1_arm64.deb ...
	Unpacking libfakeroot:arm64 (1.24-1) ...
	Selecting previously unselected package fakeroot.
	Preparing to unpack .../12-fakeroot_1.24-1_arm64.deb ...
	Unpacking fakeroot (1.24-1) ...
	Selecting previously unselected package gnupg-l10n.
	Preparing to unpack .../13-gnupg-l10n_2.2.19-3ubuntu2.2_all.deb ...
	Unpacking gnupg-l10n (2.2.19-3ubuntu2.2) ...
	Selecting previously unselected package gnupg-utils.
	Preparing to unpack .../14-gnupg-utils_2.2.19-3ubuntu2.2_arm64.deb ...
	Unpacking gnupg-utils (2.2.19-3ubuntu2.2) ...
	Selecting previously unselected package gpg.
	Preparing to unpack .../15-gpg_2.2.19-3ubuntu2.2_arm64.deb ...
	Unpacking gpg (2.2.19-3ubuntu2.2) ...
	Selecting previously unselected package pinentry-curses.
	Preparing to unpack .../16-pinentry-curses_1.1.0-3build1_arm64.deb ...
	Unpacking pinentry-curses (1.1.0-3build1) ...
	Selecting previously unselected package gpg-agent.
	Preparing to unpack .../17-gpg-agent_2.2.19-3ubuntu2.2_arm64.deb ...
	Unpacking gpg-agent (2.2.19-3ubuntu2.2) ...
	Selecting previously unselected package gpg-wks-client.
	Preparing to unpack .../18-gpg-wks-client_2.2.19-3ubuntu2.2_arm64.deb ...
	Unpacking gpg-wks-client (2.2.19-3ubuntu2.2) ...
	Selecting previously unselected package gpg-wks-server.
	Preparing to unpack .../19-gpg-wks-server_2.2.19-3ubuntu2.2_arm64.deb ...
	Unpacking gpg-wks-server (2.2.19-3ubuntu2.2) ...
	Selecting previously unselected package gpgsm.
	Preparing to unpack .../20-gpgsm_2.2.19-3ubuntu2.2_arm64.deb ...
	Unpacking gpgsm (2.2.19-3ubuntu2.2) ...
	Selecting previously unselected package gnupg.
	Preparing to unpack .../21-gnupg_2.2.19-3ubuntu2.2_all.deb ...
	Unpacking gnupg (2.2.19-3ubuntu2.2) ...
	Selecting previously unselected package libalgorithm-diff-perl.
	Preparing to unpack .../22-libalgorithm-diff-perl_1.19.03-2_all.deb ...
	Unpacking libalgorithm-diff-perl (1.19.03-2) ...
	Selecting previously unselected package libalgorithm-diff-xs-perl.
	Preparing to unpack .../23-libalgorithm-diff-xs-perl_0.04-6_arm64.deb ...
	Unpacking libalgorithm-diff-xs-perl (0.04-6) ...
	Selecting previously unselected package libalgorithm-merge-perl.
	Preparing to unpack .../24-libalgorithm-merge-perl_0.08-3_all.deb ...
	Unpacking libalgorithm-merge-perl (0.08-3) ...
	Selecting previously unselected package python-pip-whl.
	Preparing to unpack .../25-python-pip-whl_20.0.2-5ubuntu1.10_all.deb ...
	Unpacking python-pip-whl (20.0.2-5ubuntu1.10) ...
	Selecting previously unselected package python3-setuptools.
	Preparing to unpack .../26-python3-setuptools_45.2.0-1ubuntu0.1_all.deb ...
	Unpacking python3-setuptools (45.2.0-1ubuntu0.1) ...
	Selecting previously unselected package python3-wheel.
	Preparing to unpack .../27-python3-wheel_0.34.2-1ubuntu0.1_all.deb ...
	Unpacking python3-wheel (0.34.2-1ubuntu0.1) ...
	Selecting previously unselected package python3-pip.
	Preparing to unpack .../28-python3-pip_20.0.2-5ubuntu1.10_all.deb ...
	Unpacking python3-pip (20.0.2-5ubuntu1.10) ...
	Setting up python3-pkg-resources (45.2.0-1ubuntu0.1) ...
	Setting up libksba8:arm64 (1.3.5-2ubuntu0.20.04.2) ...
	Setting up python3-setuptools (45.2.0-1ubuntu0.1) ...
	Setting up libstdc++-9-dev:arm64 (9.4.0-1ubuntu1~20.04.2) ...
	Setting up libalgorithm-diff-perl (1.19.03-2) ...
	Setting up libnpth0:arm64 (1.6-1) ...
	Setting up libassuan0:arm64 (2.5.3-7ubuntu2) ...
	Setting up python3-wheel (0.34.2-1ubuntu0.1) ...
	Setting up libfakeroot:arm64 (1.24-1) ...
	Setting up dpkg-dev (1.19.7ubuntu3.2) ...
	Setting up fakeroot (1.24-1) ...
	update-alternatives: using /usr/bin/fakeroot-sysv to provide /usr/bin/fakeroot (fakeroot) in auto mode
	update-alternatives: warning: skip creation of /usr/share/man/man1/fakeroot.1.gz because associated file /usr/share/man/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist
	update-alternatives: warning: skip creation of /usr/share/man/man1/faked.1.gz because associated file /usr/share/man/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist
	update-alternatives: warning: skip creation of /usr/share/man/es/man1/fakeroot.1.gz because associated file /usr/share/man/es/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist
	update-alternatives: warning: skip creation of /usr/share/man/es/man1/faked.1.gz because associated file /usr/share/man/es/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist
	update-alternatives: warning: skip creation of /usr/share/man/fr/man1/fakeroot.1.gz because associated file /usr/share/man/fr/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist
	update-alternatives: warning: skip creation of /usr/share/man/fr/man1/faked.1.gz because associated file /usr/share/man/fr/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist
	update-alternatives: warning: skip creation of /usr/share/man/sv/man1/fakeroot.1.gz because associated file /usr/share/man/sv/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist
	update-alternatives: warning: skip creation of /usr/share/man/sv/man1/faked.1.gz because associated file /usr/share/man/sv/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist
	Setting up gnupg-l10n (2.2.19-3ubuntu2.2) ...
	Setting up g++-9 (9.4.0-1ubuntu1~20.04.2) ...
	Setting up g++ (4:9.3.0-1ubuntu2) ...
	update-alternatives: using /usr/bin/g++ to provide /usr/bin/c++ (c++) in auto mode
	update-alternatives: warning: skip creation of /usr/share/man/man1/c++.1.gz because associated file /usr/share/man/man1/g++.1.gz (of link group c++) doesn't exist
	Setting up gpgconf (2.2.19-3ubuntu2.2) ...
	Setting up build-essential (12.8ubuntu1.1) ...
	Setting up python-pip-whl (20.0.2-5ubuntu1.10) ...
	Setting up libalgorithm-diff-xs-perl (0.04-6) ...
	Setting up gpg (2.2.19-3ubuntu2.2) ...
	Setting up libalgorithm-merge-perl (0.08-3) ...
	Setting up gnupg-utils (2.2.19-3ubuntu2.2) ...
	Setting up pinentry-curses (1.1.0-3build1) ...
	Setting up gpg-agent (2.2.19-3ubuntu2.2) ...
	Setting up gpgsm (2.2.19-3ubuntu2.2) ...
	Setting up dirmngr (2.2.19-3ubuntu2.2) ...
	Setting up python3-pip (20.0.2-5ubuntu1.10) ...
	Setting up gpg-wks-server (2.2.19-3ubuntu2.2) ...
	Setting up gpg-wks-client (2.2.19-3ubuntu2.2) ...
	Setting up gnupg (2.2.19-3ubuntu2.2) ...
	Processing triggers for libc-bin (2.31-0ubuntu9.14) ...
[12/22] RUN cd /usr/src && git clone --branch docker-image https://github.com/omniscientoctopus/AI-Toolbox.git
	Cloning into 'AI-Toolbox'...
[13/22] WORKDIR /usr/src/AI-Toolbox
[14/22] RUN mkdir build
[15/22] RUN export CXX=/usr/bin/g++-10
[16/22] RUN cd build &&  cmake  -DCMAKE_CXX_COMPILER=/usr/bin/g++-10         -DMAKE_ALL=1         -DMAKE_PYTHON=1         -DAI_PYTHON_VERSION=3        -DAI_LOGGING_ENABLED=1 /usr/src/AI-Toolbox
	-- The CXX compiler identification is GNU 10.5.0
	-- Check for working CXX compiler: /usr/bin/g++-10
	-- Check for working CXX compiler: /usr/bin/g++-10 -- works
	-- Detecting CXX compiler ABI info
	-- Detecting CXX compiler ABI info - done
	-- Detecting CXX compile features
	-- Detecting CXX compile features - done
	-- Found Boost: /usr/lib/aarch64-linux-gnu/cmake/Boost-1.71.0/BoostConfig.cmake (found suitable version "1.71.0", minimum required is "1.67")
	-- Found Eigen3: /usr/include/eigen3 (Required is at least version "3.2.92")
	-- Performing Test LPSOLVE_LINKS_ALONE
	-- Performing Test LPSOLVE_LINKS_ALONE - Success
	-- Found LpSolve: /usr/lib/lp_solve/liblpsolve55.so
	-- Found Python: /usr/bin/python3.8 (found version "3.8.10") found components: Interpreter Development
	-- Found Boost: /usr/lib/aarch64-linux-gnu/cmake/Boost-1.71.0/BoostConfig.cmake (found suitable version "1.71.0", minimum required is "1.67") found components: python38
	-- Found Boost: /usr/lib/aarch64-linux-gnu/cmake/Boost-1.71.0/BoostConfig.cmake (found suitable version "1.71.0", minimum required is "1.67") found components: unit_test_framework
	
	### AI_TOOLBOX SETTINGS ###
	-- CMAKE_BUILD_TYPE:   Release
	-- IPO / LTO enabled
	[32m[m Building everything      (-DMAKE_ALL=1)
	[32m[m Building entire library  (-DMAKE_LIB=1)
	[32m[m Building MDP             (-DMAKE_MDP=1)
	[32m[m Building Factored MDP    (-DMAKE_FMDP=1)
	[32m[m Building POMDP           (-DMAKE_POMDP=1)
	[32m[m Building Tests           (-DMAKE_TESTS=1)
	[32m[m Building Examples        (-DMAKE_EXAMPLES=1)
	[32m[m Building Python bindings (-DMAKE_PYTHON=1)
	- Selected Python 3.8    (-DAI_PYTHON_VERSION=3)
	[32m[m Enabled runtime logging  (-DAI_LOGGING_ENABLED=1)
	
	-- Configuring done
	-- Generating done
	-- Build files have been written to: /usr/src/AI-Toolbox/build
[17/22] RUN cd build && make
	Scanning dependencies of target AIToolboxMDP
	[  1%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/Seeder.cpp.o
	[  1%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/Utils/Adam.cpp.o
	[  1%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/Utils/Combinatorics.cpp.o
	[  2%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/Utils/IO.cpp.o
	[  2%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/Utils/Probability.cpp.o
	[  2%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/Utils/Polytope.cpp.o
	[  2%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/Utils/StorageEigen.cpp.o
	[  3%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/Utils/LP/LpSolveWrapper.cpp.o
	[  3%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/Tools/Statistics.cpp.o
	[  3%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/Tools/CassandraParser.cpp.o
	[  3%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/Bandit/Experience.cpp.o
	[  4%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/Bandit/Policies/EpsilonPolicy.cpp.o
	[  4%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/Bandit/Policies/RandomPolicy.cpp.o
	[  4%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/Bandit/Policies/QGreedyPolicy.cpp.o
	[  4%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/Bandit/Policies/QSoftmaxPolicy.cpp.o
	[  5%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/Bandit/Policies/ThompsonSamplingPolicy.cpp.o
	[  5%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/Bandit/Policies/TopTwoThompsonSamplingPolicy.cpp.o
	[  5%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/Bandit/Policies/T3CPolicy.cpp.o
	[  5%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/Bandit/Policies/SuccessiveRejectsPolicy.cpp.o
	[  6%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/Bandit/Policies/LRPPolicy.cpp.o
	[  6%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/Bandit/Policies/ESRLPolicy.cpp.o
	[  6%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/MDP/Experience.cpp.o
	[  6%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/MDP/Utils.cpp.o
	[  7%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/MDP/Model.cpp.o
	[  7%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/MDP/SparseExperience.cpp.o
	[  7%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/MDP/SparseModel.cpp.o
	[  7%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/MDP/IO.cpp.o
	[  8%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/MDP/Algorithms/QLearning.cpp.o
	[  8%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/MDP/Algorithms/RLearning.cpp.o
	[  8%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/MDP/Algorithms/DoubleQLearning.cpp.o
	[  9%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/MDP/Algorithms/HystereticQLearning.cpp.o
	[  9%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/MDP/Algorithms/SARSA.cpp.o
	[  9%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/MDP/Algorithms/ExpectedSARSA.cpp.o
	[  9%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/MDP/Algorithms/SARSAL.cpp.o
	[ 10%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/MDP/Algorithms/ValueIteration.cpp.o
	[ 10%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/MDP/Algorithms/PolicyIteration.cpp.o
	[ 10%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/MDP/Algorithms/Utils/OffPolicyTemplate.cpp.o
	[ 10%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/MDP/Policies/PolicyWrapper.cpp.o
	[ 11%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/MDP/Policies/Policy.cpp.o
	[ 11%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/MDP/Policies/EpsilonPolicy.cpp.o
	[ 11%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/MDP/Policies/QPolicyInterface.cpp.o
	[ 11%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/MDP/Policies/QGreedyPolicy.cpp.o
	[ 12%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/MDP/Policies/QSoftmaxPolicy.cpp.o
	[ 12%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/MDP/Policies/WoLFPolicy.cpp.o
	[ 12%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/MDP/Policies/PGAAPPPolicy.cpp.o
	[ 12%] Building CXX object src/CMakeFiles/AIToolboxMDP.dir/MDP/Environments/Utils/GridWorld.cpp.o
	[ 13%] Linking CXX static library ../libAIToolboxMDP.a
	[ 13%] Built target AIToolboxMDP
	Scanning dependencies of target AIToolboxFMDP
	[ 13%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/Utils/Trie.cpp.o
	[ 13%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/Utils/FasterTrie.cpp.o
	[ 13%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/Utils/Core.cpp.o
	[ 14%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/Utils/FactoredMatrix.cpp.o
	[ 14%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/Utils/FactoredVectorOps.cpp.o
	[ 14%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/Utils/FactoredMatrix2DOps.cpp.o
	[ 14%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/Utils/BayesianNetwork.cpp.o
	[ 15%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/Bandit/Algorithms/Utils/VariableElimination.cpp.o
	[ 15%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/Bandit/Algorithms/Utils/MaxPlus.cpp.o
	[ 15%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/Bandit/Algorithms/Utils/LocalSearch.cpp.o
	[ 15%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/Bandit/Algorithms/Utils/ReusingIterativeLocalSearch.cpp.o
	[ 16%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/Bandit/Algorithms/Utils/MultiObjectiveVariableElimination.cpp.o
	[ 16%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/Bandit/Algorithms/Utils/UCVE.cpp.o
	[ 16%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/Bandit/Environments/MiningProblem.cpp.o
	[ 17%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/Bandit/Experience.cpp.o
	[ 17%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/Bandit/Policies/SingleActionPolicy.cpp.o
	[ 17%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/Bandit/Policies/RandomPolicy.cpp.o
	[ 17%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/Bandit/Policies/EpsilonPolicy.cpp.o
	[ 18%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/Bandit/Policies/ThompsonSamplingPolicy.cpp.o
	[ 18%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/Bandit/Policies/LLRPolicy.cpp.o
	[ 18%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/Bandit/Policies/MAUCEPolicy.cpp.o
	[ 18%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/Bandit/Policies/MARMaxPolicy.cpp.o
	[ 19%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/MDP/Utils.cpp.o
	[ 19%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/MDP/CooperativeExperience.cpp.o
	[ 19%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/MDP/CooperativeMaximumLikelihoodModel.cpp.o
	[ 19%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/MDP/CooperativeThompsonModel.cpp.o
	[ 20%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/MDP/CooperativeModel.cpp.o
	[ 20%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/MDP/Policies/EpsilonPolicy.cpp.o
	[ 20%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/MDP/Algorithms/Utils/FactoredLP.cpp.o
	[ 20%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/MDP/Algorithms/Utils/CPSQueue.cpp.o
	[ 21%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/MDP/Algorithms/SparseCooperativeQLearning.cpp.o
	[ 21%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/MDP/Algorithms/CooperativeQLearning.cpp.o
	[ 21%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/MDP/Algorithms/JointActionLearner.cpp.o
	[ 21%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/MDP/Algorithms/LinearProgramming.cpp.o
	[ 22%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/MDP/Environments/SysAdminRing.cpp.o
	[ 22%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/MDP/Environments/SysAdminGrid.cpp.o
	[ 22%] Building CXX object src/CMakeFiles/AIToolboxFMDP.dir/Factored/MDP/Environments/TigerAntelope.cpp.o
	[ 22%] Linking CXX static library ../libAIToolboxFMDP.a
	[ 22%] Built target AIToolboxFMDP
	Scanning dependencies of target AIToolboxPOMDP
	[ 22%] Building CXX object src/CMakeFiles/AIToolboxPOMDP.dir/POMDP/Utils.cpp.o
	[ 22%] Building CXX object src/CMakeFiles/AIToolboxPOMDP.dir/POMDP/IO.cpp.o
	[ 22%] Building CXX object src/CMakeFiles/AIToolboxPOMDP.dir/POMDP/Algorithms/AMDP.cpp.o
	[ 23%] Building CXX object src/CMakeFiles/AIToolboxPOMDP.dir/POMDP/Algorithms/GapMin.cpp.o
	[ 23%] Building CXX object src/CMakeFiles/AIToolboxPOMDP.dir/POMDP/Algorithms/SARSOP.cpp.o
	/usr/src/AI-Toolbox/src/POMDP/Algorithms/SARSOP.cpp: In member function 'std::pair<double, double> AIToolbox::POMDP::SARSOP::LBPredictor::predict(size_t, const AIToolbox::POMDP::SARSOP::TreeNode&)':
	/usr/src/AI-Toolbox/src/POMDP/Algorithms/SARSOP.cpp:433:92: note: parameter passing for argument of type 'std::pair<double, double>' when C++17 is enabled changed to match C++14 in GCC 10.1
	433 |     std::pair<double, double> SARSOP::LBPredictor::predict(size_t id, const TreeNode & node) {
	|                                                                                            ^
	[ 23%] Building CXX object src/CMakeFiles/AIToolboxPOMDP.dir/POMDP/Algorithms/IncrementalPruning.cpp.o
	[ 23%] Building CXX object src/CMakeFiles/AIToolboxPOMDP.dir/POMDP/Algorithms/LinearSupport.cpp.o
	[ 24%] Building CXX object src/CMakeFiles/AIToolboxPOMDP.dir/POMDP/Algorithms/PBVI.cpp.o
	[ 24%] Building CXX object src/CMakeFiles/AIToolboxPOMDP.dir/POMDP/Algorithms/PERSEUS.cpp.o
	[ 24%] Building CXX object src/CMakeFiles/AIToolboxPOMDP.dir/POMDP/Algorithms/BlindStrategies.cpp.o
	[ 25%] Building CXX object src/CMakeFiles/AIToolboxPOMDP.dir/POMDP/Algorithms/FastInformedBound.cpp.o
	[ 25%] Building CXX object src/CMakeFiles/AIToolboxPOMDP.dir/POMDP/Algorithms/QMDP.cpp.o
	[ 25%] Building CXX object src/CMakeFiles/AIToolboxPOMDP.dir/POMDP/Algorithms/Witness.cpp.o
	[ 25%] Building CXX object src/CMakeFiles/AIToolboxPOMDP.dir/POMDP/Policies/Policy.cpp.o
	[ 26%] Linking CXX static library ../libAIToolboxPOMDP.a
	[ 26%] Built target AIToolboxPOMDP
	Scanning dependencies of target AIToolbox
	[ 26%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/AIToolboxWrappers.cpp.o
	[ 26%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/BanditWrappers.cpp.o
	[ 26%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDPWrappers.cpp.o
	[ 27%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/POMDPWrappers.cpp.o
	[ 27%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/FactoredWrappers.cpp.o
	[ 27%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/Utils.cpp.o
	[ 27%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/Types.cpp.o
	[ 28%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/Bandit/Experience.cpp.o
	[ 28%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/Bandit/Policies/PolicyInterface.cpp.o
	[ 28%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/Bandit/Policies/EpsilonPolicy.cpp.o
	[ 28%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/Bandit/Policies/RandomPolicy.cpp.o
	[ 29%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/Bandit/Policies/QGreedyPolicy.cpp.o
	[ 29%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/Bandit/Policies/QSoftmaxPolicy.cpp.o
	[ 29%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/Bandit/Policies/ThompsonSamplingPolicy.cpp.o
	[ 29%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/Bandit/Policies/TopTwoThompsonSamplingPolicy.cpp.o
	[ 30%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/Bandit/Policies/T3CPolicy.cpp.o
	[ 30%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/Bandit/Policies/SuccessiveRejectsPolicy.cpp.o
	[ 30%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/Bandit/Policies/LRPPolicy.cpp.o
	[ 30%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/Bandit/Policies/ESRLPolicy.cpp.o
	[ 31%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Types.cpp.o
	[ 31%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Utils.cpp.o
	[ 31%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/IO.cpp.o
	[ 31%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Experience.cpp.o
	[ 32%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/MaximumLikelihoodModel.cpp.o
	[ 32%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/SparseExperience.cpp.o
	[ 32%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/SparseMaximumLikelihoodModel.cpp.o
	[ 32%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Model.cpp.o
	[ 33%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/SparseModel.cpp.o
	[ 33%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/GenerativeModelPython.cpp.o
	[ 33%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Algorithms/QLearning.cpp.o
	[ 34%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Algorithms/RLearning.cpp.o
	[ 34%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Algorithms/DoubleQLearning.cpp.o
	[ 34%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Algorithms/HystereticQLearning.cpp.o
	[ 34%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Algorithms/SARSA.cpp.o
	[ 35%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Algorithms/ExpectedSARSA.cpp.o
	[ 35%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Algorithms/SARSAL.cpp.o
	[ 35%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Algorithms/QL.cpp.o
	[ 35%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Algorithms/ValueIteration.cpp.o
	[ 36%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Algorithms/PolicyIteration.cpp.o
	[ 36%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Algorithms/PrioritizedSweeping.cpp.o
	[ 36%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Algorithms/MCTS.cpp.o
	[ 36%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Policies/PolicyInterface.cpp.o
	[ 37%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Policies/QPolicyInterface.cpp.o
	[ 37%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Policies/Policy.cpp.o
	[ 37%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Policies/QGreedyPolicy.cpp.o
	[ 37%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Policies/QSoftmaxPolicy.cpp.o
	[ 38%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Policies/EpsilonPolicy.cpp.o
	[ 38%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Policies/WoLFPolicy.cpp.o
	[ 38%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Policies/PGAAPPPolicy.cpp.o
	[ 38%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Environments/Utils/GridWorld.cpp.o
	[ 39%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/MDP/Environments/SimpleEnvironments.cpp.o
	[ 39%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/POMDP/Types.cpp.o
	[ 39%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/POMDP/Utils.cpp.o
	[ 39%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/POMDP/IO.cpp.o
	[ 40%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/POMDP/Model.cpp.o
	[ 40%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/POMDP/SparseModel.cpp.o
	[ 40%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/POMDP/Algorithms/POMCP.cpp.o
	[ 41%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/POMDP/Algorithms/GapMin.cpp.o
	[ 41%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/POMDP/Algorithms/Witness.cpp.o
	[ 41%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/POMDP/Algorithms/IncrementalPruning.cpp.o
	[ 41%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/POMDP/Algorithms/LinearSupport.cpp.o
	[ 42%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/POMDP/Algorithms/QMDP.cpp.o
	[ 42%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/POMDP/Algorithms/RTBSS.cpp.o
	[ 42%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/POMDP/Algorithms/AMDP.cpp.o
	[ 42%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/POMDP/Algorithms/PERSEUS.cpp.o
	[ 43%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/POMDP/Algorithms/PBVI.cpp.o
	[ 43%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/POMDP/Policies/PolicyInterface.cpp.o
	[ 43%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/POMDP/Policies/Policy.cpp.o
	[ 43%] Building CXX object src/CMakeFiles/AIToolbox.dir/Python/Factored/MDP/Algorithms/JointActionLearner.cpp.o
	[ 44%] Linking CXX shared library ../AIToolbox.so
	lto-wrapper: warning: using serial compilation of 48 LTRANS jobs
	[ 44%] Built target AIToolbox
	Scanning dependencies of target POMDP_WitnessTests
	[ 45%] Building CXX object test/CMakeFiles/POMDP_WitnessTests.dir/POMDP/WitnessTests.cpp.o
	[ 45%] Linking CXX executable POMDP_WitnessTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 45%] Built target POMDP_WitnessTests
	Scanning dependencies of target POMDP_RTBSSTests
	[ 45%] Building CXX object test/CMakeFiles/POMDP_RTBSSTests.dir/POMDP/RTBSSTests.cpp.o
	[ 45%] Linking CXX executable POMDP_RTBSSTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 45%] Built target POMDP_RTBSSTests
	Scanning dependencies of target POMDP_PBVITests
	[ 46%] Building CXX object test/CMakeFiles/POMDP_PBVITests.dir/POMDP/PBVITests.cpp.o
	[ 46%] Linking CXX executable POMDP_PBVITests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 46%] Built target POMDP_PBVITests
	Scanning dependencies of target POMDP_LinearSupportTests
	[ 46%] Building CXX object test/CMakeFiles/POMDP_LinearSupportTests.dir/POMDP/LinearSupportTests.cpp.o
	[ 47%] Linking CXX executable POMDP_LinearSupportTests
	lto-wrapper: warning: using serial compilation of 3 LTRANS jobs
	[ 47%] Built target POMDP_LinearSupportTests
	Scanning dependencies of target POMDP_IncrementalPruningTests
	[ 47%] Building CXX object test/CMakeFiles/POMDP_IncrementalPruningTests.dir/POMDP/IncrementalPruningTests.cpp.o
	[ 47%] Linking CXX executable POMDP_IncrementalPruningTests
	lto-wrapper: warning: using serial compilation of 3 LTRANS jobs
	[ 47%] Built target POMDP_IncrementalPruningTests
	Scanning dependencies of target POMDP_SARSOPTests
	[ 48%] Building CXX object test/CMakeFiles/POMDP_SARSOPTests.dir/POMDP/SARSOPTests.cpp.o
	[ 48%] Linking CXX executable POMDP_SARSOPTests
	lto-wrapper: warning: using serial compilation of 3 LTRANS jobs
	/usr/src/AI-Toolbox/src/POMDP/Algorithms/SARSOP.cpp: In member function 'predict':
	/usr/src/AI-Toolbox/src/POMDP/Algorithms/SARSOP.cpp:408:51: note: parameter passing for argument of type 'struct pair' when C++17 is enabled changed to match C++14 in GCC 10.1
	408 |             auto [avg, err] = bin.predict(id, node);
	|                                                   ^
	[ 48%] Built target POMDP_SARSOPTests
	Scanning dependencies of target POMDP_TypesTests
	[ 49%] Building CXX object test/CMakeFiles/POMDP_TypesTests.dir/POMDP/TypesTests.cpp.o
	[ 49%] Linking CXX executable POMDP_TypesTests
	[ 49%] Built target POMDP_TypesTests
	Scanning dependencies of target Factored_MDP_CooperativePrioritizedSweepingTests
	[ 50%] Building CXX object test/CMakeFiles/Factored_MDP_CooperativePrioritizedSweepingTests.dir/Factored/MDP/CooperativePrioritizedSweepingTests.cpp.o
	[ 50%] Linking CXX executable Factored_MDP_CooperativePrioritizedSweepingTests
	lto-wrapper: warning: using serial compilation of 3 LTRANS jobs
	[ 50%] Built target Factored_MDP_CooperativePrioritizedSweepingTests
	Scanning dependencies of target Factored_MDP_JointActionLearnerTests
	[ 50%] Building CXX object test/CMakeFiles/Factored_MDP_JointActionLearnerTests.dir/Factored/MDP/JointActionLearnerTests.cpp.o
	[ 51%] Linking CXX executable Factored_MDP_JointActionLearnerTests
	[ 51%] Built target Factored_MDP_JointActionLearnerTests
	Scanning dependencies of target Factored_MDP_LinearProgrammingTests
	[ 51%] Building CXX object test/CMakeFiles/Factored_MDP_LinearProgrammingTests.dir/Factored/MDP/LinearProgrammingTests.cpp.o
	[ 51%] Linking CXX executable Factored_MDP_LinearProgrammingTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 51%] Built target Factored_MDP_LinearProgrammingTests
	Scanning dependencies of target Factored_MDP_SparseCooperativeQLearningTests
	[ 51%] Building CXX object test/CMakeFiles/Factored_MDP_SparseCooperativeQLearningTests.dir/Factored/MDP/SparseCooperativeQLearningTests.cpp.o
	[ 52%] Linking CXX executable Factored_MDP_SparseCooperativeQLearningTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 52%] Built target Factored_MDP_SparseCooperativeQLearningTests
	Scanning dependencies of target POMDP_POMCPTests
	[ 52%] Building CXX object test/CMakeFiles/POMDP_POMCPTests.dir/POMDP/POMCPTests.cpp.o
	[ 52%] Linking CXX executable POMDP_POMCPTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 52%] Built target POMDP_POMCPTests
	Scanning dependencies of target Factored_MDP_FactoredLPTests
	[ 53%] Building CXX object test/CMakeFiles/Factored_MDP_FactoredLPTests.dir/Factored/MDP/FactoredLPTests.cpp.o
	[ 53%] Linking CXX executable Factored_MDP_FactoredLPTests
	[ 53%] Built target Factored_MDP_FactoredLPTests
	Scanning dependencies of target Factored_MDP_CooperativeModelTests
	[ 53%] Building CXX object test/CMakeFiles/Factored_MDP_CooperativeModelTests.dir/Factored/MDP/CooperativeModelTests.cpp.o
	[ 53%] Linking CXX executable Factored_MDP_CooperativeModelTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 53%] Built target Factored_MDP_CooperativeModelTests
	Scanning dependencies of target MDP_Dyna2Tests
	[ 53%] Building CXX object test/CMakeFiles/MDP_Dyna2Tests.dir/MDP/Dyna2Tests.cpp.o
	[ 53%] Linking CXX executable MDP_Dyna2Tests
	[ 53%] Built target MDP_Dyna2Tests
	Scanning dependencies of target Factored_Bandit_MultiObjectiveVariableEliminationTests
	[ 54%] Building CXX object test/CMakeFiles/Factored_Bandit_MultiObjectiveVariableEliminationTests.dir/Factored/Bandit/MultiObjectiveVariableEliminationTests.cpp.o
	[ 54%] Linking CXX executable Factored_Bandit_MultiObjectiveVariableEliminationTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 54%] Built target Factored_Bandit_MultiObjectiveVariableEliminationTests
	Scanning dependencies of target MDP_ExpectedSARSATests
	[ 54%] Building CXX object test/CMakeFiles/MDP_ExpectedSARSATests.dir/MDP/ExpectedSARSATests.cpp.o
	[ 55%] Linking CXX executable MDP_ExpectedSARSATests
	[ 55%] Built target MDP_ExpectedSARSATests
	Scanning dependencies of target MDP_PGAAPPPolicyTests
	[ 55%] Building CXX object test/CMakeFiles/MDP_PGAAPPPolicyTests.dir/MDP/PGAAPPPolicyTests.cpp.o
	[ 55%] Linking CXX executable MDP_PGAAPPPolicyTests
	[ 55%] Built target MDP_PGAAPPPolicyTests
	Scanning dependencies of target MDP_QGreedyPolicyTests
	[ 55%] Building CXX object test/CMakeFiles/MDP_QGreedyPolicyTests.dir/MDP/QGreedyPolicyTests.cpp.o
	[ 55%] Linking CXX executable MDP_QGreedyPolicyTests
	[ 55%] Built target MDP_QGreedyPolicyTests
	Scanning dependencies of target MDP_QLearningTests
	[ 55%] Building CXX object test/CMakeFiles/MDP_QLearningTests.dir/MDP/QLearningTests.cpp.o
	[ 55%] Linking CXX executable MDP_QLearningTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 55%] Built target MDP_QLearningTests
	Scanning dependencies of target Bandit_ModelTests
	[ 55%] Building CXX object test/CMakeFiles/Bandit_ModelTests.dir/Bandit/ModelTests.cpp.o
	[ 55%] Linking CXX executable Bandit_ModelTests
	[ 55%] Built target Bandit_ModelTests
	Scanning dependencies of target MDP_MaximumLikelihoodModelTests
	[ 55%] Building CXX object test/CMakeFiles/MDP_MaximumLikelihoodModelTests.dir/MDP/MaximumLikelihoodModelTests.cpp.o
	[ 55%] Linking CXX executable MDP_MaximumLikelihoodModelTests
	[ 55%] Built target MDP_MaximumLikelihoodModelTests
	Scanning dependencies of target MDP_WoLFPolicyTests
	[ 55%] Building CXX object test/CMakeFiles/MDP_WoLFPolicyTests.dir/MDP/WoLFPolicyTests.cpp.o
	[ 56%] Linking CXX executable MDP_WoLFPolicyTests
	[ 56%] Built target MDP_WoLFPolicyTests
	Scanning dependencies of target MDP_ExperienceTests
	[ 56%] Building CXX object test/CMakeFiles/MDP_ExperienceTests.dir/MDP/ExperienceTests.cpp.o
	[ 56%] Linking CXX executable MDP_ExperienceTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 56%] Built target MDP_ExperienceTests
	Scanning dependencies of target Bandit_ESRLPolicyTests
	[ 56%] Building CXX object test/CMakeFiles/Bandit_ESRLPolicyTests.dir/Bandit/ESRLPolicyTests.cpp.o
	[ 56%] Linking CXX executable Bandit_ESRLPolicyTests
	[ 56%] Built target Bandit_ESRLPolicyTests
	Scanning dependencies of target POMDP_UtilsTests
	[ 56%] Building CXX object test/CMakeFiles/POMDP_UtilsTests.dir/POMDP/UtilsTests.cpp.o
	[ 56%] Linking CXX executable POMDP_UtilsTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 56%] Built target POMDP_UtilsTests
	Scanning dependencies of target MDP_UtilsPolytopeTests
	[ 56%] Building CXX object test/CMakeFiles/MDP_UtilsPolytopeTests.dir/MDP/UtilsPolytopeTests.cpp.o
	[ 57%] Linking CXX executable MDP_UtilsPolytopeTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 57%] Built target MDP_UtilsPolytopeTests
	Scanning dependencies of target POMDP_SparseModelTests
	[ 57%] Building CXX object test/CMakeFiles/POMDP_SparseModelTests.dir/POMDP/SparseModelTests.cpp.o
	[ 57%] Linking CXX executable POMDP_SparseModelTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 57%] Built target POMDP_SparseModelTests
	Scanning dependencies of target Global_UtilsPruneTests
	[ 57%] Building CXX object test/CMakeFiles/Global_UtilsPruneTests.dir/UtilsPruneTests.cpp.o
	[ 57%] Building CXX object test/CMakeFiles/Global_UtilsPruneTests.dir/__/src/Seeder.cpp.o
	[ 58%] Building CXX object test/CMakeFiles/Global_UtilsPruneTests.dir/__/src/Tools/Statistics.cpp.o
	[ 58%] Building CXX object test/CMakeFiles/Global_UtilsPruneTests.dir/__/src/Utils/Adam.cpp.o
	[ 58%] Building CXX object test/CMakeFiles/Global_UtilsPruneTests.dir/__/src/Utils/Combinatorics.cpp.o
	[ 58%] Building CXX object test/CMakeFiles/Global_UtilsPruneTests.dir/__/src/Utils/IO.cpp.o
	[ 59%] Building CXX object test/CMakeFiles/Global_UtilsPruneTests.dir/__/src/Utils/Probability.cpp.o
	[ 59%] Building CXX object test/CMakeFiles/Global_UtilsPruneTests.dir/__/src/Utils/LP/LpSolveWrapper.cpp.o
	[ 59%] Linking CXX executable Global_UtilsPruneTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 59%] Built target Global_UtilsPruneTests
	Scanning dependencies of target MDP_TypesTests
	[ 59%] Building CXX object test/CMakeFiles/MDP_TypesTests.dir/MDP/TypesTests.cpp.o
	[ 59%] Linking CXX executable MDP_TypesTests
	[ 59%] Built target MDP_TypesTests
	Scanning dependencies of target Global_UtilsAdamTests
	[ 60%] Building CXX object test/CMakeFiles/Global_UtilsAdamTests.dir/UtilsAdamTests.cpp.o
	[ 60%] Building CXX object test/CMakeFiles/Global_UtilsAdamTests.dir/__/src/Seeder.cpp.o
	[ 60%] Building CXX object test/CMakeFiles/Global_UtilsAdamTests.dir/__/src/Tools/Statistics.cpp.o
	[ 60%] Building CXX object test/CMakeFiles/Global_UtilsAdamTests.dir/__/src/Utils/Adam.cpp.o
	[ 61%] Building CXX object test/CMakeFiles/Global_UtilsAdamTests.dir/__/src/Utils/Combinatorics.cpp.o
	[ 61%] Building CXX object test/CMakeFiles/Global_UtilsAdamTests.dir/__/src/Utils/IO.cpp.o
	[ 61%] Building CXX object test/CMakeFiles/Global_UtilsAdamTests.dir/__/src/Utils/Probability.cpp.o
	[ 61%] Building CXX object test/CMakeFiles/Global_UtilsAdamTests.dir/__/src/Utils/LP/LpSolveWrapper.cpp.o
	[ 62%] Linking CXX executable Global_UtilsAdamTests
	[ 62%] Built target Global_UtilsAdamTests
	Scanning dependencies of target POMDP_ModelTests
	[ 62%] Building CXX object test/CMakeFiles/POMDP_ModelTests.dir/POMDP/ModelTests.cpp.o
	[ 62%] Linking CXX executable POMDP_ModelTests
	lto-wrapper: warning: using serial compilation of 3 LTRANS jobs
	[ 62%] Built target POMDP_ModelTests
	Scanning dependencies of target MDP_LinearProgrammingTests
	[ 62%] Building CXX object test/CMakeFiles/MDP_LinearProgrammingTests.dir/MDP/LinearProgrammingTests.cpp.o
	[ 62%] Linking CXX executable MDP_LinearProgrammingTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 62%] Built target MDP_LinearProgrammingTests
	Scanning dependencies of target MDP_EnvironmentsTests
	[ 62%] Building CXX object test/CMakeFiles/MDP_EnvironmentsTests.dir/MDP/EnvironmentsTests.cpp.o
	[ 62%] Linking CXX executable MDP_EnvironmentsTests
	[ 62%] Built target MDP_EnvironmentsTests
	Scanning dependencies of target POMDP_PolicyTests
	[ 63%] Building CXX object test/CMakeFiles/POMDP_PolicyTests.dir/POMDP/PolicyTests.cpp.o
	[ 63%] Linking CXX executable POMDP_PolicyTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 63%] Built target POMDP_PolicyTests
	Scanning dependencies of target Factored_Bandit_VariableEliminationTests
	[ 63%] Building CXX object test/CMakeFiles/Factored_Bandit_VariableEliminationTests.dir/Factored/Bandit/VariableEliminationTests.cpp.o
	[ 63%] Linking CXX executable Factored_Bandit_VariableEliminationTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 63%] Built target Factored_Bandit_VariableEliminationTests
	Scanning dependencies of target MDP_SparseExperienceTests
	[ 63%] Building CXX object test/CMakeFiles/MDP_SparseExperienceTests.dir/MDP/SparseExperienceTests.cpp.o
	[ 63%] Linking CXX executable MDP_SparseExperienceTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 63%] Built target MDP_SparseExperienceTests
	Scanning dependencies of target POMDP_GapMinTests
	[ 63%] Building CXX object test/CMakeFiles/POMDP_GapMinTests.dir/POMDP/GapMinTests.cpp.o
	[ 64%] Linking CXX executable POMDP_GapMinTests
	lto-wrapper: warning: using serial compilation of 4 LTRANS jobs
	[ 64%] Built target POMDP_GapMinTests
	Scanning dependencies of target MDP_SparseMaximumLikelihoodModelTests
	[ 65%] Building CXX object test/CMakeFiles/MDP_SparseMaximumLikelihoodModelTests.dir/MDP/SparseMaximumLikelihoodModelTests.cpp.o
	[ 65%] Linking CXX executable MDP_SparseMaximumLikelihoodModelTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 65%] Built target MDP_SparseMaximumLikelihoodModelTests
	Scanning dependencies of target Global_UtilsIOTests
	[ 65%] Building CXX object test/CMakeFiles/Global_UtilsIOTests.dir/UtilsIOTests.cpp.o
	[ 66%] Building CXX object test/CMakeFiles/Global_UtilsIOTests.dir/__/src/Seeder.cpp.o
	[ 66%] Building CXX object test/CMakeFiles/Global_UtilsIOTests.dir/__/src/Tools/Statistics.cpp.o
	[ 66%] Building CXX object test/CMakeFiles/Global_UtilsIOTests.dir/__/src/Utils/Adam.cpp.o
	[ 66%] Building CXX object test/CMakeFiles/Global_UtilsIOTests.dir/__/src/Utils/Combinatorics.cpp.o
	[ 67%] Building CXX object test/CMakeFiles/Global_UtilsIOTests.dir/__/src/Utils/IO.cpp.o
	[ 67%] Building CXX object test/CMakeFiles/Global_UtilsIOTests.dir/__/src/Utils/Probability.cpp.o
	[ 67%] Building CXX object test/CMakeFiles/Global_UtilsIOTests.dir/__/src/Utils/LP/LpSolveWrapper.cpp.o
	[ 67%] Linking CXX executable Global_UtilsIOTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 67%] Built target Global_UtilsIOTests
	Scanning dependencies of target Bandit_QGreedyPolicyTests
	[ 67%] Building CXX object test/CMakeFiles/Bandit_QGreedyPolicyTests.dir/Bandit/QGreedyPolicyTests.cpp.o
	[ 68%] Linking CXX executable Bandit_QGreedyPolicyTests
	[ 68%] Built target Bandit_QGreedyPolicyTests
	Scanning dependencies of target POMDP_rPOMCPTests
	[ 68%] Building CXX object test/CMakeFiles/POMDP_rPOMCPTests.dir/POMDP/rPOMCPTests.cpp.o
	[ 68%] Linking CXX executable POMDP_rPOMCPTests
	[ 68%] Built target POMDP_rPOMCPTests
	Scanning dependencies of target Global_ToolsTests
	[ 68%] Building CXX object test/CMakeFiles/Global_ToolsTests.dir/ToolsTests.cpp.o
	[ 69%] Building CXX object test/CMakeFiles/Global_ToolsTests.dir/__/src/Seeder.cpp.o
	[ 69%] Building CXX object test/CMakeFiles/Global_ToolsTests.dir/__/src/Tools/Statistics.cpp.o
	[ 69%] Building CXX object test/CMakeFiles/Global_ToolsTests.dir/__/src/Utils/Adam.cpp.o
	[ 69%] Building CXX object test/CMakeFiles/Global_ToolsTests.dir/__/src/Utils/Combinatorics.cpp.o
	[ 70%] Building CXX object test/CMakeFiles/Global_ToolsTests.dir/__/src/Utils/IO.cpp.o
	[ 70%] Building CXX object test/CMakeFiles/Global_ToolsTests.dir/__/src/Utils/Probability.cpp.o
	[ 70%] Building CXX object test/CMakeFiles/Global_ToolsTests.dir/__/src/Utils/LP/LpSolveWrapper.cpp.o
	[ 70%] Linking CXX executable Global_ToolsTests
	[ 70%] Built target Global_ToolsTests
	Scanning dependencies of target Bandit_LRPPolicyTests
	[ 70%] Building CXX object test/CMakeFiles/Bandit_LRPPolicyTests.dir/Bandit/LRPPolicyTests.cpp.o
	/usr/src/AI-Toolbox/test/Bandit/LRPPolicyTests.cpp: In function 'std::pair<double, double> testPrisonersDilemma(unsigned int, unsigned int)':
	/usr/src/AI-Toolbox/test/Bandit/LRPPolicyTests.cpp:10:70: note: parameter passing for argument of type 'std::pair<double, double>' when C++17 is enabled changed to match C++14 in GCC 10.1
	10 | std::pair<double, double> testPrisonersDilemma(unsigned a, unsigned b) {
	|                                                                      ^
	[ 71%] Linking CXX executable Bandit_LRPPolicyTests
	/usr/src/AI-Toolbox/test/Bandit/LRPPolicyTests.cpp: In function 'testPrisonersDilemma':
	/usr/src/AI-Toolbox/test/Bandit/LRPPolicyTests.cpp:36:55: note: parameter passing for argument of type 'struct pair' when C++17 is enabled changed to match C++14 in GCC 10.1
	36 |         const auto [r1, r2] = testPrisonersDilemma(a,b);
	|                                                       ^
	[ 71%] Built target Bandit_LRPPolicyTests
	Scanning dependencies of target MDP_ModelTests
	[ 72%] Building CXX object test/CMakeFiles/MDP_ModelTests.dir/MDP/ModelTests.cpp.o
	[ 72%] Linking CXX executable MDP_ModelTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 72%] Built target MDP_ModelTests
	Scanning dependencies of target Bandit_SuccessiveRejectsPolicyTests
	[ 72%] Building CXX object test/CMakeFiles/Bandit_SuccessiveRejectsPolicyTests.dir/Bandit/SuccessiveRejectsPolicyTests.cpp.o
	[ 73%] Linking CXX executable Bandit_SuccessiveRejectsPolicyTests
	[ 73%] Built target Bandit_SuccessiveRejectsPolicyTests
	Scanning dependencies of target Bandit_QSoftmaxPolicyTests
	[ 73%] Building CXX object test/CMakeFiles/Bandit_QSoftmaxPolicyTests.dir/Bandit/QSoftmaxPolicyTests.cpp.o
	[ 73%] Linking CXX executable Bandit_QSoftmaxPolicyTests
	[ 73%] Built target Bandit_QSoftmaxPolicyTests
	Scanning dependencies of target POMDP_AMDPTests
	[ 73%] Building CXX object test/CMakeFiles/POMDP_AMDPTests.dir/POMDP/AMDPTests.cpp.o
	[ 73%] Linking CXX executable POMDP_AMDPTests
	lto-wrapper: warning: using serial compilation of 3 LTRANS jobs
	[ 73%] Built target POMDP_AMDPTests
	Scanning dependencies of target Factored_Bandit_MAUCEPolicyTests
	[ 73%] Building CXX object test/CMakeFiles/Factored_Bandit_MAUCEPolicyTests.dir/Factored/Bandit/MAUCEPolicyTests.cpp.o
	[ 73%] Linking CXX executable Factored_Bandit_MAUCEPolicyTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 73%] Built target Factored_Bandit_MAUCEPolicyTests
	Scanning dependencies of target MDP_ThompsonModelTests
	[ 73%] Building CXX object test/CMakeFiles/MDP_ThompsonModelTests.dir/MDP/ThompsonModelTests.cpp.o
	[ 73%] Linking CXX executable MDP_ThompsonModelTests
	[ 73%] Built target MDP_ThompsonModelTests
	Scanning dependencies of target Global_UtilsProbabilityTests
	[ 74%] Building CXX object test/CMakeFiles/Global_UtilsProbabilityTests.dir/UtilsProbabilityTests.cpp.o
	[ 74%] Building CXX object test/CMakeFiles/Global_UtilsProbabilityTests.dir/__/src/Seeder.cpp.o
	[ 74%] Building CXX object test/CMakeFiles/Global_UtilsProbabilityTests.dir/__/src/Tools/Statistics.cpp.o
	[ 74%] Building CXX object test/CMakeFiles/Global_UtilsProbabilityTests.dir/__/src/Utils/Adam.cpp.o
	[ 75%] Building CXX object test/CMakeFiles/Global_UtilsProbabilityTests.dir/__/src/Utils/Combinatorics.cpp.o
	[ 75%] Building CXX object test/CMakeFiles/Global_UtilsProbabilityTests.dir/__/src/Utils/IO.cpp.o
	[ 75%] Building CXX object test/CMakeFiles/Global_UtilsProbabilityTests.dir/__/src/Utils/Probability.cpp.o
	[ 75%] Building CXX object test/CMakeFiles/Global_UtilsProbabilityTests.dir/__/src/Utils/LP/LpSolveWrapper.cpp.o
	[ 76%] Linking CXX executable Global_UtilsProbabilityTests
	[ 76%] Built target Global_UtilsProbabilityTests
	Scanning dependencies of target Global_UtilsCoreTests
	[ 76%] Building CXX object test/CMakeFiles/Global_UtilsCoreTests.dir/UtilsCoreTests.cpp.o
	[ 76%] Building CXX object test/CMakeFiles/Global_UtilsCoreTests.dir/__/src/Seeder.cpp.o
	[ 77%] Building CXX object test/CMakeFiles/Global_UtilsCoreTests.dir/__/src/Tools/Statistics.cpp.o
	[ 77%] Building CXX object test/CMakeFiles/Global_UtilsCoreTests.dir/__/src/Utils/Adam.cpp.o
	[ 77%] Building CXX object test/CMakeFiles/Global_UtilsCoreTests.dir/__/src/Utils/Combinatorics.cpp.o
	[ 77%] Building CXX object test/CMakeFiles/Global_UtilsCoreTests.dir/__/src/Utils/IO.cpp.o
	[ 78%] Building CXX object test/CMakeFiles/Global_UtilsCoreTests.dir/__/src/Utils/Probability.cpp.o
	[ 78%] Building CXX object test/CMakeFiles/Global_UtilsCoreTests.dir/__/src/Utils/LP/LpSolveWrapper.cpp.o
	[ 78%] Linking CXX executable Global_UtilsCoreTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	Copying data to test folder...
	[ 78%] Built target Global_UtilsCoreTests
	Scanning dependencies of target Bandit_ThompsonSamplingPolicyTests
	[ 78%] Building CXX object test/CMakeFiles/Bandit_ThompsonSamplingPolicyTests.dir/Bandit/ThompsonSamplingPolicyTests.cpp.o
	[ 78%] Linking CXX executable Bandit_ThompsonSamplingPolicyTests
	[ 78%] Built target Bandit_ThompsonSamplingPolicyTests
	Scanning dependencies of target MDP_SparseModelTests
	[ 78%] Building CXX object test/CMakeFiles/MDP_SparseModelTests.dir/MDP/SparseModelTests.cpp.o
	[ 79%] Linking CXX executable MDP_SparseModelTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 79%] Built target MDP_SparseModelTests
	Scanning dependencies of target Factored_Bandit_ModelTests
	[ 79%] Building CXX object test/CMakeFiles/Factored_Bandit_ModelTests.dir/Factored/Bandit/ModelTests.cpp.o
	[ 79%] Linking CXX executable Factored_Bandit_ModelTests
	[ 79%] Built target Factored_Bandit_ModelTests
	Scanning dependencies of target MDP_HystereticQLearningTests
	[ 79%] Building CXX object test/CMakeFiles/MDP_HystereticQLearningTests.dir/MDP/HystereticQLearningTests.cpp.o
	[ 80%] Linking CXX executable MDP_HystereticQLearningTests
	[ 80%] Built target MDP_HystereticQLearningTests
	Scanning dependencies of target MDP_MCTSTests
	[ 80%] Building CXX object test/CMakeFiles/MDP_MCTSTests.dir/MDP/MCTSTests.cpp.o
	[ 81%] Linking CXX executable MDP_MCTSTests
	[ 81%] Built target MDP_MCTSTests
	Scanning dependencies of target Factored_MDP_CooperativeQLearningTests
	[ 81%] Building CXX object test/CMakeFiles/Factored_MDP_CooperativeQLearningTests.dir/Factored/MDP/CooperativeQLearningTests.cpp.o
	[ 81%] Linking CXX executable Factored_MDP_CooperativeQLearningTests
	lto-wrapper: warning: using serial compilation of 3 LTRANS jobs
	[ 81%] Built target Factored_MDP_CooperativeQLearningTests
	Scanning dependencies of target MDP_PolicyEvaluationTests
	[ 82%] Building CXX object test/CMakeFiles/MDP_PolicyEvaluationTests.dir/MDP/PolicyEvaluationTests.cpp.o
	[ 82%] Linking CXX executable MDP_PolicyEvaluationTests
	[ 82%] Built target MDP_PolicyEvaluationTests
	Scanning dependencies of target MDP_PolicyIterationTests
	[ 82%] Building CXX object test/CMakeFiles/MDP_PolicyIterationTests.dir/MDP/PolicyIterationTests.cpp.o
	[ 82%] Linking CXX executable MDP_PolicyIterationTests
	[ 82%] Built target MDP_PolicyIterationTests
	Scanning dependencies of target MDP_PrioritizedSweepingTests
	[ 83%] Building CXX object test/CMakeFiles/MDP_PrioritizedSweepingTests.dir/MDP/PrioritizedSweepingTests.cpp.o
	[ 83%] Linking CXX executable MDP_PrioritizedSweepingTests
	[ 83%] Built target MDP_PrioritizedSweepingTests
	Scanning dependencies of target MDP_QLTests
	[ 84%] Building CXX object test/CMakeFiles/MDP_QLTests.dir/MDP/QLTests.cpp.o
	[ 84%] Linking CXX executable MDP_QLTests
	[ 84%] Built target MDP_QLTests
	Scanning dependencies of target MDP_DoubleQLearningTests
	[ 84%] Building CXX object test/CMakeFiles/MDP_DoubleQLearningTests.dir/MDP/DoubleQLearningTests.cpp.o
	[ 85%] Linking CXX executable MDP_DoubleQLearningTests
	[ 85%] Built target MDP_DoubleQLearningTests
	Scanning dependencies of target Factored_Bandit_FlattenedModelTests
	[ 85%] Building CXX object test/CMakeFiles/Factored_Bandit_FlattenedModelTests.dir/Factored/Bandit/FlattenedModelTests.cpp.o
	[ 86%] Linking CXX executable Factored_Bandit_FlattenedModelTests
	[ 86%] Built target Factored_Bandit_FlattenedModelTests
	Scanning dependencies of target MDP_RetraceLTests
	[ 87%] Building CXX object test/CMakeFiles/MDP_RetraceLTests.dir/MDP/RetraceLTests.cpp.o
	[ 87%] Linking CXX executable MDP_RetraceLTests
	[ 87%] Built target MDP_RetraceLTests
	Scanning dependencies of target POMDP_FastInformedBoundTests
	[ 87%] Building CXX object test/CMakeFiles/POMDP_FastInformedBoundTests.dir/POMDP/FastInformedBoundTests.cpp.o
	[ 87%] Linking CXX executable POMDP_FastInformedBoundTests
	[ 87%] Built target POMDP_FastInformedBoundTests
	Scanning dependencies of target MDP_SARSATests
	[ 88%] Building CXX object test/CMakeFiles/MDP_SARSATests.dir/MDP/SARSATests.cpp.o
	[ 88%] Linking CXX executable MDP_SARSATests
	[ 88%] Built target MDP_SARSATests
	Scanning dependencies of target POMDP_BlindStrategiesTests
	[ 88%] Building CXX object test/CMakeFiles/POMDP_BlindStrategiesTests.dir/POMDP/BlindStrategiesTests.cpp.o
	[ 89%] Linking CXX executable POMDP_BlindStrategiesTests
	[ 89%] Built target POMDP_BlindStrategiesTests
	Scanning dependencies of target MDP_TreeBackupLTests
	[ 89%] Building CXX object test/CMakeFiles/MDP_TreeBackupLTests.dir/MDP/TreeBackupLTests.cpp.o
	[ 90%] Linking CXX executable MDP_TreeBackupLTests
	[ 90%] Built target MDP_TreeBackupLTests
	Scanning dependencies of target Factored_MDP_CooperativeExperienceTests
	[ 90%] Building CXX object test/CMakeFiles/Factored_MDP_CooperativeExperienceTests.dir/Factored/MDP/CooperativeExperienceTests.cpp.o
	[ 90%] Linking CXX executable Factored_MDP_CooperativeExperienceTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 90%] Built target Factored_MDP_CooperativeExperienceTests
	Scanning dependencies of target MDP_ValueIterationTests
	[ 90%] Building CXX object test/CMakeFiles/MDP_ValueIterationTests.dir/MDP/ValueIterationTests.cpp.o
	[ 90%] Linking CXX executable MDP_ValueIterationTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 90%] Built target MDP_ValueIterationTests
	Scanning dependencies of target Factored_UtilsTests
	[ 90%] Building CXX object test/CMakeFiles/Factored_UtilsTests.dir/Factored/UtilsTests.cpp.o
	[ 90%] Linking CXX executable Factored_UtilsTests
	lto-wrapper: warning: using serial compilation of 3 LTRANS jobs
	[ 90%] Built target Factored_UtilsTests
	Scanning dependencies of target MDP_SARSALTests
	[ 90%] Building CXX object test/CMakeFiles/MDP_SARSALTests.dir/MDP/SARSALTests.cpp.o
	[ 90%] Linking CXX executable MDP_SARSALTests
	[ 90%] Built target MDP_SARSALTests
	Scanning dependencies of target Factored_Bandit_LocalSearchTests
	[ 90%] Building CXX object test/CMakeFiles/Factored_Bandit_LocalSearchTests.dir/Factored/Bandit/LocalSearchTests.cpp.o
	[ 91%] Linking CXX executable Factored_Bandit_LocalSearchTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 91%] Built target Factored_Bandit_LocalSearchTests
	Scanning dependencies of target Factored_BayesianNetworkTests
	[ 92%] Building CXX object test/CMakeFiles/Factored_BayesianNetworkTests.dir/Factored/BayesianNetworkTests.cpp.o
	[ 92%] Linking CXX executable Factored_BayesianNetworkTests
	[ 92%] Built target Factored_BayesianNetworkTests
	Scanning dependencies of target MDP_DynaQTests
	[ 92%] Building CXX object test/CMakeFiles/MDP_DynaQTests.dir/MDP/DynaQTests.cpp.o
	[ 93%] Linking CXX executable MDP_DynaQTests
	[ 93%] Built target MDP_DynaQTests
	Scanning dependencies of target Factored_FactorGraphTests
	[ 93%] Building CXX object test/CMakeFiles/Factored_FactorGraphTests.dir/Factored/FactorGraphTests.cpp.o
	[ 93%] Linking CXX executable Factored_FactorGraphTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 93%] Built target Factored_FactorGraphTests
	Scanning dependencies of target Factored_FilterMapTests
	[ 94%] Building CXX object test/CMakeFiles/Factored_FilterMapTests.dir/Factored/FilterMapTests.cpp.o
	[ 94%] Linking CXX executable Factored_FilterMapTests
	lto-wrapper: warning: using serial compilation of 6 LTRANS jobs
	[ 94%] Built target Factored_FilterMapTests
	Scanning dependencies of target Factored_Bandit_ReusingIterativeLocalSearchTests
	[ 94%] Building CXX object test/CMakeFiles/Factored_Bandit_ReusingIterativeLocalSearchTests.dir/Factored/Bandit/ReusingIterativeLocalSearchTests.cpp.o
	[ 94%] Linking CXX executable Factored_Bandit_ReusingIterativeLocalSearchTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 94%] Built target Factored_Bandit_ReusingIterativeLocalSearchTests
	Scanning dependencies of target Factored_Bandit_UCVETests
	[ 95%] Building CXX object test/CMakeFiles/Factored_Bandit_UCVETests.dir/Factored/Bandit/UCVETests.cpp.o
	[ 95%] Linking CXX executable Factored_Bandit_UCVETests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 95%] Built target Factored_Bandit_UCVETests
	Scanning dependencies of target Factored_Bandit_MaxPlusTests
	[ 96%] Building CXX object test/CMakeFiles/Factored_Bandit_MaxPlusTests.dir/Factored/Bandit/MaxPlusTests.cpp.o
	[ 96%] Linking CXX executable Factored_Bandit_MaxPlusTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 96%] Built target Factored_Bandit_MaxPlusTests
	Scanning dependencies of target Factored_Bandit_LLRPolicyTests
	[ 96%] Building CXX object test/CMakeFiles/Factored_Bandit_LLRPolicyTests.dir/Factored/Bandit/LLRPolicyTests.cpp.o
	[ 96%] Linking CXX executable Factored_Bandit_LLRPolicyTests
	[ 96%] Built target Factored_Bandit_LLRPolicyTests
	Scanning dependencies of target Factored_MDP_CooperativeMaximumLikelihoodModelTests
	[ 97%] Building CXX object test/CMakeFiles/Factored_MDP_CooperativeMaximumLikelihoodModelTests.dir/Factored/MDP/CooperativeMaximumLikelihoodModelTests.cpp.o
	[ 97%] Linking CXX executable Factored_MDP_CooperativeMaximumLikelihoodModelTests
	lto-wrapper: warning: using serial compilation of 2 LTRANS jobs
	[ 97%] Built target Factored_MDP_CooperativeMaximumLikelihoodModelTests
	Scanning dependencies of target tiger_door
	[ 97%] Building CXX object examples/CMakeFiles/tiger_door.dir/POMDP/tiger_door.cpp.o
	[ 97%] Linking CXX executable tiger_door
	[ 97%] Built target tiger_door
	Scanning dependencies of target tiger_antelope_py
	[ 98%] Copying python tiger_antelope to example folder...
	[ 98%] Built target tiger_antelope_py
	Scanning dependencies of target tiger_door_py
	[ 99%] Copying python tiger_door to example folder...
	[ 99%] Built target tiger_door_py
	Scanning dependencies of target cliff
	[100%] Building CXX object examples/CMakeFiles/cliff.dir/MDP/cliff.cpp.o
	[100%] Linking CXX executable cliff
	[100%] Built target cliff
	Scanning dependencies of target tiger_antelope
	[100%] Building CXX object examples/CMakeFiles/tiger_antelope.dir/MDP/tiger_antelope.cpp.o
	[100%] Linking CXX executable tiger_antelope
	[100%] Built target tiger_antelope
[18/22] RUN cd build/examples && ./cliff QL
	Learning with QLearning...
	Starting training...
	Training over!
	
	. . . . . . . . . . . .
	. . . . . . . . . . . .
	. . . . . . . . . . . .
	@ C C C C C C C C C C .
	[4A. . . . . . . . . . . .
	. . . . . . . . . . . .
	@ . . . . . . . . . . .
	. C C C C C C C C C C .
	[4A. . . . . . . . . . . .
	. . . . . . . . . . . .
	. @ . . . . . . . . . .
	. C C C C C C C C C C .
	[4A. . . . . . . . . . . .
	. . . . . . . . . . . .
	. . @ . . . . . . . . .
	. C C C C C C C C C C .
	[4A. . . . . . . . . . . .
	. . . . . . . . . . . .
	. . . @ . . . . . . . .
	. C C C C C C C C C C .
	[4A. . . . . . . . . . . .
	. . . . . . . . . . . .
	. . . . @ . . . . . . .
	. C C C C C C C C C C .
	[4A. . . . . . . . . . . .
	. . . . . . . . . . . .
	. . . . . @ . . . . . .
	. C C C C C C C C C C .
	[4A. . . . . . . . . . . .
	. . . . . . . . . . . .
	. . . . . . @ . . . . .
	. C C C C C C C C C C .
	[4A. . . . . . . . . . . .
	. . . . . . . . . . . .
	. . . . . . . @ . . . .
	. C C C C C C C C C C .
	[4A. . . . . . . . . . . .
	. . . . . . . . . . . .
	. . . . . . . . @ . . .
	. C C C C C C C C C C .
	[4A. . . . . . . . . . . .
	. . . . . . . . . . . .
	. . . . . . . . . @ . .
	. C C C C C C C C C C .
	[4A. . . . . . . . . . . .
	. . . . . . . . . . . .
	. . . . . . . . . . @ .
	. C C C C C C C C C C .
	[4A. . . . . . . . . . . .
	. . . . . . . . . . . .
	. . . . . . . . . . . @
	. C C C C C C C C C C .
	[4A. . . . . . . . . . . .
	. . . . . . . . . . . .
	. . . . . . . . . . . .
	. C C C C C C C C C C @
[19/22] RUN cd build && cp AIToolbox.so examples/
[20/22] RUN cd build/examples && python3 ./tiger_antelope.py
	00:10:00 - Constructing MDP...
	00:10:01 - Solving MDP using ValueIteration(horizon=1000000, tolerance=0.001)
	00:10:01 - Converged: True
	.....
	...A.
	.@...
	.....
	.....
	[6A
	...A.
	.....
	..@..
	.....
	.....
	[6A
	...A.
	..@..
	.....
	.....
	.....
	[6A
	..@..
	...A.
	.....
	.....
	.....
	[6A
	.....
	..@A.
	.....
	.....
	.....
	[6A
	...A.
	...@.
	.....
	.....
	.....
	[6A
	...@A
	.....
	.....
	.....
	.....
	[6A
	....@
	.....
	.....
	.....
	....A
	[6A
	.....
	.....
	.....
	....A
	....@
	[6A
	.....
	.....
	.....
	....@
	.....
[21/22] RUN cd build/examples && python3 -c "import AIToolbox; help(AIToolbox.MDP)"
	Help on module AIToolbox.MDP in AIToolbox:
	
	NAME
	AIToolbox.MDP
	
	CLASSES
	Boost.Python.instance(builtins.object)
	DoubleQLearning
	ExpectedSARSA
	Experience
	GenerativeModelPython
	GridWorld
	HystereticQLearning
	IO
	MCTSGenerativeModelPython
	MCTSMaximumLikelihoodModel
	MCTSModel
	MCTSSparseMaximumLikelihoodModel
	MCTSSparseModel
	MaximumLikelihoodModel
	Model
	PolicyInterface
	EpsilonPolicy
	Policy
	QPolicyInterface
	QGreedyPolicy
	QSoftmaxPolicy
	WoLFPolicy
	PolicyIteration
	PrioritizedSweepingMaximumLikelihoodModel
	PrioritizedSweepingModel
	PrioritizedSweepingSparseMaximumLikelihoodModel
	PrioritizedSweepingSparseModel
	QL
	QLearning
	RLearning
	SARSA
	SARSAL
	SparseExperience
	SparseMaximumLikelihoodModel
	SparseModel
	ValueFunction
	ValueIteration
	vec_trace
	
	class DoubleQLearning(Boost.Python.instance)
	|  This class represents the double QLearning algorithm.
	|
	|  The QLearning algorithm is biased to overestimate the expected future
	|  reward during the Bellman equation update, as the bootstrapped max over
	|  the same QFunction is actually an unbiased estimator for the expected
	|  max, rather than the max expected.
	|
	|  This is a problem for certain classes of problems, and DoubleQLearning
	|  tries to fix that.
	|
	|  DoubleQLearning maintains two separate QFunctions, and in a given
	|  timestep one is selected randomly to be updated. The update has the same
	|  form as the standard QLearning update, except that the *other* QFunction
	|  is used to estimate the expected future reward. The math shows that this
	|  technique still results in a bias estimation, but in this case we tend
	|  to underestimate.
	|
	|  We can still try to counteract this with optimistic initialization, and
	|  the final result is often more stable than simple QLearning.
	|
	|  Since action selection should be performed w.r.t. both QFunctions,
	|  DoubleQLearning stores two things: the first QFunction, and the sum
	|  between the first QFunction and the second. The second QFunction is not
	|  stored explicitly, and is instead always computed on-the-fly when
	|  needed.
	|
	|  We do this so we can easily return the sum of both QFunction to apply a
	|  Policy to, without the need to store three separate QFunctions
	|  explicitly (lowering a bit the memory requirements).
	|
	|  If you are interested in the actual values stored in the two 'main'
	|  QFunctions, please use getQFunctionA() and getQFunctionB(). Note that
	|  getQFunctionB() will not return a reference!
	|
	|  Method resolution order:
	|      DoubleQLearning
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (int)S, (int)A [, (float)discount [, (float)alpha]]) -> None :
	|          Basic constructor.
	|
	|          The learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          @param S The size of the state space.
	|          @param A The size of the action space.
	|          @param discount The discount to use when learning.
	|          @param alpha The learning rate of the DoubleQLearning method.
	|
	|      __init__( (object)self, (MaximumLikelihoodModel)model [, (float)alpha]) -> None :
	|          Basic constructor from MaximumLikelihoodModel
	|
	|          The learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          This constructor copies the S and A and discount parameters from
	|          the supplied model. It does not keep the reference, so if the
	|          discount needs to change you'll need to update it here manually
	|          too.
	|
	|          @param model The MDP model that DoubleQLearning will use as a base.
	|          @param alpha The learning rate of the DoubleQLearning method.
	|
	|      __init__( (object)self, (SparseMaximumLikelihoodModel)model [, (float)alpha]) -> None :
	|          Basic constructor from SparseMaximumLikelihoodModel
	|
	|          The learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          This constructor copies the S and A and discount parameters
	|          from the supplied model. It does not conserve the reference.
	|
	|          @param model The MDP model that DoubleQLearning will use as a base.
	|          @param alpha The learning rate of the DoubleQLearning method.
	|
	|      __init__( (object)self, (Model)model [, (float)alpha]) -> None :
	|          Basic constructor from Model
	|
	|          The learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          This constructor copies the S and A and discount parameters
	|          from the supplied model. It does not conserve the reference.
	|
	|          @param model The MDP model that DoubleQLearning will use as a base.
	|          @param alpha The learning rate of the DoubleQLearning method.
	|
	|      __init__( (object)self, (SparseModel)model [, (float)alpha]) -> None :
	|          Basic constructor from SparseModel
	|
	|          The learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          This constructor copies the S and A and discount parameters
	|          from the supplied model. It does not conserve the reference.
	|
	|          @param model The MDP model that DoubleQLearning will use as a base.
	|          @param alpha The learning rate of the DoubleQLearning method.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getA(...)
	|      getA( (DoubleQLearning)self) -> int :
	|          This function returns the number of actions on which DoubleQLearning is working.
	|
	|  getDiscount(...)
	|      getDiscount( (DoubleQLearning)self) -> float :
	|          This function returns the currently set discount parameter.
	|
	|  getLearningRate(...)
	|      getLearningRate( (DoubleQLearning)self) -> float :
	|          This function will return the current set learning rate parameter.
	|
	|  getQFunction(...)
	|      getQFunction( (DoubleQLearning)self) -> Matrix2D :
	|          This function returns a reference to the internal 'sum' QFunction.
	|
	|          The QFunction that is returned does not contain 'true' values,
	|          but instead is the sum of the two QFunctions that are being
	|          updated by DoubleQLearning. This is to make it possible to
	|          select actions using standard policy classes.
	|
	|          The returned reference can be used to build Policies, for example
	|          MDP::QGreedyPolicy.
	|
	|          @return The internal 'sum' QFunction.
	|
	|  getQFunctionA(...)
	|      getQFunctionA( (DoubleQLearning)self) -> Matrix2D :
	|          This function returns a reference to the first internal QFunction.
	|
	|          The returned reference can be used to build Policies, for
	|          example MDP::QGreedyPolicy, but you should probably use
	|          getQFunction() for that.
	|
	|          @return The internal first QFunction.
	|
	|  getQFunctionB(...)
	|      getQFunctionB( (DoubleQLearning)self) -> Matrix2D :
	|          This function returns a copy to the second QFunction.
	|
	|          This QFunction is constructed on the fly, and so is not returned by reference!
	|
	|          @return What the second QFunction should be.
	|
	|  getS(...)
	|      getS( (DoubleQLearning)self) -> int :
	|          This function returns the number of states on which DoubleQLearning is working.
	|
	|  setDiscount(...)
	|      setDiscount( (DoubleQLearning)self, (float)d) -> None :
	|          This function sets the new discount parameter.
	|
	|          The discount parameter controls the amount that future rewards are considered
	|          by DoubleQLearning. If 1, then any reward is the same, if obtained now or in a million
	|          timesteps. Thus the algorithm will optimize overall reward accretion. When less
	|          than 1, rewards obtained in the presents are valued more than future rewards.
	|
	|          @param d The new discount factor.
	|
	|  setLearningRate(...)
	|      setLearningRate( (DoubleQLearning)self, (float)a) -> None :
	|          This function sets the learning rate parameter.
	|
	|          The learning parameter determines the speed at which the
	|          QFunction is modified with respect to new data. In fully
	|          deterministic environments (such as an agent moving through
	|          a grid, for example), this parameter can be safely set to
	|          1.0 for maximum learning.
	|
	|          On the other side, in stochastic environments, in order to
	|          converge this parameter should be higher when first starting
	|          to learn, and decrease slowly over time.
	|
	|          Otherwise it can be kept somewhat high if the environment
	|          dynamics change progressively, and the algorithm will adapt
	|          accordingly. The final behavior of DoubleQLearning is very
	|          dependent on this parameter.
	|
	|          The learning rate parameter must be > 0.0 and <= 1.0,
	|          otherwise the function will throw an std::invalid_argument.
	|
	|          @param a The new learning rate parameter.
	|
	|  stepUpdateQ(...)
	|      stepUpdateQ( (DoubleQLearning)self, (int)s, (int)a, (int)s1, (float)rew) -> None :
	|          This function updates the internal QFunction using the discount set during construction.
	|
	|          This function takes a single experience point and uses it to
	|          update the QFunction. This is a very efficient method to
	|          keep the QFunction up to date with the latest experience.
	|
	|          @param s The previous state.
	|          @param a The action performed.
	|          @param s1 The new state.
	|          @param rew The reward obtained.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class EpsilonPolicy(PolicyInterface)
	|  This class is a policy wrapper for epsilon action choice.
	|
	|  This class is used to wrap already existing policies to implement
	|  automatic exploratory behaviour (e.g. epsilon-greedy policies).
	|
	|  An epsilon-greedy policy is a policy that takes a greedy action a
	|  certain percentage of the time, and otherwise takes a random action.
	|  They are useful to force the agent to explore an unknown model, in order
	|  to gain new information to refine it and thus gain more reward.
	|
	|  Please note that to obtain an epsilon-greedy policy the wrapped
	|  policy needs to already be greedy with respect to the model.
	|
	|  Method resolution order:
	|      EpsilonPolicy
	|      PolicyInterface
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (PolicyInterface)p [, (float)epsilon]) -> None :
	|          Basic constructor.
	|
	|          This constructor saves the input policy and the epsilon
	|          parameter for later use.
	|
	|          The epsilon parameter must be >= 0.0 and <= 1.0,
	|          otherwise the constructor will throw an std::invalid_argument.
	|
	|          @param p The policy that is being extended.
	|          @param epsilon The parameter that controls the amount of exploration.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getActionProbability(...)
	|      getActionProbability( (EpsilonPolicy)self, (int)s, (int)a) -> float :
	|          This function returns the probability of taking the specified action in the specified state.
	|
	|          This function takes into account parameter epsilon
	|          while computing the final probability.
	|
	|          @param s The selected state.
	|          @param a The selected action.
	|
	|          @return The probability of taking the selected action in the specified state.
	|
	|  getEpsilon(...)
	|      getEpsilon( (EpsilonPolicy)self) -> float :
	|          This function will return the currently set epsilon parameter.
	|
	|  sampleAction(...)
	|      sampleAction( (EpsilonPolicy)self, (int)s) -> int :
	|          This function chooses an action for state s, following the policy distribution and epsilon.
	|
	|          This function has a probability of (1 - epsilon) of selecting
	|          a random action. Otherwise, it selects an action according
	|          to the distribution specified by the wrapped policy.
	|
	|          @param s The sampled state of the policy.
	|
	|          @return The chosen action.
	|
	|  setEpsilon(...)
	|      setEpsilon( (EpsilonPolicy)self, (float)e) -> None :
	|          This function sets the epsilon parameter.
	|
	|          The epsilon parameter determines the amount of exploration this
	|          policy will enforce when selecting actions. In particular
	|          actions are going to selected randomly with probability
	|          (1-epsilon), and are going to be selected following the
	|          underlying policy with probability epsilon.
	|
	|          The epsilon parameter must be >= 0.0 and <= 1.0,
	|          otherwise the function will do throw std::invalid_argument.
	|
	|          @param e The new epsilon parameter.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from PolicyInterface:
	|
	|  getA(...)
	|      getA( (PolicyInterface)self) -> int :
	|          This function returns the number of available actions to the agent.
	|
	|  getS(...)
	|      getS( (PolicyInterface)self) -> int :
	|          This function returns the number of states of the world.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class ExpectedSARSA(Boost.Python.instance)
	|  This algorithm is a subtle improvement over the SARSA algorithm.
	|
	|  \sa SARSA
	|
	|  The difference between this algorithm and the original SARSA algorithm
	|  lies in the value used to approximate the value for the next timestep.
	|  In standard SARSA this value is directly taken as the current
	|  approximation of the value of the QFunction for the newly sampled state
	|  and the next action to be performed (the final 'SA' in SAR'SA').
	|
	|  In Expected SARSA this value is instead replaced by the expected value
	|  for the newly sampled state, given the policy from which we will sample
	|  the next action. In this sense Expected SARSA is more similar to
	|  QLearning: where QLearning uses the max over the QFunction for the next
	|  state, Expected SARSA uses the future expectation over the current
	|  online policy.
	|
	|  This reduces considerably the variance of the updates performed, which
	|  in turn allows to somewhat increase the learning rate for the method,
	|  which allows Expected SARSA to learn faster than simple SARSA. All
	|  guarantees of normal SARSA are maintained.
	|
	|  Method resolution order:
	|      ExpectedSARSA
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (Matrix2D)qfun, (PolicyInterface)policy [, (float)discount [, (float)alpha]]) -> None :
	|          Basic constructor.
	|
	|          Note that differently from normal SARSA, ExpectedSARSA does not
	|          self-contain its own QFunction. This is because many policies
	|          are implemented in terms of a QFunction continuously updated by
	|          a method (e.g. QGreedyPolicy).
	|
	|          At the same time ExpectedSARSA needs this policy in order to be
	|          able to perform its expected value computation. In order to
	|          avoid having a chicken and egg problem, ExpectedSARSA takes a
	|          QFunction as parameter to allow the user to create it an use the
	|          same one for both ExpectedSARSA and the policy.
	|
	|          The learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          @param qfun The QFunction underlying the ExpectedSARSA algorithm.
	|          @param policy The policy used to select actions.
	|          @param discount The discount of the underlying MDP model.
	|          @param alpha The learning rate of the ExpectedSARSA method.
	|
	|      __init__( (object)self, (Matrix2D)qfun, (PolicyInterface)policy, (MaximumLikelihoodModel)model [, (float)alpha]) -> None :
	|          Basic constructor for MaximumLikelihoodModel.
	|
	|          Note that differently from normal SARSA, ExpectedSARSA does not
	|          self-contain its own QFunction. This is because many policies
	|          are implemented in terms of a QFunction continuously updated by
	|          a method (e.g. QGreedyPolicy).
	|
	|          At the same time ExpectedSARSA needs this policy in order to be
	|          able to perform its expected value computation. In order to
	|          avoid having a chicken and egg problem, ExpectedSARSA takes a
	|          QFunction as parameter to allow the user to create it an use the
	|          same one for both ExpectedSARSA and the policy.
	|
	|          The learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          This constructor copies the discount parameter from the supplied
	|          model. It does not keep the reference, so if the discount needs
	|          to change you'll need to update it here manually too.
	|
	|          @param qfun The QFunction underlying the ExpectedSARSA algorithm.
	|          @param policy The policy used to select actions.
	|          @param model The MDP model that ExpectedSARSA will use as a base.
	|          @param alpha The learning rate of the ExpectedSARSA method.
	|
	|      __init__( (object)self, (Matrix2D)qfun, (PolicyInterface)policy, (SparseMaximumLikelihoodModel)model [, (float)alpha]) -> None :
	|          Basic constructor for SparseMaximumLikelihoodModel.
	|
	|          Note that differently from normal SARSA, ExpectedSARSA does not
	|          self-contain its own QFunction. This is because many policies
	|          are implemented in terms of a QFunction continuously updated by
	|          a method (e.g. QGreedyPolicy).
	|
	|          At the same time ExpectedSARSA needs this policy in order to be
	|          able to perform its expected value computation. In order to
	|          avoid having a chicken and egg problem, ExpectedSARSA takes a
	|          QFunction as parameter to allow the user to create it an use the
	|          same one for both ExpectedSARSA and the policy.
	|
	|          The learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          This constructor copies the discount parameter from the supplied
	|          model. It does not keep the reference, so if the discount needs
	|          to change you'll need to update it here manually too.
	|
	|          @param qfun The QFunction underlying the ExpectedSARSA algorithm.
	|          @param policy The policy used to select actions.
	|          @param model The MDP model that ExpectedSARSA will use as a base.
	|          @param alpha The learning rate of the ExpectedSARSA method.
	|
	|      __init__( (object)self, (Matrix2D)qfun, (PolicyInterface)policy, (Model)model [, (float)alpha]) -> None :
	|          Basic constructor for Model.
	|
	|          Note that differently from normal SARSA, ExpectedSARSA does not
	|          self-contain its own QFunction. This is because many policies
	|          are implemented in terms of a QFunction continuously updated by
	|          a method (e.g. QGreedyPolicy).
	|
	|          At the same time ExpectedSARSA needs this policy in order to be
	|          able to perform its expected value computation. In order to
	|          avoid having a chicken and egg problem, ExpectedSARSA takes a
	|          QFunction as parameter to allow the user to create it an use the
	|          same one for both ExpectedSARSA and the policy.
	|
	|          The learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          This constructor copies the discount parameter from the supplied
	|          model. It does not keep the reference, so if the discount needs
	|          to change you'll need to update it here manually too.
	|
	|          @param qfun The QFunction underlying the ExpectedSARSA algorithm.
	|          @param policy The policy used to select actions.
	|          @param model The MDP model that ExpectedSARSA will use as a base.
	|          @param alpha The learning rate of the ExpectedSARSA method.
	|
	|      __init__( (object)self, (Matrix2D)qfun, (PolicyInterface)policy, (SparseModel)model [, (float)alpha]) -> None :
	|          Basic constructor for SparseModel.
	|
	|          Note that differently from normal SARSA, ExpectedSARSA does not
	|          self-contain its own QFunction. This is because many policies
	|          are implemented in terms of a QFunction continuously updated by
	|          a method (e.g. QGreedyPolicy).
	|
	|          At the same time ExpectedSARSA needs this policy in order to be
	|          able to perform its expected value computation. In order to
	|          avoid having a chicken and egg problem, ExpectedSARSA takes a
	|          QFunction as parameter to allow the user to create it an use the
	|          same one for both ExpectedSARSA and the policy.
	|
	|          The learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          This constructor copies the discount parameter from the supplied
	|          model. It does not keep the reference, so if the discount needs
	|          to change you'll need to update it here manually too.
	|
	|          @param qfun The QFunction underlying the ExpectedSARSA algorithm.
	|          @param policy The policy used to select actions.
	|          @param model The MDP model that ExpectedSARSA will use as a base.
	|          @param alpha The learning rate of the ExpectedSARSA method.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getA(...)
	|      getA( (ExpectedSARSA)self) -> int :
	|          This function returns the number of actions on which QLearning is working.
	|
	|  getDiscount(...)
	|      getDiscount( (ExpectedSARSA)self) -> float :
	|          This function returns the currently set discount parameter.
	|
	|  getLearningRate(...)
	|      getLearningRate( (ExpectedSARSA)self) -> float :
	|          This function will return the current set learning rate parameter.
	|
	|  getPolicy(...)
	|      getPolicy( (ExpectedSARSA)self) -> PolicyInterface :
	|          This function returns a reference to the policy used by ExpectedSARSA.
	|
	|  getQFunction(...)
	|      getQFunction( (ExpectedSARSA)self) -> Matrix2D :
	|          This function returns a reference to the internal QFunction.
	|
	|          The returned reference can be used to build Policies, for example
	|          MDP::QGreedyPolicy.
	|
	|  getS(...)
	|      getS( (ExpectedSARSA)self) -> int :
	|          This function returns the number of states on which QLearning is working.
	|
	|  setDiscount(...)
	|      setDiscount( (ExpectedSARSA)self, (float)d) -> None :
	|          This function sets the new discount parameter.
	|
	|          The discount parameter controls the amount that future rewards are considered
	|          by ExpectedSARSA. If 1, then any reward is the same, if obtained now or in a million
	|          timesteps. Thus the algorithm will optimize overall reward accretion. When less
	|          than 1, rewards obtained in the presents are valued more than future rewards.
	|
	|          @param d The new discount factor.
	|
	|  setLearningRate(...)
	|      setLearningRate( (ExpectedSARSA)self, (float)a) -> None :
	|          This function sets the learning rate parameter.
	|
	|          The learning parameter determines the speed at which the
	|          QFunction is modified with respect to new data. In fully
	|          deterministic environments (such as an agent moving through
	|          a grid, for example), this parameter can be safely set to
	|          1.0 for maximum learning.
	|
	|          On the other side, in stochastic environments, in order to
	|          converge this parameter should be higher when first starting
	|          to learn, and decrease slowly over time.
	|
	|          Otherwise it can be kept somewhat high if the environment
	|          dynamics change progressively, and the algorithm will adapt
	|          accordingly. The final behaviour of ExpectedSARSA is very
	|          dependent on this parameter.
	|
	|          The learning rate parameter must be > 0.0 and <= 1.0,
	|          otherwise the function will throw an std::invalid_argument.
	|
	|          @param a The new learning rate parameter.
	|
	|  stepUpdateQ(...)
	|      stepUpdateQ( (ExpectedSARSA)self, (int)s, (int)a, (int)s1, (float)rew) -> None :
	|          This function updates the internal QFunction using the discount set during construction.
	|
	|          This function takes a single experience point and uses it to
	|          update the QFunction. This is a very efficient method to
	|          keep the QFunction up to date with the latest experience.
	|
	|          Keep in mind that, since ExpectedSARSA needs to compute the
	|          QFunction for the currently used policy, it needs to know
	|          two consecutive state-action pairs, in order to correctly
	|          relate how the policy acts from state to state.
	|
	|          @param s The previous state.
	|          @param a The action performed.
	|          @param s1 The new state.
	|          @param rew The reward obtained.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class Experience(Boost.Python.instance)
	|  This class keeps track of registered events and rewards.
	|
	|  This class is a simple aggregator of events. It keeps track of both the
	|  number of times a particular transition has been visited, and the
	|  average reward gained per state-action pair (i.e. the maximum likelihood
	|  estimator of a QFunction from the data). It also computes the M2
	|  statistic for the rewards (avg sum of squares minus square avg).
	|
	|  It does not record each event separately (i.e. you can't extract the
	|  results of a particular transition in the past).
	|
	|  Method resolution order:
	|      Experience
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (int)s, (int)a) -> None :
	|          Basic constructor.
	|
	|          @param s The number of states of the world.
	|          @param a The number of actions available to the agent.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getA(...)
	|      getA( (Experience)self) -> int :
	|          This function returns the number of available actions to the agent.
	|
	|          @return The total number of actions.
	|
	|  getM2(...)
	|      getM2( (Experience)self, (int)s, (int)a) -> float :
	|          This function returns the M2 statistic for the specified state-action pair.
	|
	|          @param s     Old state.
	|          @param a     Performed action.
	|
	|  getReward(...)
	|      getReward( (Experience)self, (int)s, (int)a) -> float :
	|          This function returns the average reward obtained for the specified state-action pair.
	|
	|          @param s     Old state.
	|          @param a     Performed action.
	|
	|  getS(...)
	|      getS( (Experience)self) -> int :
	|          This function returns the number of states of the world.
	|
	|          @return The total number of states.
	|
	|  getTimesteps(...)
	|      getTimesteps( (Experience)self) -> int :
	|          This function returns the number of times that record has been called.
	|
	|  getVisits(...)
	|      getVisits( (Experience)self, (int)s, (int)a, (int)s1) -> int :
	|          This function returns the current recorded visits for a transitions.
	|
	|          @param s     Old state.
	|          @param a     Performed action.
	|          @param s1    New state.
	|
	|  getVisitsSum(...)
	|      getVisitsSum( (Experience)self, (int)s, (int)a) -> int :
	|          This function returns the number of transitions recorded that start with the specified state and action.
	|
	|          @param s     The initial state.
	|          @param a     Performed action.
	|
	|          @return The total number of transitions that start with the specified state-action pair.
	|
	|  record(...)
	|      record( (Experience)self, (int)s, (int)a, (int)s1, (float)rew) -> None :
	|          This function adds a new event to the recordings.
	|
	|          @param s     Old state.
	|          @param a     Performed action.
	|          @param s1    New state.
	|          @param rew   Obtained reward.
	|
	|  reset(...)
	|      reset( (Experience)self) -> None :
	|          This function resets all experienced rewards and transitions.
	|
	|  setM2Matrix(...)
	|      setM2Matrix( (Experience)self, (object)mm) -> None :
	|          Compatibility setter.
	|
	|          This function takes an arbitrary two dimensional
	|          container and tries to copy its contents into the
	|          M2s matrix.
	|
	|          Currently the Python wrappings support reading through native
	|          2d Python arrays (so [][]). As long as the dimensions are
	|          correct it should work.
	|
	|          @param mm The external M2s matrix.
	|
	|  setRewardMatrix(...)
	|      setRewardMatrix( (Experience)self, (object)r) -> None :
	|          Compatibility setter.
	|
	|          This function takes an arbitrary two dimensional
	|          container and tries to copy its contents into the
	|          rewards matrix.
	|
	|          Currently the Python wrappings support reading through native
	|          2d Python arrays (so [][]). As long as the dimensions are
	|          correct it should work.
	|
	|          @param r The external rewards matrix.
	|
	|  setVisitsTable(...)
	|      setVisitsTable( (Experience)self, (object)v) -> None :
	|          Compatibility setter.
	|
	|          This function takes an arbitrary three dimensional
	|          containers and tries to copy its contents into the
	|          visits table.
	|
	|          Currently the Python wrappings support reading through native
	|          3d Python arrays (so [][][]). As long as the dimensions are
	|          correct it should work.
	|
	|          @param v The external visits container.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class GenerativeModelPython(Boost.Python.instance)
	|  This class allows to import generative models from Python.
	|  This class wraps an instance of a Python class that provides generator
	|  methods to sample states and rewards from, so that one does not need to
	|  always specify transition and reward functions from Python.
	|
	|  Method resolution order:
	|      GenerativeModelPython
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (object)instance) -> None :
	|          Basic constructor.
	|          This constructor takes a Python object, which will be used to
	|          call the generative methods from C++.
	|
	|          This class expects the instance to have at least the following methods:
	|
	|          - getS(): returns the number of states of the environment.
	|          - getA(): returns the number of actions of the environment, in ALL states.
	|          - getDiscount(): returns the discount of the environment, [0, 1].
	|          - isTerminal(s): returns whether a given state is a terminal state.
	|          - sampleSR(s, a): returns a tuple containing new state and reward, from the input state and action.
	|
	|          @param instance The Python object instance to call methods on.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getA(...)
	|      getA( (GenerativeModelPython)self) -> int :
	|          This function returns the number of available actions to the agent.
	|
	|  getDiscount(...)
	|      getDiscount( (GenerativeModelPython)self) -> float :
	|          This function returns the currently set discount factor.
	|
	|  getS(...)
	|      getS( (GenerativeModelPython)self) -> int :
	|          This function returns the number of states of the world.
	|
	|  isTerminal(...)
	|      isTerminal( (GenerativeModelPython)self, (int)s) -> bool :
	|          This function returns whether a given state is a terminal.
	|
	|  sampleSR(...)
	|      sampleSR( (GenerativeModelPython)self, (int)s, (int)a) -> object :
	|          This function samples the MDP for the specified state action pair.
	|
	|          This function samples the model for simulated experience.
	|          The transition and reward functions are used to produce,
	|          from the state action pair inserted as arguments, a possible
	|          new state with respective reward.  The new state is picked
	|          from all possible states that the MDP allows transitioning
	|          to, each with probability equal to the same probability of
	|          the transition in the model. After a new state is picked,
	|          the reward is the corresponding reward contained in the
	|          reward function.
	|
	|          @param s The state that needs to be sampled.
	|          @param a The action that needs to be sampled.
	|
	|          @return A tuple containing a new state and a reward.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class GridWorld(Boost.Python.instance)
	|  This class represents a simple rectangular gridworld.
	|
	|  Method resolution order:
	|      GridWorld
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (int)width, (int)height [, (bool)torus]) -> None :
	|          Basic constructor.
	|
	|          @param width The number of columns in the world.
	|          @param height The number of rows in the world.
	|          @param torus Whether to join the edges of the grid as in a torus.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getHeight(...)
	|      getHeight( (GridWorld)self) -> int :
	|          This function returns the height of the GridWorld.
	|
	|  getS(...)
	|      getS( (GridWorld)self) -> int :
	|          This function returns the number of cells in the grid.
	|
	|  getWidth(...)
	|      getWidth( (GridWorld)self) -> int :
	|          This function returns the width of the GridWorld.
	|
	|  isTorus(...)
	|      isTorus( (GridWorld)self) -> bool :
	|          This function returns whether the GridWorld represents a torus.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class HystereticQLearning(Boost.Python.instance)
	|  This class represents the Hysteretic QLearning algorithm.
	|
	|  This algorithm is a very simple but powerful way to learn the
	|  optimal QFunction for an MDP model, where the transition and reward
	|  functions are unknown. It works in an offline fashion, meaning that
	|  it can be used even if the policy that the agent is currently using
	|  is not the optimal one, or is different by the one currently
	|  specified by the HystereticQLearning QFunction.
	|
	|  The algorithm functions quite like the normal QLearning algorithm, with
	|  a small difference: it has an additional learning parameter, beta.
	|
	|  One of the learning parameters (alpha) is used when the change to the
	|  underlying QFunction is positive. The other (beta), which should be kept
	|  lower than alpha, is used when the change is negative.
	|
	|  This is useful when using QLearning for multi-agent RL where each agent
	|  is independent. A multi-agent environment is non-stationary from the
	|  point of view of a single agent, which is disruptive for normal
	|  QLearning and generally prevents it to learn to coordinate with the
	|  other agents well.
	|
	|  By assigning a higher learning parameter to transitions resulting in a
	|  positive feedback, the agent insulates itself from bad results which
	|  happen when the other agents take exploratory actions.
	|
	|  Bad results are still guaranteed to be discovered, since the learning
	|  parameter is still greater than zero, but the algorithm tries to focus
	|  on the good things rather than the bad.
	|
	|  If the beta parameter is equal to the alpha, this becomes standard
	|  QLearning. When the beta parameter is zero, the algorithm becomes
	|  equivalent to Distributed QLearning.
	|
	|  Method resolution order:
	|      HystereticQLearning
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (int)S, (int)A [, (float)discount [, (float)alpha [, (float)beta]]]) -> None :
	|          Basic constructor.
	|
	|          The alpha learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          The beta learning rate must be >= 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument. It can be zero.
	|
	|          Keep in mind that the beta parameter should be lower than the
	|          alpha parameter, although this is not enforced.
	|
	|          @param S The size of the state space.
	|          @param A The size of the action space.
	|          @param discount The discount to use when learning.
	|          @param alpha The learning rate for positive updates.
	|          @param beta The learning rate for negative updates.
	|
	|      __init__( (object)self, (MaximumLikelihoodModel)model [, (float)alpha [, (float)beta]]) -> None :
	|          Basic constructor for MaximumLikelihoodModel.
	|
	|          The alpha learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          The beta learning rate must be >= 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument. It can be zero.
	|
	|          Keep in mind that the beta parameter should be lower than the
	|          alpha parameter, although this is not enforced.
	|
	|          This constructor copies the S and A and discount parameters from
	|          the supplied model. It does not keep the reference, so if the
	|          discount needs to change you'll need to update it here manually
	|          too.
	|
	|          @param model The MDP model that HystereticQLearning will use as a base.
	|          @param alpha The learning rate of the HystereticQLearning method.
	|          @param beta The learning rate for negative updates.
	|
	|      __init__( (object)self, (SparseMaximumLikelihoodModel)model [, (float)alpha [, (float)beta]]) -> None :
	|          Basic constructor for SparseMaximumLikelihoodModel.
	|
	|          The alpha learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          The beta learning rate must be >= 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument. It can be zero.
	|
	|          Keep in mind that the beta parameter should be lower than the
	|          alpha parameter, although this is not enforced.
	|
	|          This constructor copies the S and A and discount parameters from
	|          the supplied model. It does not keep the reference, so if the
	|          discount needs to change you'll need to update it here manually
	|          too.
	|
	|          @param model The MDP model that HystereticQLearning will use as a base.
	|          @param alpha The learning rate of the HystereticQLearning method.
	|          @param beta The learning rate for negative updates.
	|
	|      __init__( (object)self, (Model)model [, (float)alpha [, (float)beta]]) -> None :
	|          Basic constructor for Model.
	|
	|          The alpha learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          The beta learning rate must be >= 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument. It can be zero.
	|
	|          Keep in mind that the beta parameter should be lower than the
	|          alpha parameter, although this is not enforced.
	|
	|          This constructor copies the S and A and discount parameters from
	|          the supplied model. It does not keep the reference, so if the
	|          discount needs to change you'll need to update it here manually
	|          too.
	|
	|          @param model The MDP model that HystereticQLearning will use as a base.
	|          @param alpha The learning rate of the HystereticQLearning method.
	|          @param beta The learning rate for negative updates.
	|
	|      __init__( (object)self, (SparseModel)model [, (float)alpha [, (float)beta]]) -> None :
	|          Basic constructor for SparseModel.
	|
	|          The alpha learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          The beta learning rate must be >= 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument. It can be zero.
	|
	|          Keep in mind that the beta parameter should be lower than the
	|          alpha parameter, although this is not enforced.
	|
	|          This constructor copies the S and A and discount parameters from
	|          the supplied model. It does not keep the reference, so if the
	|          discount needs to change you'll need to update it here manually
	|          too.
	|
	|          @param model The MDP model that HystereticQLearning will use as a base.
	|          @param alpha The learning rate of the HystereticQLearning method.
	|          @param beta The learning rate for negative updates.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getA(...)
	|      getA( (HystereticQLearning)self) -> int :
	|          This function returns the number of actions on which HystereticQLearning is working.
	|
	|  getDiscount(...)
	|      getDiscount( (HystereticQLearning)self) -> float :
	|          This function returns the currently set discount parameter.
	|
	|  getNegativeLearningRate(...)
	|      getNegativeLearningRate( (HystereticQLearning)self) -> float :
	|          This function will return the currently set negative learning rate parameter.
	|
	|  getPositiveLearningRate(...)
	|      getPositiveLearningRate( (HystereticQLearning)self) -> float :
	|          This function will return the currently set positive learning rate parameter.
	|
	|  getQFunction(...)
	|      getQFunction( (HystereticQLearning)self) -> Matrix2D :
	|          This function returns a reference to the internal QFunction.
	|
	|          The returned reference can be used to build Policies, for example
	|          MDP::QGreedyPolicy.
	|
	|  getS(...)
	|      getS( (HystereticQLearning)self) -> int :
	|          This function returns the number of states on which HystereticQLearning is working.
	|
	|  setDiscount(...)
	|      setDiscount( (HystereticQLearning)self, (float)d) -> None :
	|          This function sets the new discount parameter.
	|
	|          The discount parameter controls the amount that future rewards
	|          are considered by HystereticQLearning. If 1, then any reward is
	|          the same, if obtained now or in a million timesteps. Thus the
	|          algorithm will optimize overall reward accretion. When less than
	|          1, rewards obtained in the presents are valued more than future
	|          rewards.
	|
	|          @param d The new discount factor.
	|
	|  setNegativeLearningRate(...)
	|      setNegativeLearningRate( (HystereticQLearning)self, (float)b) -> None :
	|          This function sets the learning rate parameter for negative updates.
	|
	|          The learning parameter determines the speed at which the
	|          QFunction is modified with respect to new data, when updates are
	|          negative.
	|
	|          Note that this parameter can be zero.
	|
	|          The learning rate parameter must be >= 0.0 and <= 1.0,
	|          otherwise the function will throw an std::invalid_argument.
	|
	|          @param b The new learning rate parameter for negative updates.
	|
	|  setPositiveLearningRate(...)
	|      setPositiveLearningRate( (HystereticQLearning)self, (float)a) -> None :
	|          This function sets the learning rate parameter for positive updates.
	|
	|          The learning parameter determines the speed at which the
	|          QFunction is modified with respect to new data, when updates are
	|          positive.
	|
	|          The learning rate parameter must be > 0.0 and <= 1.0,
	|          otherwise the function will throw an std::invalid_argument.
	|
	|          @param a The new learning rate parameter for positive updates.
	|
	|  stepUpdateQ(...)
	|      stepUpdateQ( (HystereticQLearning)self, (int)s, (int)a, (int)s1, (float)rew) -> None :
	|          This function updates the internal QFunction using the discount set during construction.
	|
	|          This function takes a single experience point and uses it to
	|          update the QFunction. This is a very efficient method to
	|          keep the QFunction up to date with the latest experience.
	|
	|          @param s The previous state.
	|          @param a The action performed.
	|          @param s1 The new state.
	|          @param rew The reward obtained.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class IO(Boost.Python.instance)
	|  This class wraps C++ MDP IO functionality.
	|
	|  While the models in Python can be pickled in order to save them,
	|  this does not allow direct interaction between C++ and Python code.
	|
	|  This class wraps the common AIToolbox operator<< and operator>> for
	|  MDP classes, so that they can be saved and loaded equally from both
	|  C++ and Python. The format is human-friendly (and thus
	|  space-unfriendly); if the models are supposed to only be used in
	|  Python, pickling is recommended.
	|
	|  The models are returned in strings as to avoid having Boost Python
	|  pass Python files to C++; the strings can be saved on files in
	|  whatever method you wish
	|
	|  Method resolution order:
	|      IO
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)arg1) -> None
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  readExperience(...)
	|      readExperience( (IO)arg1, (str)arg2, (Experience)arg3) -> None
	|
	|  readModel(...)
	|      readModel( (IO)arg1, (str)arg2, (Model)arg3) -> None
	|
	|  readSparseExperience(...)
	|      readSparseExperience( (IO)arg1, (str)arg2, (SparseExperience)arg3) -> None
	|
	|  readSparseModel(...)
	|      readSparseModel( (IO)arg1, (str)arg2, (SparseModel)arg3) -> None
	|
	|  writeExperience(...)
	|      writeExperience( (IO)arg1, (Experience)arg2) -> str
	|
	|  writeModel(...)
	|      writeModel( (IO)arg1, (Model)arg2) -> str
	|
	|  writeSparseExperience(...)
	|      writeSparseExperience( (IO)arg1, (SparseExperience)arg2) -> str
	|
	|  writeSparseModel(...)
	|      writeSparseModel( (IO)arg1, (SparseModel)arg2) -> str
	|
	|  ----------------------------------------------------------------------
	|  Data and other attributes defined here:
	|
	|  __instance_size__ = 24
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class MCTSGenerativeModelPython(Boost.Python.instance)
	|  This class represents the MCTS online planner using UCB1 for GenerativeModelPython.
	|
	|  NOTE: This algorithm is wrapped in Python, but as it uses the internal
	|  Models rather than a custom generative model to simulate rollouts it will
	|  probably be rather slow for interesting applications. You are of course
	|  welcome to try it out, but it is recommended that the generative model
	|  is written in C++.
	|
	|  This algorithm is an online planner for MDPs. As an online planner,
	|  it needs to have a generative model of the problem. This means that
	|  it only needs a way to sample transitions and rewards from the
	|  model, but it does not need to know directly the distribution
	|  probabilities for them.
	|
	|  MCTS plans for a single state at a time. It builds a tree structure
	|  progressively and action values are deduced as averages of the
	|  obtained rewards over rollouts. If the number of sample episodes is
	|  high enough, it is guaranteed to converge to the optimal solution.
	|
	|  At each rollout, we follow each action and resulting state within the
	|  tree from root to leaves. During this path we chose actions using an
	|  algorithm called UCT. What this does is privilege the most promising
	|  actions, while guaranteeing that in the limit every action will still
	|  be tried an infinite amount of times.
	|
	|  Once we arrive to a leaf in the tree, we then expand it with a
	|  single new node, representing a new state for the path we just
	|  followed. We then proceed outside the tree following a random
	|  policy, but this time we do not track which actions and states
	|  we actually experience. The final reward obtained by this random
	|  rollout policy is used to approximate the values for all nodes
	|  visited in this rollout inside the tree, before leaving it.
	|
	|  Since MCTS expands a tree, it can reuse work it has done if
	|  multiple action requests are done in order. To do so, it simply asks
	|  for the action that has been performed and its respective new state.
	|  Then it simply makes that root branch the new root, and starts
	|  again.
	|
	|  Method resolution order:
	|      MCTSGenerativeModelPython
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (GenerativeModelPython)m, (int)iterations, (float)exp) -> None :
	|          Basic constructor.
	|
	|          @param m The MDP model that MCTS will operate upon.
	|          @param iterations The number of episodes to run before completion.
	|          @param exp The exploration constant. This parameter is VERY important
	|                     to determine the final MCTS performance.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getExploration(...)
	|      getExploration( (MCTSGenerativeModelPython)self) -> float :
	|          This function returns the currently set exploration constant.
	|
	|  getIterations(...)
	|      getIterations( (MCTSGenerativeModelPython)self) -> int :
	|          This function returns the number of iterations performed to plan for an action.
	|
	|  getModel(...)
	|      getModel( (MCTSGenerativeModelPython)self) -> GenerativeModelPython :
	|          This function returns the MDP generative model being used.
	|
	|  sampleAction(...)
	|      sampleAction( (MCTSGenerativeModelPython)self, (int)s, (int)horizon) -> int :
	|          This function resets the internal graph and samples for the provided state and horizon.
	|
	|          @param s The initial state for the environment.
	|          @param horizon The horizon to plan for.
	|
	|          @return The best action.
	|
	|      sampleAction( (MCTSGenerativeModelPython)self, (int)a, (int)s1, (int)horizon) -> int :
	|          This function uses the internal graph to plan.
	|
	|          This function can be called after a previous call to
	|          sampleAction with a state. Otherwise, it will invoke it
	|          anyway with the provided next state.
	|
	|          If a graph is already present though, this function will
	|          select the branch defined by the input action and
	|          observation, and prune the rest. The search will be started
	|          using the existing graph: this should make search faster.
	|
	|          @param a The action taken in the last timestep.
	|          @param s1 The state experienced after the action was taken.
	|          @param horizon The horizon to plan for.
	|
	|          @return The best action.
	|
	|  setExploration(...)
	|      setExploration( (MCTSGenerativeModelPython)self, (float)exp) -> None :
	|          This function sets the new exploration constant for MCTS.
	|
	|          This parameter is EXTREMELY important to determine MCTS
	|          performance and, ultimately, convergence. In general it is
	|          better to find it empirically, by testing some values and
	|          see which one performs best. Tune this parameter, it really
	|          matters!
	|
	|          @param exp The new exploration constant.
	|
	|  setIterations(...)
	|      setIterations( (MCTSGenerativeModelPython)self, (int)iterations) -> None :
	|          This function sets the number of performed rollouts in MCTS.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class MCTSMaximumLikelihoodModel(Boost.Python.instance)
	|  This class represents the MCTS online planner using UCB1 for MaximumLikelihoodModel.
	|
	|  NOTE: This algorithm is wrapped in Python, but as it uses the internal
	|  Models rather than a custom generative model to simulate rollouts it will
	|  probably be rather slow for interesting applications. You are of course
	|  welcome to try it out, but it is recommended that the generative model
	|  is written in C++.
	|
	|  This algorithm is an online planner for MDPs. As an online planner,
	|  it needs to have a generative model of the problem. This means that
	|  it only needs a way to sample transitions and rewards from the
	|  model, but it does not need to know directly the distribution
	|  probabilities for them.
	|
	|  MCTS plans for a single state at a time. It builds a tree structure
	|  progressively and action values are deduced as averages of the
	|  obtained rewards over rollouts. If the number of sample episodes is
	|  high enough, it is guaranteed to converge to the optimal solution.
	|
	|  At each rollout, we follow each action and resulting state within the
	|  tree from root to leaves. During this path we chose actions using an
	|  algorithm called UCT. What this does is privilege the most promising
	|  actions, while guaranteeing that in the limit every action will still
	|  be tried an infinite amount of times.
	|
	|  Once we arrive to a leaf in the tree, we then expand it with a
	|  single new node, representing a new state for the path we just
	|  followed. We then proceed outside the tree following a random
	|  policy, but this time we do not track which actions and states
	|  we actually experience. The final reward obtained by this random
	|  rollout policy is used to approximate the values for allnodes
	|  visited in this rollout inside the tree, before leaving it.
	|
	|  Since MCTS expands a tree, it can reuse work it has done if
	|  multiple action requests are done in order. To do so, it simply asks
	|  for the action that has been performed and its respective new state.
	|  Then it simply makes that root branch the new root, and starts
	|  again.
	|
	|  Method resolution order:
	|      MCTSMaximumLikelihoodModel
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (MaximumLikelihoodModel)m, (int)iterations, (float)exp) -> None :
	|          Basic constructor.
	|
	|          @param m The MDP model that MCTS will operate upon.
	|          @param iterations The number of episodes to run before completion.
	|          @param exp The exploration constant. This parameter is VERY important
	|                     to determine the final MCTS performance.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getExploration(...)
	|      getExploration( (MCTSMaximumLikelihoodModel)self) -> float :
	|          This function returns the currently set exploration constant.
	|
	|  getIterations(...)
	|      getIterations( (MCTSMaximumLikelihoodModel)self) -> int :
	|          This function returns the number of iterations performed to plan for an action.
	|
	|  getModel(...)
	|      getModel( (MCTSMaximumLikelihoodModel)self) -> MaximumLikelihoodModel :
	|          This function returns the MDP generative model being used.
	|
	|  sampleAction(...)
	|      sampleAction( (MCTSMaximumLikelihoodModel)self, (int)s, (int)horizon) -> int :
	|          This function resets the internal graph and samples for the provided state and horizon.
	|
	|          @param s The initial state for the environment.
	|          @param horizon The horizon to plan for.
	|
	|          @return The best action.
	|
	|      sampleAction( (MCTSMaximumLikelihoodModel)self, (int)a, (int)s1, (int)horizon) -> int :
	|          This function uses the internal graph to plan.
	|
	|          This function can be called after a previous call to
	|          sampleAction with a state. Otherwise, it will invoke it
	|          anyway with the provided next state.
	|
	|          If a graph is already present though, this function will
	|          select the branch defined by the input action and
	|          observation, and prune the rest. The search will be started
	|          using the existing graph: this should make search faster.
	|
	|          @param a The action taken in the last timestep.
	|          @param s1 The state experienced after the action was taken.
	|          @param horizon The horizon to plan for.
	|
	|          @return The best action.
	|
	|  setExploration(...)
	|      setExploration( (MCTSMaximumLikelihoodModel)self, (float)exp) -> None :
	|          This function sets the new exploration constant for MCTS.
	|
	|          This parameter is EXTREMELY important to determine MCTS
	|          performance and, ultimately, convergence. In general it is
	|          better to find it empirically, by testing some values and
	|          see which one performs best. Tune this parameter, it really
	|          matters!
	|
	|          @param exp The new exploration constant.
	|
	|  setIterations(...)
	|      setIterations( (MCTSMaximumLikelihoodModel)self, (int)iterations) -> None :
	|          This function sets the number of performed rollouts in MCTS.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class MCTSModel(Boost.Python.instance)
	|  This class represents the MCTS online planner using UCB1 for Model.
	|
	|  NOTE: This algorithm is wrapped in Python, but as it uses the internal
	|  Models rather than a custom generative model to simulate rollouts it will
	|  probably be rather slow for interesting applications. You are of course
	|  welcome to try it out, but it is recommended that the generative model
	|  is written in C++.
	|
	|  This algorithm is an online planner for MDPs. As an online planner,
	|  it needs to have a generative model of the problem. This means that
	|  it only needs a way to sample transitions and rewards from the
	|  model, but it does not need to know directly the distribution
	|  probabilities for them.
	|
	|  MCTS plans for a single state at a time. It builds a tree structure
	|  progressively and action values are deduced as averages of the
	|  obtained rewards over rollouts. If the number of sample episodes is
	|  high enough, it is guaranteed to converge to the optimal solution.
	|
	|  At each rollout, we follow each action and resulting state within the
	|  tree from root to leaves. During this path we chose actions using an
	|  algorithm called UCT. What this does is privilege the most promising
	|  actions, while guaranteeing that in the limit every action will still
	|  be tried an infinite amount of times.
	|
	|  Once we arrive to a leaf in the tree, we then expand it with a
	|  single new node, representing a new state for the path we just
	|  followed. We then proceed outside the tree following a random
	|  policy, but this time we do not track which actions and states
	|  we actually experience. The final reward obtained by this random
	|  rollout policy is used to approximate the values for all nodes
	|  visited in this rollout inside the tree, before leaving it.
	|
	|  Since MCTS expands a tree, it can reuse work it has done if
	|  multiple action requests are done in order. To do so, it simply asks
	|  for the action that has been performed and its respective new state.
	|  Then it simply makes that root branch the new root, and starts
	|  again.
	|
	|  Method resolution order:
	|      MCTSModel
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (Model)m, (int)iterations, (float)exp) -> None :
	|          Basic constructor.
	|
	|          @param m The MDP model that MCTS will operate upon.
	|          @param iterations The number of episodes to run before completion.
	|          @param exp The exploration constant. This parameter is VERY important
	|                     to determine the final MCTS performance.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getExploration(...)
	|      getExploration( (MCTSModel)self) -> float :
	|          This function returns the currently set exploration constant.
	|
	|  getIterations(...)
	|      getIterations( (MCTSModel)self) -> int :
	|          This function returns the number of iterations performed to plan for an action.
	|
	|  getModel(...)
	|      getModel( (MCTSModel)self) -> Model :
	|          This function returns the MDP generative model being used.
	|
	|  sampleAction(...)
	|      sampleAction( (MCTSModel)self, (int)s, (int)horizon) -> int :
	|          This function resets the internal graph and samples for the provided state and horizon.
	|
	|          @param s The initial state for the environment.
	|          @param horizon The horizon to plan for.
	|
	|          @return The best action.
	|
	|      sampleAction( (MCTSModel)self, (int)a, (int)s1, (int)horizon) -> int :
	|          This function uses the internal graph to plan.
	|
	|          This function can be called after a previous call to
	|          sampleAction with a state. Otherwise, it will invoke it
	|          anyway with the provided next state.
	|
	|          If a graph is already present though, this function will
	|          select the branch defined by the input action and
	|          observation, and prune the rest. The search will be started
	|          using the existing graph: this should make search faster.
	|
	|          @param a The action taken in the last timestep.
	|          @param s1 The state experienced after the action was taken.
	|          @param horizon The horizon to plan for.
	|
	|          @return The best action.
	|
	|  setExploration(...)
	|      setExploration( (MCTSModel)self, (float)exp) -> None :
	|          This function sets the new exploration constant for MCTS.
	|
	|          This parameter is EXTREMELY important to determine MCTS
	|          performance and, ultimately, convergence. In general it is
	|          better to find it empirically, by testing some values and
	|          see which one performs best. Tune this parameter, it really
	|          matters!
	|
	|          @param exp The new exploration constant.
	|
	|  setIterations(...)
	|      setIterations( (MCTSModel)self, (int)iterations) -> None :
	|          This function sets the number of performed rollouts in MCTS.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class MCTSSparseMaximumLikelihoodModel(Boost.Python.instance)
	|  This class represents the MCTS online planner using UCB1 for SparseMaximumLikelihoodModel.
	|
	|  NOTE: This algorithm is wrapped in Python, but as it uses the internal
	|  Models rather than a custom generative model to simulate rollouts it will
	|  probably be rather slow for interesting applications. You are of course
	|  welcome to try it out, but it is recommended that the generative model
	|  is written in C++.
	|
	|  This algorithm is an online planner for MDPs. As an online planner,
	|  it needs to have a generative model of the problem. This means that
	|  it only needs a way to sample transitions and rewards from the
	|  model, but it does not need to know directly the distribution
	|  probabilities for them.
	|
	|  MCTS plans for a single state at a time. It builds a tree structure
	|  progressively and action values are deduced as averages of the
	|  obtained rewards over rollouts. If the number of sample episodes is
	|  high enough, it is guaranteed to converge to the optimal solution.
	|
	|  At each rollout, we follow each action and resulting state within the
	|  tree from root to leaves. During this path we chose actions using an
	|  algorithm called UCT. What this does is privilege the most promising
	|  actions, while guaranteeing that in the limit every action will still
	|  be tried an infinite amount of times.
	|
	|  Once we arrive to a leaf in the tree, we then expand it with a
	|  single new node, representing a new state for the path we just
	|  followed. We then proceed outside the tree following a random
	|  policy, but this time we do not track which actions and states
	|  we actually experience. The final reward obtained by this random
	|  rollout policy is used to approximate the values for all nodes
	|  visited in this rollout inside the tree, before leaving it.
	|
	|  Since MCTS expands a tree, it can reuse work it has done if
	|  multiple action requests are done in order. To do so, it simply asks
	|  for the action that has been performed and its respective new state.
	|  Then it simply makes that root branch the new root, and starts
	|  again.
	|
	|  Method resolution order:
	|      MCTSSparseMaximumLikelihoodModel
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (SparseMaximumLikelihoodModel)m, (int)iterations, (float)exp) -> None :
	|          Basic constructor.
	|
	|          @param m The MDP model that MCTS will operate upon.
	|          @param iterations The number of episodes to run before completion.
	|          @param exp The exploration constant. This parameter is VERY important
	|                     to determine the final MCTS performance.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getExploration(...)
	|      getExploration( (MCTSSparseMaximumLikelihoodModel)self) -> float :
	|          This function returns the currently set exploration constant.
	|
	|  getIterations(...)
	|      getIterations( (MCTSSparseMaximumLikelihoodModel)self) -> int :
	|          This function returns the number of iterations performed to plan for an action.
	|
	|  getModel(...)
	|      getModel( (MCTSSparseMaximumLikelihoodModel)self) -> SparseMaximumLikelihoodModel :
	|          This function returns the MDP generative model being used.
	|
	|  sampleAction(...)
	|      sampleAction( (MCTSSparseMaximumLikelihoodModel)self, (int)s, (int)horizon) -> int :
	|          This function resets the internal graph and samples for the provided state and horizon.
	|
	|          @param s The initial state for the environment.
	|          @param horizon The horizon to plan for.
	|
	|          @return The best action.
	|
	|      sampleAction( (MCTSSparseMaximumLikelihoodModel)self, (int)a, (int)s1, (int)horizon) -> int :
	|          This function uses the internal graph to plan.
	|
	|          This function can be called after a previous call to
	|          sampleAction with a state. Otherwise, it will invoke it
	|          anyway with the provided next state.
	|
	|          If a graph is already present though, this function will
	|          select the branch defined by the input action and
	|          observation, and prune the rest. The search will be started
	|          using the existing graph: this should make search faster.
	|
	|          @param a The action taken in the last timestep.
	|          @param s1 The state experienced after the action was taken.
	|          @param horizon The horizon to plan for.
	|
	|          @return The best action.
	|
	|  setExploration(...)
	|      setExploration( (MCTSSparseMaximumLikelihoodModel)self, (float)exp) -> None :
	|          This function sets the new exploration constant for MCTS.
	|
	|          This parameter is EXTREMELY important to determine MCTS
	|          performance and, ultimately, convergence. In general it is
	|          better to find it empirically, by testing some values and
	|          see which one performs best. Tune this parameter, it really
	|          matters!
	|
	|          @param exp The new exploration constant.
	|
	|  setIterations(...)
	|      setIterations( (MCTSSparseMaximumLikelihoodModel)self, (int)iterations) -> None :
	|          This function sets the number of performed rollouts in MCTS.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class MCTSSparseModel(Boost.Python.instance)
	|  This class represents the MCTS online planner using UCB1 for SparseModel.
	|
	|  NOTE: This algorithm is wrapped in Python, but as it uses the internal
	|  Models rather than a custom generative model to simulate rollouts it will
	|  probably be rather slow for interesting applications. You are of course
	|  welcome to try it out, but it is recommended that the generative model
	|  is written in C++.
	|
	|  This algorithm is an online planner for MDPs. As an online planner,
	|  it needs to have a generative model of the problem. This means that
	|  it only needs a way to sample transitions and rewards from the
	|  model, but it does not need to know directly the distribution
	|  probabilities for them.
	|
	|  MCTS plans for a single state at a time. It builds a tree structure
	|  progressively and action values are deduced as averages of the
	|  obtained rewards over rollouts. If the number of sample episodes is
	|  high enough, it is guaranteed to converge to the optimal solution.
	|
	|  At each rollout, we follow each action and resulting state within the
	|  tree from root to leaves. During this path we chose actions using an
	|  algorithm called UCT. What this does is privilege the most promising
	|  actions, while guaranteeing that in the limit every action will still
	|  be tried an infinite amount of times.
	|
	|  Once we arrive to a leaf in the tree, we then expand it with a
	|  single new node, representing a new state for the path we just
	|  followed. We then proceed outside the tree following a random
	|  policy, but this time we do not track which actions and states
	|  we actually experience. The final reward obtained by this random
	|  rollout policy is used to approximate the values for all nodes
	|  visited in this rollout inside the tree, before leaving it.
	|
	|  Since MCTS expands a tree, it can reuse work it has done if
	|  multiple action requests are done in order. To do so, it simply asks
	|  for the action that has been performed and its respective new state.
	|  Then it simply makes that root branch the new root, and starts
	|  again.
	|
	|  Method resolution order:
	|      MCTSSparseModel
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (SparseModel)m, (int)iterations, (float)exp) -> None :
	|          Basic constructor.
	|
	|          @param m The MDP model that MCTS will operate upon.
	|          @param iterations The number of episodes to run before completion.
	|          @param exp The exploration constant. This parameter is VERY important
	|                     to determine the final MCTS performance.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getExploration(...)
	|      getExploration( (MCTSSparseModel)self) -> float :
	|          This function returns the currently set exploration constant.
	|
	|  getIterations(...)
	|      getIterations( (MCTSSparseModel)self) -> int :
	|          This function returns the number of iterations performed to plan for an action.
	|
	|  getModel(...)
	|      getModel( (MCTSSparseModel)self) -> SparseModel :
	|          This function returns the MDP generative model being used.
	|
	|  sampleAction(...)
	|      sampleAction( (MCTSSparseModel)self, (int)s, (int)horizon) -> int :
	|          This function resets the internal graph and samples for the provided state and horizon.
	|
	|          @param s The initial state for the environment.
	|          @param horizon The horizon to plan for.
	|
	|          @return The best action.
	|
	|      sampleAction( (MCTSSparseModel)self, (int)a, (int)s1, (int)horizon) -> int :
	|          This function uses the internal graph to plan.
	|
	|          This function can be called after a previous call to
	|          sampleAction with a state. Otherwise, it will invoke it
	|          anyway with the provided next state.
	|
	|          If a graph is already present though, this function will
	|          select the branch defined by the input action and
	|          observation, and prune the rest. The search will be started
	|          using the existing graph: this should make search faster.
	|
	|          @param a The action taken in the last timestep.
	|          @param s1 The state experienced after the action was taken.
	|          @param horizon The horizon to plan for.
	|
	|          @return The best action.
	|
	|  setExploration(...)
	|      setExploration( (MCTSSparseModel)self, (float)exp) -> None :
	|          This function sets the new exploration constant for MCTS.
	|
	|          This parameter is EXTREMELY important to determine MCTS
	|          performance and, ultimately, convergence. In general it is
	|          better to find it empirically, by testing some values and
	|          see which one performs best. Tune this parameter, it really
	|          matters!
	|
	|          @param exp The new exploration constant.
	|
	|  setIterations(...)
	|      setIterations( (MCTSSparseModel)self, (int)iterations) -> None :
	|          This function sets the number of performed rollouts in MCTS.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class MaximumLikelihoodModel(Boost.Python.instance)
	|  @brief This class models Experience as a Markov Decision Process using Maximum Likelihood.
	|
	|  Often an MDP is not known in advance. It is known that it can assume
	|  a certain set of states, and that a certain set of actions are
	|  available to the agent, but not much more. Thus, in these cases, the
	|  goal is not only to find out the best policy for the MDP we have,
	|  but at the same time learn the actual transition and reward
	|  functions of such a model. This task is called 'reinforcement
	|  learning'.
	|
	|  This class helps with this. A naive approach in reinforcement learning
	|  is to keep track, for each action, of its results, and deduce transition
	|  probabilities and rewards based on the data collected in such a way.
	|  This class does just this, using Maximum Likelihood Estimates to decide
	|  what the transition probabilities and rewards are.
	|
	|  This class maps an Experience object to the most likely transition
	|  reward functions that produced it. The transition function is guaranteed
	|  to be a correct probability function, as in the sum of the probabilities
	|  of all transitions from a particular state and a particular action is
	|  always 1. Each instance is not directly synced with the supplied
	|  Experience object. This is to avoid possible overheads, as the user can
	|  optimize better depending on their use case. See sync().
	|
	|  When little data is available, the deduced transition and reward
	|  functions may be significantly subject to noise. A possible way to
	|  improve on this is to artificially bias the data as to skew it towards
	|  certain distributions.  This could be done if some knowledge of the
	|  model (even approximate) is known, in order to speed up the learning
	|  process. Another way is to assume that all transitions are possible, add
	|  data to support that claim, and simply wait until the averages converge
	|  to the true values.  Another thing that can be done is to associate with
	|  each fake datapoint an high reward: this will skew the agent into trying
	|  out new actions, thinking it will obtained the high rewards. This is
	|  able to obtain automatically a good degree of exploration in the early
	|  stages of an episode. Such a technique is called 'optimistic
	|  initialization'.
	|
	|  Whether any of these techniques work or not can definitely depend on
	|  the model you are trying to approximate. Trying out things is good!
	|
	|  Method resolution order:
	|      MaximumLikelihoodModel
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (Experience)exp [, (float)discount [, (bool)sync]]) -> None :
	|          Constructor using previous Experience.
	|
	|          This constructor selects the Experience that will
	|          be used to learn an MDP Model from the data, and initializes
	|          internal Model data.
	|
	|          The user can choose whether he wants to directly sync
	|          the MaximumLikelihoodModel to the underlying Experience, or delay
	|          it for later.
	|
	|          In the latter case the default transition function
	|          defines a transition of probability 1 for each
	|          state to itself, no matter the action.
	|
	|          In general it would be better to add some amount of bias
	|          to the Experience so that when a new state-action pair is
	|          tried, the MaximumLikelihoodModel doesn't automatically compute 100%
	|          probability of transitioning to the resulting state, but smooths
	|          into it. This may depend on your problem though.
	|
	|          The default reward function is 0.
	|
	|          @param exp The base Experience of the model.
	|          @param discount The discount used in solving methods.
	|          @param sync Whether to sync with the Experience immediately or delay it.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getA(...)
	|      getA( (MaximumLikelihoodModel)self) -> int :
	|          This function returns the number of available actions to the agent.
	|
	|  getDiscount(...)
	|      getDiscount( (MaximumLikelihoodModel)self) -> float :
	|          This function returns the currently set discount factor.
	|
	|  getExpectedReward(...)
	|      getExpectedReward( (MaximumLikelihoodModel)self, (int)s, (int)a, (int)s1) -> float :
	|          This function returns the stored expected reward for the specified transition.
	|
	|  getExperience(...)
	|      getExperience( (MaximumLikelihoodModel)self) -> Experience :
	|          This function enables inspection of the underlying Experience of the MaximumLikelihoodModel.
	|
	|  getS(...)
	|      getS( (MaximumLikelihoodModel)self) -> int :
	|          This function returns the number of states of the world.
	|
	|  getTransitionProbability(...)
	|      getTransitionProbability( (MaximumLikelihoodModel)self, (int)s, (int)a, (int)s1) -> float :
	|          This function returns the stored transition probability for the specified transition.
	|
	|  isTerminal(...)
	|      isTerminal( (MaximumLikelihoodModel)self, (int)s) -> bool :
	|          This function returns whether a given state is a terminal.
	|
	|  sampleSR(...)
	|      sampleSR( (MaximumLikelihoodModel)self, (int)s, (int)a) -> object :
	|          This function samples the MDP for the specified state action pair.
	|
	|          This function samples the model for simulate experience. The transition
	|          and reward functions are used to produce, from the state action pair
	|          inserted as arguments, a possible new state with respective reward.
	|          The new state is picked from all possible states that the MDP allows
	|          transitioning to, each with probability equal to the same probability
	|          of the transition in the model. After a new state is picked, the reward
	|          is the corresponding reward contained in the reward function.
	|
	|          @param s The state that needs to be sampled.
	|          @param a The action that needs to be sampled.
	|
	|          @return A tuple containing a new state and a reward.
	|
	|  setDiscount(...)
	|      setDiscount( (MaximumLikelihoodModel)self, (float)discount) -> None :
	|          This function sets a new discount factor for the Model.
	|
	|  sync(...)
	|      sync( (MaximumLikelihoodModel)self) -> None :
	|          This function syncs the whole MaximumLikelihoodModel to the underlying Experience.
	|
	|          Since use cases in AI are very varied, one may not want to
	|          update its MaximumLikelihoodModel for each single transition
	|          experienced by the agent. To avoid this we leave to the user the
	|          task of syncing between the underlying Experience and the
	|          MaximumLikelihoodModel, as he/she sees fit.
	|
	|          After this function is run the transition and reward functions
	|          will accurately reflect the state of the underlying Experience.
	|
	|      sync( (MaximumLikelihoodModel)self, (int)s, (int)a) -> None :
	|          This function syncs a state action pair in the MaximumLikelihoodModel to the underlying Experience.
	|
	|          Since use cases in AI are very varied, one may not want to
	|          update its MaximumLikelihoodModel for each single transition
	|          experienced by the agent. To avoid this we leave to the user the
	|          task of syncing between the underlying Experience and the
	|          MaximumLikelihoodModel, as he/she sees fit.
	|
	|          This function updates a single state action pair with the underlying
	|          Experience. This function is offered to avoid having to recompute the
	|          whole MaximumLikelihoodModel if the user knows that only few transitions
	|          have been experienced by the agent.
	|
	|          After this function is run the transition and reward functions
	|          will accurately reflect the state of the underlying Experience
	|          for the specified state action pair.
	|
	|          @param s The state that needs to be synced.
	|          @param a The action that needs to be synced.
	|
	|      sync( (MaximumLikelihoodModel)self, (int)s, (int)a, (int)s1) -> None :
	|          This function syncs a state action pair in the MaximumLikelihoodModel to the underlying Experience in the fastest possible way.
	|
	|          This function updates a state action pair given that the last increased transition
	|          in the underlying Experience is the triplet s, a, s1. In addition, this function only
	|works if it needs to add information from this single new point of information (if
	|          more has changed from the last sync, use sync(s,a) ). The performance boost that
	|          this function obtains increases with the increase of the number of states in the model.
	|
	|          @param s The state that needs to be synced.
	|          @param a The action that needs to be synced.
	|          @param s1 The final state of the transition that got updated in the Experience.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class Model(Boost.Python.instance)
	|  This class represents a Markov Decision Process.
	|
	|  A Markov Decision Process (MDP) is a way to model decision making.
	|  The idea is that there is an agent situated in a stochastic
	|  environment which changes in discrete 'timesteps'. The agent can
	|  influence the way the environment changes via 'actions'. For each
	|  action the agent can perform, the environment will transition from a
	|  state 's' to a state 's1' following a certain transition function.
	|  The transition function specifies, for each triple SxAxS' the
	|  probability that such a transition will happen.
	|
	|  In addition, associated with transitions, the agent is able to
	|  obtain rewards. Thus, if it does good, the agent will obtain a
	|  higher reward than if it performed badly. The reward obtained by the
	|  agent is in addition associated with a 'discount' factor: at every
	|  step, the possible reward that the agent can collect is multiplied
	|  by this factor, which is a number between 0 and 1. The discount
	|  factor is used to model the fact that often it is preferable to
	|  obtain something sooner, rather than later.
	|
	|  Since all of this is governed by probabilities, it is possible to
	|  solve an MDP model in order to obtain an 'optimal policy', which is
	|  a way to select an action from a state which will maximize the
	|  expected reward that the agent is going to collect during its life.
	|  The expected reward is computed as the sum of every reward the agent
	|  collects at every timestep, keeping in mind that at every timestep
	|  the reward is further and further discounted.
	|
	|  Solving an MDP in such a way is called 'planning'. Planning
	|  solutions often include an 'horizon', which is the number of
	|  timesteps that are included in an episode. They can be finite or
	|  infinite. The optimal policy changes with respect to the horizon,
	|  since a higher horizon may offer access to reward-gaining
	|  opportunities farther in the future.
	|
	|  An MDP policy (be it the optimal one or another), is associated with
	|  two functions: a ValueFunction and a QFunction. The ValueFunction
	|  represents the expected return for the agent from any initial state,
	|  given that actions are going to be selected according to the policy.
	|  The QFunction is similar: it gives the expected return for a
	|  specific state-action pair, given that after the specified action
	|  one will act according to the policy.
	|
	|  Given that we are usually interested about the optimal policy, there
	|  are a couple of properties that are associated with the optimal
	|  policies functions.  First, the optimal policy can be derived from
	|  the optimal QFunction. The optimal policy simply selects, in a given
	|  state 's', the action that maximizes the value of the QFunction.  In
	|  the same way, the optimal ValueFunction can be computed from the
	|  optimal QFunction by selecting the max with respect to the action.
	|
	|  Since so much information can be extracted from the QFunction, lots
	|  of methods (mostly in Reinforcement Learning) try to learn it.
	|
	|  Method resolution order:
	|      Model
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __getinitargs__(...)
	|      __getinitargs__( (Model)arg1) -> tuple
	|
	|  __getstate__(...)
	|      __getstate__( (Model)arg1) -> tuple
	|
	|  __init__(...)
	|      __init__( (object)self, (int)s, (int)a [, (float)discount]) -> None :
	|          Basic constructor.
	|
	|          This constructor initializes the Model so that all
	|          transitions happen with probability 0 but for transitions
	|          that bring back to the same state, no matter the action.
	|
	|          All rewards are set to 0. The discount parameter is set to
	|          1.
	|
	|          @param s The number of states of the world.
	|          @param a The number of actions available to the agent.
	|          @param discount The discount factor for the MDP.
	|
	|      __init__( (object)self, (Model)model) -> None :
	|          This allows to copy from any other model. A nice use for this is to
	|          convert any model which computes probabilities on the fly into an
	|          MDP::Model where probabilities are all stored for fast access. Of
	|          course such a solution can be done only when the number of states
	|          and actions is not too big.
	|
	|      __init__( (object)self, (SparseModel)sparseModel) -> None :
	|          This allows to copy from any other model. A nice use for this is to
	|          convert any model which computes probabilities on the fly into an
	|          MDP::Model where probabilities are all stored for fast access. Of
	|          course such a solution can be done only when the number of states
	|          and actions is not too big.
	|
	|      __init__( (object)self, (MaximumLikelihoodModel)maximumLikelihoodModel) -> None :
	|          This allows to copy from any other model. A nice use for this is to
	|          convert any model which computes probabilities on the fly into an
	|          MDP::Model where probabilities are all stored for fast access. Of
	|          course such a solution can be done only when the number of states
	|          and actions is not too big.
	|
	|      __init__( (object)self, (SparseMaximumLikelihoodModel)sparseMaximumLikelihoodModel) -> None :
	|          This allows to copy from any other model. A nice use for this is to
	|          convert any model which computes probabilities on the fly into an
	|          MDP::Model where probabilities are all stored for fast access. Of
	|          course such a solution can be done only when the number of states
	|          and actions is not too big.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  __setstate__(...)
	|      __setstate__( (Model)arg1, (tuple)arg2) -> None
	|
	|  getA(...)
	|      getA( (Model)self) -> int :
	|          This function returns the number of available actions to the agent.
	|
	|  getDiscount(...)
	|      getDiscount( (Model)self) -> float :
	|          This function returns the currently set discount factor.
	|
	|  getExpectedReward(...)
	|      getExpectedReward( (Model)self, (int)s, (int)a, (int)s1) -> float :
	|          This function returns the stored expected reward for the specified transition.
	|
	|  getS(...)
	|      getS( (Model)self) -> int :
	|          This function returns the number of states of the world.
	|
	|  getTransitionProbability(...)
	|      getTransitionProbability( (Model)self, (int)s, (int)a, (int)s1) -> float :
	|          This function returns the stored transition probability for the specified transition.
	|
	|  isTerminal(...)
	|      isTerminal( (Model)self, (int)s) -> bool :
	|          This function returns whether a given state is a terminal.
	|
	|  sampleSR(...)
	|      sampleSR( (Model)self, (int)s, (int)a) -> object :
	|          This function samples the MDP for the specified state action pair.
	|
	|          This function samples the model for simulated experience.
	|          The transition and reward functions are used to produce,
	|          from the state action pair inserted as arguments, a possible
	|          new state with respective reward.  The new state is picked
	|          from all possible states that the MDP allows transitioning
	|          to, each with probability equal to the same probability of
	|          the transition in the model. After a new state is picked,
	|          the reward is the corresponding reward contained in the
	|          reward function.
	|
	|          @param s The state that needs to be sampled.
	|          @param a The action that needs to be sampled.
	|
	|          @return A tuple containing a new state and a reward.
	|
	|  setDiscount(...)
	|      setDiscount( (Model)self, (float)discount) -> None :
	|          This function sets a new discount factor for the Model.
	|
	|  setRewardFunction(...)
	|      setRewardFunction( (Model)self, (object)rewardFunction3D) -> None :
	|          This function replaces the Model reward function with the one provided.
	|
	|          Currently the Python wrappings support reading through native 3d Python
	|          arrays (so [][][]). As long as the dimensions are correct and they contain
	|          correct probabilities everything should be fine. The code should reject
	|          them otherwise.
	|
	|  setTransitionFunction(...)
	|      setTransitionFunction( (Model)self, (object)transitionFunction3D) -> None :
	|          This function replaces the Model transition function with the one provided.
	|
	|          Currently the Python wrappings support reading through native 3d Python
	|          arrays (so [][][]). As long as the dimensions are correct and they contain
	|          correct probabilities everything should be fine. The code should reject
	|          them otherwise.
	|
	|  ----------------------------------------------------------------------
	|  Data and other attributes defined here:
	|
	|  __safe_for_unpickling__ = True
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class Policy(PolicyInterface)
	|  This class represents an MDP Policy.
	|
	|  This class is one of the many ways to represent an MDP Policy. In
	|  particular, it maintains a 2 dimensional matrix of probabilities
	|  determining the probability of choosing an action in a given state.
	|
	|  The class offers facilities to sample from these distributions, so
	|  that you can directly embed it into a decision-making process.
	|
	|  Building this object is somewhat expensive, so it should be done
	|  mostly when it is known that the final solution won't change again.
	|  Otherwise you may want to build a wrapper around some data to
	|  extract the policy dynamically.
	|
	|  Method resolution order:
	|      Policy
	|      PolicyInterface
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __getinitargs__(...)
	|      __getinitargs__( (Policy)arg1) -> tuple
	|
	|  __getstate__(...)
	|      __getstate__( (Policy)arg1) -> tuple
	|
	|  __init__(...)
	|      __init__( (object)self, (int)s, (int)a) -> None :
	|          Basic constructor.
	|
	|          This constructor initializes the internal policy matrix so that
	|          each action in each state has the same probability of being
	|          chosen (random policy). This class guarantees that at any point
	|          the internal policy is a true probability distribution, i.e.
	|          for each state the sum of the probabilities of choosing an action
	|          sums up to 1.
	|
	|          @param s The number of states of the world.
	|          @param a The number of actions available to the agent.
	|
	|      __init__( (object)self, (PolicyInterface)p) -> None :
	|          Basic constructor.
	|
	|          This constructor simply copies policy probability values
	|          from any other compatible PolicyInterface, and stores them
	|          internally. This is probably the main way you may want to use
	|          this class.
	|
	|          This may be a useful thing to do in case the policy that is
	|          being copied is very costly to use (for example, QGreedyPolicy)
	|          and it is known that it will not change anymore.
	|
	|          @param p The policy which is being copied.
	|
	|      __init__( (object)self, (int)s, (int)a, (ValueFunction)v) -> None :
	|          Basic constructor.
	|
	|          This constructor copies the implied policy contained in a ValueFunction.
	|          Keep in mind that the policy stored within a ValueFunction is
	|          non-stochastic in nature, since for each state it can only
	|          save a single action.
	|
	|          @param s The number of states of the world.
	|          @param a The number of actions available to the agent.
	|          @param v The ValueFunction used as a basis for the Policy.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  __setstate__(...)
	|      __setstate__( (Policy)arg1, (tuple)arg2) -> None
	|
	|  getPolicyMatrix(...)
	|      getPolicyMatrix( (Policy)self) -> Matrix2D :
	|          This function enables inspection of the internal policy.
	|
	|          @return A constant reference to the internal policy.
	|
	|  ----------------------------------------------------------------------
	|  Data and other attributes defined here:
	|
	|  __safe_for_unpickling__ = True
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from PolicyInterface:
	|
	|  getA(...)
	|      getA( (PolicyInterface)self) -> int :
	|          This function returns the number of available actions to the agent.
	|
	|  getActionProbability(...)
	|      getActionProbability( (PolicyInterface)self, (int)s, (int)a) -> float :
	|          This function returns the probability of taking the specified action in the specified state.
	|
	|          @param s The selected state.
	|          @param a The selected action.
	|
	|          @return The probability of taking the selected action in the specified state.
	|
	|  getS(...)
	|      getS( (PolicyInterface)self) -> int :
	|          This function returns the number of states of the world.
	|
	|  sampleAction(...)
	|      sampleAction( (PolicyInterface)self, (int)s) -> int :
	|          This function chooses a random action for state s, following the policy distribution.
	|
	|          @param s The sampled state of the policy.
	|
	|          @return The chosen action.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class PolicyInterface(Boost.Python.instance)
	|  This class represents the base interface for policies in MDPs.
	|
	|  This class represents an interface that all policies must conform to.
	|  The interface is generic as different methods may have very different
	|  ways to store and compute policies, and this interface simply asks
	|  for a way to sample them.
	|
	|  In the case of MDPs, the class works using integer states, which
	|  represent the discrete states from which we are sampling.
	|
	|  Method resolution order:
	|      PolicyInterface
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      Raises an exception
	|      This class cannot be instantiated from Python
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getA(...)
	|      getA( (PolicyInterface)self) -> int :
	|          This function returns the number of available actions to the agent.
	|
	|  getActionProbability(...)
	|      getActionProbability( (PolicyInterface)self, (int)s, (int)a) -> float :
	|          This function returns the probability of taking the specified action in the specified state.
	|
	|          @param s The selected state.
	|          @param a The selected action.
	|
	|          @return The probability of taking the selected action in the specified state.
	|
	|  getS(...)
	|      getS( (PolicyInterface)self) -> int :
	|          This function returns the number of states of the world.
	|
	|  sampleAction(...)
	|      sampleAction( (PolicyInterface)self, (int)s) -> int :
	|          This function chooses a random action for state s, following the policy distribution.
	|
	|          @param s The sampled state of the policy.
	|
	|          @return The chosen action.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class PolicyIteration(Boost.Python.instance)
	|  This class applies the policy iteration algorithm.
	|
	|  This algorithm begins with an arbitrary policy (random), and uses
	|  the PolicyEvaluation algorithm to find out the Values for each state
	|  of this policy.
	|
	|  Once this is done, the policy can be improved by using a greedy
	|  approach towards the QFunction found. The new policy is then newly
	|  evaluated, and the process repeated.
	|
	|  When the policy does not change anymore, it is guaranteed to be
	|  optimal, and the found QFunction is returned.
	|
	|  Method resolution order:
	|      PolicyIteration
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __call__(...)
	|      __call__( (PolicyIteration)self, (Model)m) -> Matrix2D :
	|          This function applies policy iteration on an MDP to solve it.
	|
	|          The algorithm is constrained by the currently set parameters.
	|
	|          @param m The MDP that needs to be solved.@return The QFunction of the optimal policy found.
	|
	|      __call__( (PolicyIteration)self, (SparseModel)m) -> Matrix2D :
	|          This function applies policy iteration on an MDP to solve it.
	|
	|          The algorithm is constrained by the currently set parameters.
	|
	|          @param m The MDP that needs to be solved.@return The QFunction of the optimal policy found.
	|
	|      __call__( (PolicyIteration)self, (MaximumLikelihoodModel)m) -> Matrix2D :
	|          This function applies policy iteration on an MDP to solve it.
	|
	|          The algorithm is constrained by the currently set parameters.
	|
	|          @param m The MDP that needs to be solved.@return The QFunction of the optimal policy found.
	|
	|      __call__( (PolicyIteration)self, (SparseMaximumLikelihoodModel)m) -> Matrix2D :
	|          This function applies policy iteration on an MDP to solve it.
	|
	|          The algorithm is constrained by the currently set parameters.
	|
	|          @param m The MDP that needs to be solved.@return The QFunction of the optimal policy found.
	|
	|  __init__(...)
	|      __init__( (object)self, (int)horizon [, (float)tolerance]) -> None :
	|          Basic constructor.
	|
	|          @param horizon The horizon parameter to use during the PolicyEvaluation phase.
	|          @param tolerance The tolerance parameter to use during the PolicyEvaluation phase.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getHorizon(...)
	|      getHorizon( (PolicyIteration)self) -> int :
	|          This function will return the current horizon parameter.
	|
	|  getTolerance(...)
	|      getTolerance( (PolicyIteration)self) -> float :
	|          This function will return the currently set tolerance parameter.
	|
	|  setHorizon(...)
	|      setHorizon( (PolicyIteration)self, (int)horizon) -> None :
	|          This function sets the horizon parameter.
	|
	|  setTolerance(...)
	|      setTolerance( (PolicyIteration)self, (float)e) -> None :
	|          This function sets the tolerance parameter.
	|
	|          The tolerance parameter must be >= 0 or the function will throw.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class PrioritizedSweepingMaximumLikelihoodModel(Boost.Python.instance)
	|  This class represents the PrioritizedSweeping algorithm for MaximumLikelihoodModel.
	|
	|  This algorithm is a refinement of the DynaQ algorithm. Instead of
	|  randomly sampling experienced state action pairs to get more
	|  information, we order each pair based on an estimate of how much
	|  information we can still extract from them.
	|
	|  In particular, pairs are sorted based on the amount they modified
	|  the estimated ValueFunction on their last sample. This ensures that
	|  we always try to sample from useful pairs instead of randomly,
	|  extracting knowledge much faster.
	|
	|  At the same time, this algorithm keeps a threshold for each
	|  state-action pair, so that it does not have to internally store all
	|  the pairs and save some memory/cpu time keeping the queue updated.
	|  Only pairs which obtained an amount of change higher than this
	|  treshold are kept in the queue.
	|
	|  Differently from the QLearning and DynaQ algorithm, this class
	|  automatically computes the ValueFunction since it is useful to
	|  determine which state-action pairs are actually useful, so there's
	|  no need to compute it manually.
	|
	|  Given how this algorithm updates the QFunction, the only problems
	|  supported by this approach are ones with an infinite horizon.
	|
	|  Method resolution order:
	|      PrioritizedSweepingMaximumLikelihoodModel
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (MaximumLikelihoodModel)m [, (float)theta [, (int)n]]) -> None :
	|          Basic constructor.
	|
	|          @param m The model to be used to update the QFunction.
	|          @param theta The queue threshold.
	|          @param n The number of sampling passes to do on the model upon batchUpdateQ().
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  batchUpdateQ(...)
	|      batchUpdateQ( (PrioritizedSweepingMaximumLikelihoodModel)self) -> None :
	|          This function updates a QFunction based on simulated experience.
	|
	|          In PrioritizedSweepingEigen we sample from the queue at most N times for
	|          state action pairs that need updating. For each one of them we update
	|          the QFunction and recursively check whether this produces new changes
	|          worth updating. If so, they are inserted in the queue_ and the function
	|          proceeds to the next most urgent iteration.
	|
	|  getModel(...)
	|      getModel( (PrioritizedSweepingMaximumLikelihoodModel)self) -> MaximumLikelihoodModel :
	|          This function returns a reference to the referenced Model.
	|
	|  getN(...)
	|      getN( (PrioritizedSweepingMaximumLikelihoodModel)self) -> int :
	|          This function returns the currently set number of sampling passes during batchUpdateQ().
	|
	|  getQFunction(...)
	|      getQFunction( (PrioritizedSweepingMaximumLikelihoodModel)self) -> Matrix2D :
	|          This function returns a reference to the internal QFunction.
	|
	|  getQueueLength(...)
	|      getQueueLength( (PrioritizedSweepingMaximumLikelihoodModel)self) -> int :
	|          This function returns the current number of elements unprocessed in the queue.
	|
	|  getQueueThreshold(...)
	|      getQueueThreshold( (PrioritizedSweepingMaximumLikelihoodModel)self) -> float :
	|          This function will return the currently set theta parameter.
	|
	|  getValueFunction(...)
	|      getValueFunction( (PrioritizedSweepingMaximumLikelihoodModel)self) -> ValueFunction :
	|          This function returns a reference to the internal ValueFunction.
	|
	|  setN(...)
	|      setN( (PrioritizedSweepingMaximumLikelihoodModel)self, (int)n) -> None :
	|          This function sets the number of sampling passes during batchUpdateQ().
	|
	|  setQFunction(...)
	|      setQFunction( (PrioritizedSweepingMaximumLikelihoodModel)self, (Matrix2D)q) -> None :
	|          This function allows you to set the value of the internal QFunction.
	|
	|          This function can be useful in case you are starting with an already populated
	|          Experience/Model, which you can solve (for example with ValueIteration)
	|          and then improve the solution with new experience.
	|
	|          @param q The QFunction that will be copied.
	|
	|  setQueueThreshold(...)
	|      setQueueThreshold( (PrioritizedSweepingMaximumLikelihoodModel)self, (float)t) -> None :
	|          This function sets the theta parameter.
	|
	|          The discount parameter must be >= 0.0.
	|          otherwise the function will throw an std::invalid_argument.
	|
	|          @param t The new theta parameter.
	|
	|  stepUpdateQ(...)
	|      stepUpdateQ( (PrioritizedSweepingMaximumLikelihoodModel)self, (int)s, (int)a) -> None :
	|          This function updates the PrioritizedSweepingEigen internal update queue.
	|
	|          This function updates the QFunction for the specified pair, and decides
	|          whether any parent couple that can lead to this state is worth pushing
	|          into the queue.
	|
	|          @param s The previous state.
	|          @param a The action performed.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class PrioritizedSweepingModel(Boost.Python.instance)
	|  This class represents the PrioritizedSweeping algorithm for Model.
	|
	|  This algorithm is a refinement of the DynaQ algorithm. Instead of
	|  randomly sampling experienced state action pairs to get more
	|  information, we order each pair based on an estimate of how much
	|  information we can still extract from them.
	|
	|  In particular, pairs are sorted based on the amount they modified
	|  the estimated ValueFunction on their last sample. This ensures that
	|  we always try to sample from useful pairs instead of randomly,
	|  extracting knowledge much faster.
	|
	|  At the same time, this algorithm keeps a threshold for each
	|  state-action pair, so that it does not have to internally store all
	|  the pairs and save some memory/cpu time keeping the queue updated.
	|  Only pairs which obtained an amount of change higher than this
	|  treshold are kept in the queue.
	|
	|  Differently from the QLearning and DynaQ algorithm, this class
	|  automatically computes the ValueFunction since it is useful to
	|  determine which state-action pairs are actually useful, so there's
	|  no need to compute it manually.
	|
	|  Given how this algorithm updates the QFunction, the only problems
	|  supported by this approach are ones with an infinite horizon.
	|
	|  Method resolution order:
	|      PrioritizedSweepingModel
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (Model)m [, (float)theta [, (int)n]]) -> None :
	|          Basic constructor.
	|
	|          @param m The model to be used to update the QFunction.
	|          @param theta The queue threshold.
	|          @param n The number of sampling passes to do on the model upon batchUpdateQ().
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  batchUpdateQ(...)
	|      batchUpdateQ( (PrioritizedSweepingModel)self) -> None :
	|          This function updates a QFunction based on simulated experience.
	|
	|          In PrioritizedSweepingEigen we sample from the queue at most N times for
	|          state action pairs that need updating. For each one of them we update
	|          the QFunction and recursively check whether this produces new changes
	|          worth updating. If so, they are inserted in the queue_ and the function
	|          proceeds to the next most urgent iteration.
	|
	|  getModel(...)
	|      getModel( (PrioritizedSweepingModel)self) -> Model :
	|          This function returns a reference to the referenced Model.
	|
	|  getN(...)
	|      getN( (PrioritizedSweepingModel)self) -> int :
	|          This function returns the currently set number of sampling passes during batchUpdateQ().
	|
	|  getQFunction(...)
	|      getQFunction( (PrioritizedSweepingModel)self) -> Matrix2D :
	|          This function returns a reference to the internal QFunction.
	|
	|  getQueueLength(...)
	|      getQueueLength( (PrioritizedSweepingModel)self) -> int :
	|          This function returns the current number of elements unprocessed in the queue.
	|
	|  getQueueThreshold(...)
	|      getQueueThreshold( (PrioritizedSweepingModel)self) -> float :
	|          This function will return the currently set theta parameter.
	|
	|  getValueFunction(...)
	|      getValueFunction( (PrioritizedSweepingModel)self) -> ValueFunction :
	|          This function returns a reference to the internal ValueFunction.
	|
	|  setN(...)
	|      setN( (PrioritizedSweepingModel)self, (int)n) -> None :
	|          This function sets the number of sampling passes during batchUpdateQ().
	|
	|  setQFunction(...)
	|      setQFunction( (PrioritizedSweepingModel)self, (Matrix2D)q) -> None :
	|          This function allows you to set the value of the internal QFunction.
	|
	|          This function can be useful in case you are starting with an already populated
	|          Experience/Model, which you can solve (for example with ValueIteration)
	|          and then improve the solution with new experience.
	|
	|          @param q The QFunction that will be copied.
	|
	|  setQueueThreshold(...)
	|      setQueueThreshold( (PrioritizedSweepingModel)self, (float)t) -> None :
	|          This function sets the theta parameter.
	|
	|          The discount parameter must be >= 0.0.
	|          otherwise the function will throw an std::invalid_argument.
	|
	|          @param t The new theta parameter.
	|
	|  stepUpdateQ(...)
	|      stepUpdateQ( (PrioritizedSweepingModel)self, (int)s, (int)a) -> None :
	|          This function updates the PrioritizedSweepingEigen internal update queue.
	|
	|          This function updates the QFunction for the specified pair, and decides
	|          whether any parent couple that can lead to this state is worth pushing
	|          into the queue.
	|
	|          @param s The previous state.
	|          @param a The action performed.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class PrioritizedSweepingSparseMaximumLikelihoodModel(Boost.Python.instance)
	|  This class represents the PrioritizedSweeping algorithm for SparseMaximumLikelihoodModel.
	|
	|  This algorithm is a refinement of the DynaQ algorithm. Instead of
	|  randomly sampling experienced state action pairs to get more
	|  information, we order each pair based on an estimate of how much
	|  information we can still extract from them.
	|
	|  In particular, pairs are sorted based on the amount they modified
	|  the estimated ValueFunction on their last sample. This ensures that
	|  we always try to sample from useful pairs instead of randomly,
	|  extracting knowledge much faster.
	|
	|  At the same time, this algorithm keeps a threshold for each
	|  state-action pair, so that it does not have to internally store all
	|  the pairs and save some memory/cpu time keeping the queue updated.
	|  Only pairs which obtained an amount of change higher than this
	|  treshold are kept in the queue.
	|
	|  Differently from the QLearning and DynaQ algorithm, this class
	|  automatically computes the ValueFunction since it is useful to
	|  determine which state-action pairs are actually useful, so there's
	|  no need to compute it manually.
	|
	|  Given how this algorithm updates the QFunction, the only problems
	|  supported by this approach are ones with an infinite horizon.
	|
	|  Method resolution order:
	|      PrioritizedSweepingSparseMaximumLikelihoodModel
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (SparseMaximumLikelihoodModel)m [, (float)theta [, (int)n]]) -> None :
	|          Basic constructor.
	|
	|          @param m The model to be used to update the QFunction.
	|          @param theta The queue threshold.
	|          @param n The number of sampling passes to do on the model upon batchUpdateQ().
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  batchUpdateQ(...)
	|      batchUpdateQ( (PrioritizedSweepingSparseMaximumLikelihoodModel)self) -> None :
	|          This function updates a QFunction based on simulated experience.
	|
	|          In PrioritizedSweepingEigen we sample from the queue at most N times for
	|          state action pairs that need updating. For each one of them we update
	|          the QFunction and recursively check whether this produces new changes
	|          worth updating. If so, they are inserted in the queue_ and the function
	|          proceeds to the next most urgent iteration.
	|
	|  getModel(...)
	|      getModel( (PrioritizedSweepingSparseMaximumLikelihoodModel)self) -> SparseMaximumLikelihoodModel :
	|          This function returns a reference to the referenced Model.
	|
	|  getN(...)
	|      getN( (PrioritizedSweepingSparseMaximumLikelihoodModel)self) -> int :
	|          This function returns the currently set number of sampling passes during batchUpdateQ().
	|
	|  getQFunction(...)
	|      getQFunction( (PrioritizedSweepingSparseMaximumLikelihoodModel)self) -> Matrix2D :
	|          This function returns a reference to the internal QFunction.
	|
	|  getQueueLength(...)
	|      getQueueLength( (PrioritizedSweepingSparseMaximumLikelihoodModel)self) -> int :
	|          This function returns the current number of elements unprocessed in the queue.
	|
	|  getQueueThreshold(...)
	|      getQueueThreshold( (PrioritizedSweepingSparseMaximumLikelihoodModel)self) -> float :
	|          This function will return the currently set theta parameter.
	|
	|  getValueFunction(...)
	|      getValueFunction( (PrioritizedSweepingSparseMaximumLikelihoodModel)self) -> ValueFunction :
	|          This function returns a reference to the internal ValueFunction.
	|
	|  setN(...)
	|      setN( (PrioritizedSweepingSparseMaximumLikelihoodModel)self, (int)n) -> None :
	|          This function sets the number of sampling passes during batchUpdateQ().
	|
	|  setQFunction(...)
	|      setQFunction( (PrioritizedSweepingSparseMaximumLikelihoodModel)self, (Matrix2D)q) -> None :
	|          This function allows you to set the value of the internal QFunction.
	|
	|          This function can be useful in case you are starting with an already populated
	|          Experience/Model, which you can solve (for example with ValueIteration)
	|          and then improve the solution with new experience.
	|
	|          @param q The QFunction that will be copied.
	|
	|  setQueueThreshold(...)
	|      setQueueThreshold( (PrioritizedSweepingSparseMaximumLikelihoodModel)self, (float)t) -> None :
	|          This function sets the theta parameter.
	|
	|          The discount parameter must be >= 0.0.
	|          otherwise the function will throw an std::invalid_argument.
	|
	|          @param t The new theta parameter.
	|
	|  stepUpdateQ(...)
	|      stepUpdateQ( (PrioritizedSweepingSparseMaximumLikelihoodModel)self, (int)s, (int)a) -> None :
	|          This function updates the PrioritizedSweepingEigen internal update queue.
	|
	|          This function updates the QFunction for the specified pair, and decides
	|          whether any parent couple that can lead to this state is worth pushing
	|          into the queue.
	|
	|          @param s The previous state.
	|          @param a The action performed.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class PrioritizedSweepingSparseModel(Boost.Python.instance)
	|  This class represents the PrioritizedSweeping algorithm for SparseModel.
	|
	|  This algorithm is a refinement of the DynaQ algorithm. Instead of
	|  randomly sampling experienced state action pairs to get more
	|  information, we order each pair based on an estimate of how much
	|  information we can still extract from them.
	|
	|  In particular, pairs are sorted based on the amount they modified
	|  the estimated ValueFunction on their last sample. This ensures that
	|  we always try to sample from useful pairs instead of randomly,
	|  extracting knowledge much faster.
	|
	|  At the same time, this algorithm keeps a threshold for each
	|  state-action pair, so that it does not have to internally store all
	|  the pairs and save some memory/cpu time keeping the queue updated.
	|  Only pairs which obtained an amount of change higher than this
	|  treshold are kept in the queue.
	|
	|  Differently from the QLearning and DynaQ algorithm, this class
	|  automatically computes the ValueFunction since it is useful to
	|  determine which state-action pairs are actually useful, so there's
	|  no need to compute it manually.
	|
	|  Given how this algorithm updates the QFunction, the only problems
	|  supported by this approach are ones with an infinite horizon.
	|
	|  Method resolution order:
	|      PrioritizedSweepingSparseModel
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (SparseModel)m [, (float)theta [, (int)n]]) -> None :
	|          Basic constructor.
	|
	|          @param m The model to be used to update the QFunction.
	|          @param theta The queue threshold.
	|          @param n The number of sampling passes to do on the model upon batchUpdateQ().
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  batchUpdateQ(...)
	|      batchUpdateQ( (PrioritizedSweepingSparseModel)self) -> None :
	|          This function updates a QFunction based on simulated experience.
	|
	|          In PrioritizedSweepingEigen we sample from the queue at most N times for
	|          state action pairs that need updating. For each one of them we update
	|          the QFunction and recursively check whether this produces new changes
	|          worth updating. If so, they are inserted in the queue_ and the function
	|          proceeds to the next most urgent iteration.
	|
	|  getModel(...)
	|      getModel( (PrioritizedSweepingSparseModel)self) -> SparseModel :
	|          This function returns a reference to the referenced Model.
	|
	|  getN(...)
	|      getN( (PrioritizedSweepingSparseModel)self) -> int :
	|          This function returns the currently set number of sampling passes during batchUpdateQ().
	|
	|  getQFunction(...)
	|      getQFunction( (PrioritizedSweepingSparseModel)self) -> Matrix2D :
	|          This function returns a reference to the internal QFunction.
	|
	|  getQueueLength(...)
	|      getQueueLength( (PrioritizedSweepingSparseModel)self) -> int :
	|          This function returns the current number of elements unprocessed in the queue.
	|
	|  getQueueThreshold(...)
	|      getQueueThreshold( (PrioritizedSweepingSparseModel)self) -> float :
	|          This function will return the currently set theta parameter.
	|
	|  getValueFunction(...)
	|      getValueFunction( (PrioritizedSweepingSparseModel)self) -> ValueFunction :
	|          This function returns a reference to the internal ValueFunction.
	|
	|  setN(...)
	|      setN( (PrioritizedSweepingSparseModel)self, (int)n) -> None :
	|          This function sets the number of sampling passes during batchUpdateQ().
	|
	|  setQFunction(...)
	|      setQFunction( (PrioritizedSweepingSparseModel)self, (Matrix2D)q) -> None :
	|          This function allows you to set the value of the internal QFunction.
	|
	|          This function can be useful in case you are starting with an already populated
	|          Experience/Model, which you can solve (for example with ValueIteration)
	|          and then improve the solution with new experience.
	|
	|          @param q The QFunction that will be copied.
	|
	|  setQueueThreshold(...)
	|      setQueueThreshold( (PrioritizedSweepingSparseModel)self, (float)t) -> None :
	|          This function sets the theta parameter.
	|
	|          The discount parameter must be >= 0.0.
	|          otherwise the function will throw an std::invalid_argument.
	|
	|          @param t The new theta parameter.
	|
	|  stepUpdateQ(...)
	|      stepUpdateQ( (PrioritizedSweepingSparseModel)self, (int)s, (int)a) -> None :
	|          This function updates the PrioritizedSweepingEigen internal update queue.
	|
	|          This function updates the QFunction for the specified pair, and decides
	|          whether any parent couple that can lead to this state is worth pushing
	|          into the queue.
	|
	|          @param s The previous state.
	|          @param a The action performed.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class QGreedyPolicy(QPolicyInterface)
	|  This class implements a greedy policy through a QFunction.
	|
	|  This class allows you to select effortlessly the best greedy actions
	|  from a given QFunction.
	|
	|  Method resolution order:
	|      QGreedyPolicy
	|      QPolicyInterface
	|      PolicyInterface
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (Matrix2D)q) -> None :
	|          Basic constructor.
	|
	|          @param q The QFunction this policy is linked with.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from PolicyInterface:
	|
	|  getA(...)
	|      getA( (PolicyInterface)self) -> int :
	|          This function returns the number of available actions to the agent.
	|
	|  getActionProbability(...)
	|      getActionProbability( (PolicyInterface)self, (int)s, (int)a) -> float :
	|          This function returns the probability of taking the specified action in the specified state.
	|
	|          @param s The selected state.
	|          @param a The selected action.
	|
	|          @return The probability of taking the selected action in the specified state.
	|
	|  getS(...)
	|      getS( (PolicyInterface)self) -> int :
	|          This function returns the number of states of the world.
	|
	|  sampleAction(...)
	|      sampleAction( (PolicyInterface)self, (int)s) -> int :
	|          This function chooses a random action for state s, following the policy distribution.
	|
	|          @param s The sampled state of the policy.
	|
	|          @return The chosen action.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class QL(Boost.Python.instance)
	|  This class implements off-policy evaluation via Q(lambda).
	|  This algorithm is the off-policy equivalent of SARSAL. It scales traces
	|  using the lambda parameter, but is able to work in an off-line manner.
	|
	|  Unfortunately, as it does not take into account the discrepancy between
	|  behaviour and target policies, it tends to work only if the two policies
	|  are similar.
	|
	|  Note that even if the trace discount does not take into account the
	|  target policy, the error update is still computed using the target, and
	|  that is why the method works and does not just compute the value of the
	|  current behaviour policy.
	|
	|  This method behaves as an inefficient QLearning if you set the lambda
	|  parameter to zero (effectively cutting all traces), and the epsilon
	|  parameter to zero (forcing a perfectly greedy target policy).
	|
	|  Method resolution order:
	|      QL
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (int)S, (int)A [, (float)discount [, (float)alpha [, (float)lambda [, (float)tolerance [, (float)epsilon]]]]]) -> None :
	|          Basic constructor.
	|
	|          The learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          @param S The state space of the underlying model.
	|          @param A The action space of the underlying model.
	|          @param discount The discount of the underlying model.
	|          @param alpha The learning rate of the QL method.
	|          @param lambda The lambda parameter for the eligibility traces.
	|          @param tolerance The cutoff point for eligibility traces.
	|          @param epsilon The epsilon of the implied target greedy epsilon policy.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getA(...)
	|      getA( (QL)self) -> int :
	|          This function returns the number of actions on which QLearning is working.
	|
	|  getDiscount(...)
	|      getDiscount( (QL)self) -> float :
	|          This function returns the currently set discount parameter.
	|
	|  getLambda(...)
	|      getLambda( (QL)self) -> float :
	|          This function returns the currently set lambda parameter.
	|
	|  getLearningRate(...)
	|      getLearningRate( (QL)self) -> float :
	|          This function will return the current set learning rate parameter.
	|
	|  getQFunction(...)
	|      getQFunction( (QL)self) -> Matrix2D :
	|          This function returns a reference to the internal QFunction.
	|
	|          The returned reference can be used to build Policies, for example
	|          MDP::QGreedyPolicy.
	|
	|  getS(...)
	|      getS( (QL)self) -> int :
	|          This function returns the number of states on which QLearning is working.
	|
	|  getTolerance(...)
	|      getTolerance( (QL)self) -> float :
	|          This function returns the currently set trace cutoff parameter.
	|
	|  getTraces(...)
	|      getTraces( (QL)self) -> vec_trace :
	|          This function returns the currently set traces.
	|
	|  setDiscount(...)
	|      setDiscount( (QL)self, (float)d) -> None :
	|          This function sets the new discount parameter.
	|
	|          The discount parameter controls the amount that future rewards are considered
	|          by QL. If 1, then any reward is the same, if obtained now or in a million
	|          timesteps. Thus the algorithm will optimize overall reward accretion. When less
	|          than 1, rewards obtained in the presents are valued more than future rewards.
	|
	|          @param d The new discount factor.
	|
	|  setLambda(...)
	|      setLambda( (QL)self, (float)lambda) -> None :
	|          This function sets the new lambda parameter.
	|
	|          The lambda parameter must be >= 0.0 and <= 1.0, otherwise the
	|          function will throw an std::invalid_argument.
	|
	|          @param l The new lambda parameter.
	|
	|  setLearningRate(...)
	|      setLearningRate( (QL)self, (float)a) -> None :
	|          This function sets the learning rate parameter.
	|
	|          The learning parameter determines the speed at which the
	|          QFunction is modified with respect to new data. In fully
	|          deterministic environments (such as an agent moving through
	|          a grid, for example), this parameter can be safely set to
	|          1.0 for maximum learning.
	|
	|          On the other side, in stochastic environments, in order to
	|          converge this parameter should be higher when first starting
	|          to learn, and decrease slowly over time.
	|
	|          Otherwise it can be kept somewhat high if the environment
	|          dynamics change progressively, and the algorithm will adapt
	|          accordingly. The final behaviour of QL is very
	|          dependent on this parameter.
	|
	|          The learning rate parameter must be > 0.0 and <= 1.0,
	|          otherwise the function will throw an std::invalid_argument.
	|
	|          @param a The new learning rate parameter.
	|
	|  setTolerance(...)
	|      setTolerance( (QL)self, (float)t) -> None :
	|          This function sets the trace cutoff parameter.
	|
	|          This parameter determines when a trace is removed, as its
	|          coefficient has become too small to bother updating its value.
	|
	|          Note that the trace cutoff is performed on the overall
	|          discount*lambda value, and not only on lambda. So this parameter
	|          is useful even when lambda is 1.
	|
	|          @param t The new trace cutoff value.
	|
	|  stepUpdateQ(...)
	|      stepUpdateQ( (QL)self, (int)s, (int)a, (int)s1, (float)rew) -> None :
	|          This function updates the internal QFunction using the discount set during construction.
	|
	|          This function takes a single experience point and uses it to
	|          update the QFunction. This is a very efficient method to
	|          keep the QFunction up to date with the latest experience.
	|
	|          @param s The previous state.
	|          @param a The action performed.
	|          @param s1 The new state.
	|          @param rew The reward obtained.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class QLearning(Boost.Python.instance)
	|  This class represents the QLearning algorithm.
	|
	|  This algorithm is a very simple but powerful way to learn the
	|  optimal QFunction for an MDP model, where the transition and reward
	|  functions are unknown. It works in an offline fashion, meaning that
	|  it can be used even if the policy that the agent is currently using
	|  is not the optimal one, or is different by the one currently
	|  specified by the QLearning QFunction.
	|
	|  The idea is to progressively update the QFunction averaging all
	|  obtained datapoints. This can be done by generating data via the
	|  model, or by simply sending the agent into the world to try stuff
	|  out. This allows to avoid modeling directly the transition and
	|  reward functions for unknown problems.
	|
	|  This algorithm is guaranteed convergence for stationary MDPs (MDPs
	|  that do not change their transition and reward functions over time),
	|  given that the learning parameter converges to 0 over time.
	|
	|  \sa setLearningRate(double)
	|
	|  At the same time, this algorithm can be used for non-stationary
	|  MDPs, and it will try to constantly keep up with changes in the
	|  environment, given that they are not huge.
	|
	|  This algorithm does not actually need to sample from the input
	|  model, and so it can be a good algorithm to apply in real world
	|  scenarios, where there would be no way to reproduce the world's
	|  behavior aside from actually trying out actions. However it is
	|  needed to know the size of the state space, the size of the action
	|  space and the discount factor of the problem.
	|
	|  Method resolution order:
	|      QLearning
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (int)S, (int)A [, (float)discount [, (float)alpha]]) -> None :
	|          Basic constructor.
	|
	|          The learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          @param S The size of the state space.
	|          @param A The size of the action space.
	|          @param discount The discount to use when learning.
	|          @param alpha The learning rate of the QLearning method.
	|
	|      __init__( (object)self, (MaximumLikelihoodModel)model [, (float)alpha]) -> None :
	|          Basic constructor from MaximumLikelihoodModel
	|
	|          The learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          This constructor copies the S and A and discount parameters from
	|          the supplied model. It does not keep the reference, so if the
	|          discount needs to change you'll need to update it here manually
	|          too.
	|
	|          @param model The MDP model that QLearning will use as a base.
	|          @param alpha The learning rate of the QLearning method.
	|
	|      __init__( (object)self, (SparseMaximumLikelihoodModel)model [, (float)alpha]) -> None :
	|          Basic constructor from SparseMaximumLikelihoodModel
	|
	|          The learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          This constructor copies the S and A and discount parameters
	|          from the supplied model. It does not conserve the reference.
	|
	|          @param model The MDP model that QLearning will use as a base.
	|          @param alpha The learning rate of the QLearning method.
	|
	|      __init__( (object)self, (Model)model [, (float)alpha]) -> None :
	|          Basic constructor from Model
	|
	|          The learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          This constructor copies the S and A and discount parameters
	|          from the supplied model. It does not conserve the reference.
	|
	|          @param model The MDP model that QLearning will use as a base.
	|          @param alpha The learning rate of the QLearning method.
	|
	|      __init__( (object)self, (SparseModel)model [, (float)alpha]) -> None :
	|          Basic constructor from SparseModel
	|
	|          The learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          This constructor copies the S and A and discount parameters
	|          from the supplied model. It does not conserve the reference.
	|
	|          @param model The MDP model that QLearning will use as a base.
	|          @param alpha The learning rate of the QLearning method.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getA(...)
	|      getA( (QLearning)self) -> int :
	|          This function returns the number of actions on which QLearning is working.
	|
	|  getDiscount(...)
	|      getDiscount( (QLearning)self) -> float :
	|          This function returns the currently set discount parameter.
	|
	|  getLearningRate(...)
	|      getLearningRate( (QLearning)self) -> float :
	|          This function will return the current set learning rate parameter.
	|
	|  getQFunction(...)
	|      getQFunction( (QLearning)self) -> Matrix2D :
	|          This function returns a reference to the internal QFunction.
	|
	|          The returned reference can be used to build Policies, for example
	|          MDP::QGreedyPolicy.
	|
	|  getS(...)
	|      getS( (QLearning)self) -> int :
	|          This function returns the number of states on which QLearning is working.
	|
	|  setDiscount(...)
	|      setDiscount( (QLearning)self, (float)d) -> None :
	|          This function sets the new discount parameter.
	|
	|          The discount parameter controls the amount that future rewards are considered
	|          by QLearning. If 1, then any reward is the same, if obtained now or in a million
	|          timesteps. Thus the algorithm will optimize overall reward accretion. When less
	|          than 1, rewards obtained in the presents are valued more than future rewards.
	|
	|          @param d The new discount factor.
	|
	|  setLearningRate(...)
	|      setLearningRate( (QLearning)self, (float)a) -> None :
	|          This function sets the learning rate parameter.
	|
	|          The learning parameter determines the speed at whichthe
	|          QFunction is modified with respect to new data. In fully
	|          deterministic environments (such as an agent moving through
	|          a grid, for example), this parameter can be safely set to
	|          1.0 for maximum learning.
	|
	|          On the other side, in stochastic environments, in order to
	|          converge this parameter should be higher when first starting
	|          to learn, and decrease slowly over time.
	|
	|          Otherwise it can be kept somewhat high if the environment
	|          dynamics change progressively, and the algorithm will adapt
	|          accordingly. The final behavior of QLearning is very
	|          dependent on this parameter.
	|
	|          The learning rate parameter must be > 0.0 and <= 1.0,
	|          otherwise the function will throw an std::invalid_argument.
	|
	|          @param a The new learning rate parameter.
	|
	|  stepUpdateQ(...)
	|      stepUpdateQ( (QLearning)self, (int)s, (int)a, (int)s1, (float)rew) -> None :
	|          This function updates the internal QFunction using the discount set during construction.
	|
	|          This function takes a single experience point and uses it to
	|          update the QFunction. This is a very efficient method to
	|          keep the QFunction up to date with the latest experience.
	|
	|          @param s The previous state.
	|          @param a The action performed.
	|          @param s1 The new state.
	|          @param rew The reward obtained.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class QPolicyInterface(PolicyInterface)
	|  This class is an interface to specify a policy through a QFunction.
	|
	|  This class provides a way to sample actions without the
	|  need to compute a full Policy from a QFunction. This is useful
	|  because often many methods need to modify small parts of a Qfunction
	|  for progressive improvement, and computing a full Policy at each
	|  step can become too expensive to do.
	|
	|  The type of policy obtained from such sampling is left to the implementation,
	|  since there are many ways in which such a policy may be formed.
	|
	|  Method resolution order:
	|      QPolicyInterface
	|      PolicyInterface
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      Raises an exception
	|      This class cannot be instantiated from Python
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from PolicyInterface:
	|
	|  getA(...)
	|      getA( (PolicyInterface)self) -> int :
	|          This function returns the number of available actions to the agent.
	|
	|  getActionProbability(...)
	|      getActionProbability( (PolicyInterface)self, (int)s, (int)a) -> float :
	|          This function returns the probability of taking the specified action in the specified state.
	|
	|          @param s The selected state.
	|          @param a The selected action.
	|
	|          @return The probability of taking the selected action in the specified state.
	|
	|  getS(...)
	|      getS( (PolicyInterface)self) -> int :
	|          This function returns the number of states of the world.
	|
	|  sampleAction(...)
	|      sampleAction( (PolicyInterface)self, (int)s) -> int :
	|          This function chooses a random action for state s, following the policy distribution.
	|
	|          @param s The sampled state of the policy.
	|
	|          @return The chosen action.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class QSoftmaxPolicy(QPolicyInterface)
	|  This class implements a softmax policy through a QFunction.
	|
	|  A softmax policy is a policy that selects actions based on their
	|  expected reward: the more advantageous an action seems to be, the more
	|  probable its selection is. There are many ways to implement a softmax
	|  policy, this class implements selection using the most common method of
	|  sampling from a Boltzmann distribution.
	|
	|  As the epsilon-policy, this type of policy is useful to force the agent
	|  to explore an unknown model, in order to gain new information to refine
	|  it and thus gain more reward.
	|
	|  Method resolution order:
	|      QSoftmaxPolicy
	|      QPolicyInterface
	|      PolicyInterface
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (Matrix2D)q [, (float)temperature]) -> None :
	|          Basic constructor.
	|
	|          The temperature parameter must be >= 0.0
	|          otherwise the constructor will throw an std::invalid_argument.
	|
	|          @param q The QFunction this policy is linked with.
	|          @param temperature The parameter that controls the amount of exploration.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getTemperature(...)
	|      getTemperature( (QSoftmaxPolicy)self) -> float :
	|          This function will return the currently set temperature parameter.
	|
	|  sampleAction(...)
	|      sampleAction( (QSoftmaxPolicy)self, (int)s) -> int :
	|          This function chooses an action for state s with probability dependent on value.
	|
	|          This class implements softmax through the Boltzmann
	|          distribution. Thus an action will be chosen with
	|          probability:
	|
	|
	|               P(a) = (e^(Q(s,a)/t))/(Sum_b{ e^(Q(s,b)/t) })
	|
	|
	|          where t is the temperature. This value is not cached anywhere, so
	|          continuous sampling may not be extremely fast.
	|
	|          @param s The sampled state of the policy.
	|
	|          @return The chosen action.
	|
	|  setTemperature(...)
	|      setTemperature( (QSoftmaxPolicy)self, (float)t) -> None :
	|          This function sets the temperature parameter.
	|
	|          The temperature parameter determines the amount of
	|          exploration this policy will enforce when selecting actions.
	|          Following the Boltzmann distribution, as the temperature
	|          approaches infinity all actions will become equally
	|          probable. On the opposite side, as the temperature
	|          approaches zero, action selection will become completely
	|          greedy.
	|
	|          The temperature parameter must be >= 0.0 otherwise the
	|          function will do throw std::invalid_argument.
	|
	|          @param t The new temperature parameter.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from PolicyInterface:
	|
	|  getA(...)
	|      getA( (PolicyInterface)self) -> int :
	|          This function returns the number of available actions to the agent.
	|
	|  getActionProbability(...)
	|      getActionProbability( (PolicyInterface)self, (int)s, (int)a) -> float :
	|          This function returns the probability of taking the specified action in the specified state.
	|
	|          @param s The selected state.
	|          @param a The selected action.
	|
	|          @return The probability of taking the selected action in the specified state.
	|
	|  getS(...)
	|      getS( (PolicyInterface)self) -> int :
	|          This function returns the number of states of the world.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class RLearning(Boost.Python.instance)
	|  This class represents the QLearning algorithm.
	|
	|  This algorithm is a very simple but powerful way to learn the
	|  optimal QFunction for an MDP model, where the transition and reward
	|  functions are unknown. It works in an offline fashion, meaning that
	|  it can be used even if the policy that the agent is currently using
	|  is not the optimal one, or is different by the one currently
	|  specified by the QLearning QFunction.
	|
	|  The idea is to progressively update the QFunction averaging all
	|  obtained datapoints. This can be done by generating data via the
	|  model, or by simply sending the agent into the world to try stuff
	|  out. This allows to avoid modeling directly the transition and
	|  reward functions for unknown problems.
	|
	|  This algorithm is guaranteed convergence for stationary MDPs (MDPs
	|  that do not change their transition and reward functions over time),
	|  given that the learning parameter converges to 0 over time.
	|
	|  \sa setLearningRate(double)
	|
	|  At the same time, this algorithm can be used for non-stationary
	|  MDPs, and it will try to constantly keep up with changes in the
	|  environment, given that they are not huge.
	|
	|  This algorithm does not actually need to sample from the input
	|  model, and so it can be a good algorithm to apply in real world
	|  scenarios, where there would be no way to reproduce the world's
	|  behavior aside from actually trying out actions. However it is
	|  needed to know the size of the state space, the size of the action
	|  space and the discount factor of the problem.
	|
	|  Method resolution order:
	|      RLearning
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (int)S, (int)A [, (float)alpha [, (float)rho]]) -> None :
	|          Basic constructor.
	|
	|          Both learning rates must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          @param S The size of the state space.
	|          @param A The size of the action space.
	|          @param alpha The learning rate for the QFunction.
	|          @param rho The learning rate for the average reward.
	|
	|      __init__( (object)self, (MaximumLikelihoodModel)model [, (float)alpha [, (float)rho]]) -> None :
	|          Basic constructor from MaximumLikelihoodModel
	|
	|          Both learning rates must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          This constructor copies the S and A and discount parameters from
	|          the supplied model. It does not conserve the reference.
	|
	|          @param model The MDP model that QLearning will use as a base.
	|          @param alpha The learning rate for the QFunction.
	|          @param rho The learning rate for the average reward.
	|
	|      __init__( (object)self, (SparseMaximumLikelihoodModel)model [, (float)alpha [, (float)rho]]) -> None :
	|          Basic constructor from SparseMaximumLikelihoodModel
	|
	|          Both learning rates must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          This constructor copies the S and A and discount parameters from
	|          the supplied model. It does not conserve the reference.
	|
	|          @param model The MDP model that QLearning will use as a base.
	|          @param alpha The learning rate for the QFunction.
	|          @param rho The learning rate for the average reward.
	|
	|      __init__( (object)self, (Model)model [, (float)alpha [, (float)rho]]) -> None :
	|          Basic constructor from Model
	|
	|          Both learning rates must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          This constructor copies the S and A and discount parameters from
	|          the supplied model. It does not conserve the reference.
	|
	|          @param model The MDP model that QLearning will use as a base.
	|          @param alpha The learning rate for the QFunction.
	|          @param rho The learning rate for the average reward.
	|
	|      __init__( (object)self, (SparseModel)model [, (float)alpha [, (float)rho]]) -> None :
	|          Basic constructor from SparseModel
	|
	|          Both learning rates must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          This constructor copies the S and A and discount parameters from
	|          the supplied model. It does not conserve the reference.
	|
	|          @param model The MDP model that QLearning will use as a base.
	|          @param alpha The learning rate for the QFunction.
	|          @param rho The learning rate for the average reward.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getA(...)
	|      getA( (RLearning)self) -> int :
	|          This function returns the number of actions on which QLearning is working.
	|
	|  getAlphaLearningRate(...)
	|      getAlphaLearningRate( (RLearning)self) -> float :
	|          This function will return the current set alpha learning rate parameter.
	|
	|  getAverageReward(...)
	|      getAverageReward( (RLearning)self) -> float :
	|          This function returns the learned average reward.
	|
	|  getQFunction(...)
	|      getQFunction( (RLearning)self) -> Matrix2D :
	|          This function returns a reference to the internal QFunction.
	|
	|          The returned reference can be used to build Policies, for example
	|          MDP::QGreedyPolicy.
	|
	|  getRhoLearningRate(...)
	|      getRhoLearningRate( (RLearning)self) -> float :
	|          This function will return the current set rho learning rate parameter.
	|
	|  getS(...)
	|      getS( (RLearning)self) -> int :
	|          This function returns the number of states on which QLearning is working.
	|
	|  setAlphaLearningRate(...)
	|      setAlphaLearningRate( (RLearning)self, (float)a) -> None :
	|          This function sets the learning rate for the QFunction.
	|
	|          The learning parameter determines the speed at which the
	|          QFunction is modified with respect to new data. In fully
	|          deterministic environments (such as an agent moving through
	|          a grid, for example), this parameter can be safely set to
	|          1.0 for maximum learning.
	|
	|          On the other side, in stochastic environments, in order to
	|          converge this parameter should be higher when first starting
	|          to learn, and decrease slowly over time.
	|
	|          Otherwise it can be kept somewhat high if the environment
	|          dynamics change progressively, and the algorithm will adapt
	|          accordingly. The final behavior of QLearning is very
	|          dependent on this parameter.
	|
	|          The learning rate parameter must be > 0.0 and <= 1.0,
	|          otherwise the function will throw an std::invalid_argument.
	|
	|          @param a The new alpha learning rate parameter.
	|
	|  setRhoLearningRate(...)
	|      setRhoLearningRate( (RLearning)self, (float)r) -> None :
	|          This function sets the learning rate parameter for the average reward.
	|
	|          The learning parameter determines the speed at which the
	|          average reward is modified with respect to new data.
	|
	|          The learning rate parameter must be > 0.0 and <= 1.0,
	|          otherwise the function will throw an std::invalid_argument.
	|
	|          @param r The new rho learning rate parameter.
	|
	|  stepUpdateQ(...)
	|      stepUpdateQ( (RLearning)self, (int)s, (int)a, (int)s1, (float)rew) -> None :
	|          This function updates the internal QFunction using the discount set during construction.
	|
	|          This function takes a single experience point and uses it to
	|          update the QFunction. This is a very efficient method to
	|          keep the QFunction up to date with the latest experience.
	|
	|          @param s The previous state.
	|          @param a The action performed.
	|          @param s1 The new state.
	|          @param rew The reward obtained.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class SARSA(Boost.Python.instance)
	|  This class represents the SARSA algorithm.
	|
	|  This algorithm is a very simple but powerful way to learn a
	|  QFunction for an MDP model, where the transition and reward
	|  functions are unknown. It works in an online fashion, meaning that
	|  the QFunction learned is the one of the currently used policy.
	|
	|  The idea is to progressively update the QFunction averaging all
	|  obtained datapoints. This can be done by generating data via the
	|  model, or by simply sending the agent into the world to try stuff
	|  out. This allows to avoid modeling directly the transition and
	|  reward functions for unknown problems.
	|
	|  This algorithm is guaranteed convergence for stationary MDPs (MDPs
	|  that do not change their transition and reward functions over time),
	|  given that the learning parameter converges to 0 over time.
	|
	|  \sa setLearningRate(double)
	|
	|  The main difference between this algorithm and QLearning is that
	|  QLearning always tries to learn the optimal policy, regardless of
	|  the one that is currently being executed. Instead, SARSA tries to
	|  find a policy which can perform decently given exploration tradeoffs
	|  that must be done when learning the QFunction of a new environment.
	|  A possible use for this would be to run SARSA together with
	|  QLearning; during the training phase one would use SARSA actions in
	|  order to perform decently during the training. Afterwards, one could
	|  switch to the optimal policy learnt offline by QLearning.
	|
	|  This algorithm does not actually need to sample from the input
	|  model, and so it can be a good algorithm to apply in real world
	|  scenarios, where there would be no way to reproduce the world's
	|  behavior aside from actually trying out actions. However it is
	|  needed to know the size of the state space, the size of the action
	|  space and the discount factor of the problem.
	|
	|  Method resolution order:
	|      SARSA
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (int)S, (int)A [, (float)discount [, (float)alpha]]) -> None :
	|          Basic constructor.
	|
	|          The learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          @param S The state space of the underlying model.
	|          @param A The action space of the underlying model.
	|          @param discount The discount of the underlying model.
	|          @param alpha The learning rate of the SARSA method.
	|
	|      __init__( (object)self, (MaximumLikelihoodModel)model [, (float)alpha]) -> None :
	|          Basic constructor for MaximumLikelihoodModel.
	|
	|          The learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          This constructor copies the S and A and discount parameters from
	|          the supplied model. It does not keep the reference, so if the
	|          discount needs to change you'll need to update it here manually
	|          too.
	|
	|          @param model The MDP model that SARSA will use as a base.
	|          @param alpha The learning rate of the SARSA method.
	|
	|      __init__( (object)self, (SparseMaximumLikelihoodModel)model [, (float)alpha]) -> None :
	|          Basic constructor for SparseMaximumLikelihoodModel.
	|
	|          The learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          @param model The MDP model that SARSA will use as a base.
	|          @param alpha The learning rate of the SARSA method.
	|
	|      __init__( (object)self, (Model)model [, (float)alpha]) -> None :
	|          Basic constructor for Model.
	|
	|          The learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          @param model The MDP model that SARSA will use as a base.
	|          @param alpha The learning rate of the SARSA method.
	|
	|      __init__( (object)self, (SparseModel)model [, (float)alpha]) -> None :
	|          Basic constructor for SparseModel.
	|
	|          The learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          @param model The MDP model that SARSA will use as a base.
	|          @param alpha The learning rate of the SARSA method.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getA(...)
	|      getA( (SARSA)self) -> int :
	|          This function returns the number of actions on which QLearning is working.
	|
	|  getDiscount(...)
	|      getDiscount( (SARSA)self) -> float :
	|          This function returns the currently set discount parameter.
	|
	|  getLearningRate(...)
	|      getLearningRate( (SARSA)self) -> float :
	|          This function will return the current set learning rate parameter.
	|
	|  getQFunction(...)
	|      getQFunction( (SARSA)self) -> Matrix2D :
	|          This function returns a reference to the internal QFunction.
	|
	|          The returned reference can be used to build Policies, for example
	|          MDP::QGreedyPolicy.
	|
	|  getS(...)
	|      getS( (SARSA)self) -> int :
	|          This function returns the number of states on which QLearning is working.
	|
	|  setDiscount(...)
	|      setDiscount( (SARSA)self, (float)d) -> None :
	|          This function sets the new discount parameter.
	|
	|          The discount parameter controls the amount that future rewards are considered
	|          by SARSA. If 1, then any reward is the same, if obtained now or in a million
	|          timesteps. Thus the algorithm will optimize overall reward accretion. When less
	|          than 1, rewards obtained in the presents are valued more than future rewards.
	|
	|          @param d The new discount factor.
	|
	|  setLearningRate(...)
	|      setLearningRate( (SARSA)self, (float)a) -> None :
	|          This function sets the learning rate parameter.
	|
	|          The learning parameter determines the speed at which the
	|          QFunction is modified with respect to new data. In fully
	|          deterministic environments (such as an agent moving through
	|          a grid, for example), this parameter can be safely set to
	|          1.0 for maximum learning.
	|
	|          On the other side, in stochastic environments, in order to
	|          converge this parameter should be higher when first starting
	|          to learn, and decrease slowly over time.
	|
	|          Otherwise it can be kept somewhat high if the environment
	|          dynamics change progressively, and the algorithm will adapt
	|          accordingly. The final behaviour of SARSA is very
	|          dependent on this parameter.
	|
	|          The learning rate parameter must be > 0.0 and <= 1.0,
	|          otherwise the function will throw an std::invalid_argument.
	|
	|          @param a The new learning rate parameter.
	|
	|  stepUpdateQ(...)
	|      stepUpdateQ( (SARSA)self, (int)s, (int)a, (int)s1, (int)a1, (float)rew) -> None :
	|          This function updates the internal QFunction using the discount set during construction.
	|
	|          This function takes a single experience point and uses it to
	|          update the QFunction. This is a very efficient method to
	|          keep the QFunction up to date with the latest experience.
	|
	|          Keep in mind that, since SARSA needs to compute the
	|          QFunction for the currently used policy, it needs to know
	|          two consecutive state-action pairs, in order to correctly
	|          relate how the policy acts from state to state.
	|
	|          @param s The previous state.
	|          @param a The action performed.
	|          @param s1 The new state.
	|          @param a1 The action performed in the new state.
	|          @param rew The reward obtained.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class SARSAL(Boost.Python.instance)
	|  This class represents the SARSAL algorithm.
	|
	|  This algorithms adds eligibility traces to the SARSA algorithm.
	|
	|  \sa SARSA
	|
	|  In order to more effectively use the data obtained, SARSAL keeps a list
	|  of previously visited state/action pairs, which are updated together
	|  with the last experienced transition. The updates all use the same
	|  value, with the difference that state/action pairs experienced more in
	|  the past are updated less (by discount*lambda per each previous
	|  timestep). Once this reducing coefficient falls below a certain
	|  threshold, the old state/action pair is forgotten and not updated
	|  anymore. If instead the pair is visited again, the coefficient is once
	|  again increased.
	|
	|  The idea is to be able to give credit to past actions for current reward
	|  in an efficient manner. This reduces the amount of data needed in order
	|  to backpropagate rewards, and allows SARSAL to learn faster.
	|
	|  This particular version of the algorithm implements capped traces: every
	|  time an action/state pair is witnessed, its eligibility trace is reset
	|  to 1.0. This avoids potentially diverging values which can happen with
	|  the normal eligibility traces.
	|
	|  Method resolution order:
	|      SARSAL
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (int)S, (int)A [, (float)discount [, (float)alpha [, (float)lambda [, (float)tolerance]]]]) -> None :
	|          Basic constructor.
	|
	|          The learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          @param S The state space of the underlying model.
	|          @param A The action space of the underlying model.
	|          @param discount The discount of the underlying model.
	|          @param alpha The learning rate of the SARSAL method.
	|          @param lambda The lambda parameter for the eligibility traces.
	|          @param tolerance The cutoff point for eligibility traces.
	|
	|      __init__( (object)self, (MaximumLikelihoodModel)model [, (float)alpha [, (float)lambda [, (float)tolerance]]]) -> None :
	|          Basic constructor for MaximumLikelihoodModel.
	|
	|          The learning rate must be > 0.0 and <= 1.0, otherwise the
	|          constructor will throw an std::invalid_argument.
	|
	|          This constructor copies the S and A and discount parameters from
	|          the supplied model. It does not keep the reference, so if the
	|          discount needs to change you'll need to update it here manually
	|          too.
	|
	|          @param model The MDP model that SARSAL will use as a base.
	|          @param alpha The learning rate of the SARSAL method.
	|          @param lambda The lambda parameter for the eligibility traces.
	|          @param tolerance The cutoff point for eligibility traces.
	|
	|      __init__( (object)self, (SparseMaximumLikelihoodModel)model [, (float)alpha [, (float)lambda [, (float)tolerance]]]) -> None :
	|          Basic constructor for SparseMaximumLikelihoodModel.
	|
	|          This constructor copies the S and A and discount parameters from
	|          the supplied model. It does not keep the reference, so if the
	|          discount needs to change you'll need to update it here manually
	|          too.
	|
	|          @param model The MDP model that SARSALwill use as a base.
	|          @param alpha The learning rate of the SARSAL method.
	|          @param lambda The lambda parameter for the eligibility traces.
	|          @param tolerance The cutoff point for eligibility traces.
	|
	|      __init__( (object)self, (Model)model [, (float)alpha [, (float)lambda [, (float)tolerance]]]) -> None :
	|          Basic constructor for Model.
	|
	|          This constructor copies the S and A and discount parameters from
	|          the supplied model. It does not keep the reference, so if the
	|          discount needs to change you'll need to update it here manually
	|          too.
	|
	|          @param model The MDP model that SARSAL will use as a base.
	|          @param alpha The learning rate of the SARSAL method.
	|          @param lambda The lambda parameter for the eligibility traces.
	|          @param tolerance The cutoff point for eligibility traces.
	|
	|      __init__( (object)self, (SparseModel)model [, (float)alpha [, (float)lambda [, (float)tolerance]]]) -> None :
	|          Basic constructor for SparseModel.
	|
	|          This constructor copies the S and A and discount parameters from
	|          the supplied model. It does not keep the reference, so if the
	|          discount needs to change you'll need to update it here manually
	|          too.
	|
	|          @param model The MDP model that SARSAL will use as a base.
	|          @param alpha The learning rate of the SARSAL method.
	|          @param lambda The lambda parameter for the eligibility traces.
	|          @param tolerance The cutoff point for eligibility traces.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getA(...)
	|      getA( (SARSAL)self) -> int :
	|          This function returns the number of actions on which QLearning is working.
	|
	|  getDiscount(...)
	|      getDiscount( (SARSAL)self) -> float :
	|          This function returns the currently set discount parameter.
	|
	|  getLambda(...)
	|      getLambda( (SARSAL)self) -> float :
	|          This function returns the currently set lambda parameter.
	|
	|  getLearningRate(...)
	|      getLearningRate( (SARSAL)self) -> float :
	|          This function will return the current set learning rate parameter.
	|
	|  getQFunction(...)
	|      getQFunction( (SARSAL)self) -> Matrix2D :
	|          This function returns a reference to the internal QFunction.
	|
	|          The returned reference can be used to build Policies, for example
	|          MDP::QGreedyPolicy.
	|
	|  getS(...)
	|      getS( (SARSAL)self) -> int :
	|          This function returns the number of states on which QLearning is working.
	|
	|  getTolerance(...)
	|      getTolerance( (SARSAL)self) -> float :
	|          This function returns the currently set trace cutoff parameter.
	|
	|  getTraces(...)
	|      getTraces( (SARSAL)self) -> vec_trace :
	|          This function returns the currently set traces.
	|
	|  setDiscount(...)
	|      setDiscount( (SARSAL)self, (float)d) -> None :
	|          This function sets the new discount parameter.
	|
	|          The discount parameter controls the amount that future rewards are considered
	|          by SARSAL. If 1, then any reward is the same, if obtained now or in a million
	|          timesteps. Thus the algorithm will optimize overall reward accretion. When less
	|          than 1, rewards obtained in the presents are valued more than future rewards.
	|
	|          @param d The new discount factor.
	|
	|  setLambda(...)
	|      setLambda( (SARSAL)self, (float)lambda) -> None :
	|          This function sets the new lambda parameter.
	|
	|          This parameter determines how much to decrease updates for each
	|          timestep in the past. If set to zero, SARSAL effectively becomes
	|          equivalent to SARSA, as no backpropagation will be performed. If
	|          set to 1 it will result in a method similar to Monte Carlo
	|          sampling, where rewards are backed up from the end to the
	|          beginning of the episode (of course still dependent on the
	|          discount of the model).
	|
	|          The lambda parameter must be >= 0.0 and <= 1.0, otherwise the
	|          function will throw an std::invalid_argument.
	|
	|          @param l The new lambda parameter.
	|
	|  setLearningRate(...)
	|      setLearningRate( (SARSAL)self, (float)a) -> None :
	|          This function sets the learning rate parameter.
	|
	|          The learning parameter determines the speed at which the
	|          QFunction is modified with respect to new data. In fully
	|          deterministic environments (such as an agent moving through
	|          a grid, for example), this parameter can be safely set to
	|          1.0 for maximum learning.
	|
	|          On the other side, in stochastic environments, in order to
	|          converge this parameter should be higher when first starting
	|          to learn, and decrease slowly over time.
	|
	|          Otherwise it can be kept somewhat high if the environment
	|          dynamics change progressively, and the algorithm will adapt
	|          accordingly. The final behaviour of SARSAL is very
	|          dependent on this parameter.
	|
	|          The learning rate parameter must be > 0.0 and <= 1.0,
	|          otherwise the function will throw an std::invalid_argument.
	|
	|          @param a The new learning rate parameter.
	|
	|  setTolerance(...)
	|      setTolerance( (SARSAL)self, (float)t) -> None :
	|          This function sets the trace cutoff parameter.
	|
	|          This parameter determines when a trace is removed, as its
	|          coefficient has become too small to bother updating its value.
	|
	|          Note that the trace cutoff is performed on the overall
	|          discount*lambda value, and not only on lambda. So this parameter
	|          is useful even when lambda is 1.
	|
	|          @param t The new trace cutoff value.
	|
	|  stepUpdateQ(...)
	|      stepUpdateQ( (SARSAL)self, (int)s, (int)a, (int)s1, (int)a1, (float)rew) -> None :
	|          This function updates the internal QFunction using the discount set during construction.
	|
	|          This function takes a single experience point and uses it to
	|          update the QFunction. This is a very efficient method to
	|          keep the QFunction up to date with the latest experience.
	|
	|          Keep in mind that, since SARSAL needs to compute the
	|          QFunction for the currently used policy, it needs to know
	|          two consecutive state-action pairs, in order to correctly
	|          relate how the policy acts from state to state.
	|
	|          @param s The previous state.
	|          @param a The action performed.
	|          @param s1 The new state.
	|          @param a1 The action performed in the new state.
	|          @param rew The reward obtained.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	[output clipped, log limit 200KiB/s reached]
[22/22] RUN cd build/examples && python3 -c "import AIToolbox; help(AIToolbox.POMDP)"
	Help on module AIToolbox.POMDP in AIToolbox:
	
	NAME
	AIToolbox.POMDP
	
	CLASSES
	AIToolbox.MDP.Model(Boost.Python.instance)
	Model
	AIToolbox.MDP.SparseModel(Boost.Python.instance)
	SparseModel
	Boost.Python.instance(builtins.object)
	AMDP
	GapMin
	IO
	IncrementalPruning
	LinearSupport
	PBVI
	PERSEUS
	POMCPModel
	POMCPSparseModel
	POMDP_VFun
	PolicyInterface
	Policy
	QMDP
	RTBSSModel
	RTBSSSparseModel
	VEntry
	VList
	Witness
	
	class AMDP(Boost.Python.instance)
	|  This class implements the Augmented MDP algorithm.
	|
	|  This algorithm transforms a POMDP into an approximately equivalent
	|  MDP. This is done by extending the original POMDP statespace with
	|  a discretized entropy component, which approximates a sufficient
	|  statistic for the belief. In essence, AMDP builds states which
	|  contain intrinsically information about the uncertainty of the agent.
	|
	|  In order to compute a new transition and reward function, AMDP needs
	|  to sample possible transitions at random, since each belief can
	|  potentially update to any other belief. We sample beliefs using
	|  the BeliefGenerator class which creates both random beliefs and
	|  beliefs generated using the original POMDP model, in order to try
	|  to obtain beliefs distributed in a way that better resembles the
	|  original problem.
	|
	|  Once this is done, it is simply a matter of taking each belief,
	|  computing every possible new belief given an action and observation,
	|  and sum up all possibilities.
	|
	|  This class also bundles together with the resulting MDP a function
	|  to convert an original POMDP belief into an equivalent AMDP state;
	|  this is done so that a policy can be applied, observation gathered
	|  and beliefs updated while continuing to use the approximated model.
	|
	|  Method resolution order:
	|      AMDP
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (int)nBeliefs, (int)entropyBuckets) -> None :
	|          Basic constructor.
	|
	|          @param nBeliefs The number of beliefs to sample from when
	|                 building the MDP model.
	|          @param entropyBuckets The number of buckets into which
	|                 discretize entropy.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  discretizeDense(...)
	|      discretizeDense( (AMDP)self, (Model)model) -> object :
	|          This function constructs an approximate *dense* MDP of the provided POMDP model.
	|
	|          @param model The POMDP model to be approximated.
	|
	|          @return A tuple containing a dense MDP model which approximates
	|                  the POMDP argument, and a function that converts a POMDP
	|                  belief into a state of the MDP model.
	|
	|  discretizeSparse(...)
	|      discretizeSparse( (AMDP)self, (SparseModel)model) -> object :
	|          This function constructs an approximate *sparse* MDP of the provided POMDP model.
	|
	|          @param model The POMDP model to be approximated.
	|
	|          @return A tuple containing a sparse MDP model which approximates
	|                  the POMDP argument, and a function that converts a POMDP
	|                  belief into a state of the MDP model.
	|
	|  getBeliefSize(...)
	|      getBeliefSize( (AMDP)self) -> int :
	|          This function returns the currently set number of sampled beliefs.
	|
	|  getEntropyBuckets(...)
	|      getEntropyBuckets( (AMDP)self) -> int :
	|          This function returns the currently set number of entropy buckets.
	|
	|  setBeliefSize(...)
	|      setBeliefSize( (AMDP)self, (int)nBeliefs) -> None :
	|          This function sets a new number of sampled beliefs.
	|
	|  setEntropyBuckets(...)
	|      setEntropyBuckets( (AMDP)self, (int)buckets) -> None :
	|          This function sets the new number of buckets in which to discretize the entropy.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class GapMin(Boost.Python.instance)
	|  This class implements the GapMin algorithm.
	|
	|  This method works by repeatedly refining both a lower bound and upper
	|  bound for the input POMDP.
	|
	|  The lower bound is worked through PBVI.
	|
	|  The upper bound is worked through a combination of alphavectors, and a
	|  belief-value pair piecewise linear surface.
	|
	|  At each iteration, a set of beliefs are found that the algorithm thinks
	|  may be useful to reduce the bound.
	|
	|  For the lower bound, these beliefs are added to a list, and run through
	|  PBVI. Spurious beliefs are then removed.
	|
	|  For the upper bound, the beliefs are used to create a temporary belief
	|  POMDP, where each belief is a state. This belief is then used as input
	|  to the FastInformedBound algorithm, which refines its upper bound.
	|
	|  The strong point of the algorithm is that beliefs are searched by gap
	|  size, so that the beliefs that are most likely to decrease the gap are
	|  looked at first. This results in less overall work to highly reduce the
	|  bound.
	|
	|  In order to act, the output lower bound should be used (as it's the only
	|  one that gives an actual guarantee), but for this just using PBVI may be
	|  more useful.
	|
	|  Method resolution order:
	|      GapMin
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __call__(...)
	|      __call__( (GapMin)self, (Model)model, (Vector)initialBelief) -> object :
	|          This function efficiently computes bounds for the optimal value of the input belief for the input POMDP.
	|
	|          @param model The model to compute the gap for.
	|          @param initialBelief The belief to compute the gap for.
	|
	|          @return The lower and upper gap bounds, the lower bound VList, and the upper bound QFunction.
	|
	|      __call__( (GapMin)self, (SparseModel)model, (Vector)initialBelief) -> object :
	|          This function efficiently computes bounds for the optimal value of the input belief for the input POMDP.
	|
	|          @param model The model to compute the gap for.
	|          @param initialBelief The belief to compute the gap for.
	|
	|          @return The lower and upper gap bounds, the lower bound VList, and the upper bound QFunction.
	|
	|  __init__(...)
	|      __init__( (object)self, (float)initialTolerance, (int)precisionDigits) -> None :
	|          Basic constructor.
	|
	|          The input parameters can heavily influence both the time and the
	|          strictness of the resulting bound.
	|
	|          The tolerance parameter must be >= 0.0, otherwise the
	|          function will throw an std::runtime_error.
	|
	|          \sa setInitialTolerance(double)
	|          \sa setPrecisionDigits(unsigned)
	|
	|          @param initialTolerance The tolerance to compute the initial bounds.
	|          @param precisionDigits The number of digits precision to stop the gap searching process.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getInitialTolerance(...)
	|      getInitialTolerance( (GapMin)self) -> float :
	|          This function returns the initial tolerance used to compute the initial bounds.
	|
	|  getPrecisionDigits(...)
	|      getPrecisionDigits( (GapMin)self) -> int :
	|          This function returns the currently set digits of precision.
	|
	|          \sa setPrecisionDigits(unsigned);
	|
	|          @return The currently set digits of precision to use to test for convergence.
	|
	|  setInitialTolerance(...)
	|      setInitialTolerance( (GapMin)self, (float)initialTolerance) -> None :
	|          This function sets the initial tolerance used to compute the initial bounds.
	|          This value is only used before having an initial bounds
	|          approximation. Once that has been established, the tolerance is
	|          dependent on the digits of precision parameter.
	|
	|          The tolerance parameter must be >= 0.0, otherwise the
	|          function will throw an std::runtime_error.
	|
	|          \sa setPrecisionDigits(unsigned);
	|
	|          @param initialTolerance The new initial tolerance.
	|
	|  setPrecisionDigits(...)
	|      setPrecisionDigits( (GapMin)self, (int)precisionDigits) -> None :
	|          This function sets the digits in precision for the returned solution.
	|          Depending on the values for the input model, the precision of
	|          the solution is automatically adjusted to the input precision
	|          digits.
	|
	|          In particular, the return threshold is equal to:
	|
	|              std::pow(10, std::ceil(std::log10(std::max(std::fabs(ub), std::fabs(lb))))-precisionDigits);
	|
	|          This is used in two ways:
	|
	|          - To check for lower and upper bound convergence. If the bounds
	|            difference is less than the threshold, the GapMin terminates.
	|          - To check for gap size converngence. If the gap has not reduced
	|            by more than the threshold during the last iteration, GapMin
	|            terminates.
	|
	|          @param digits The number of digits of precision to use to test for convergence.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class IO(Boost.Python.instance)
	|  This class wraps C++ MDP IO functionality.
	|
	|  While the models in Python can be pickled in order to save them,
	|  this does not allow direct interaction between C++ and Python code.
	|
	|  This class wraps the common AIToolbox operator<< and operator>> for
	|  MDP classes, so that they can be saved and loaded equally from both
	|  C++ and Python. The format is human-friendly (and thus
	|  space-unfriendly); if the models are supposed to only be used in
	|  Python, pickling is recommended.
	|
	|  The models are returned in strings as to avoid having Boost Python
	|  pass Python files to C++; the strings can be saved on files in
	|  whatever method you wish
	|
	|  Method resolution order:
	|      IO
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)arg1) -> None
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  readModel(...)
	|      readModel( (IO)arg1, (str)arg2, (Model)arg3) -> None
	|
	|  readSparseModel(...)
	|      readSparseModel( (IO)arg1, (str)arg2, (SparseModel)arg3) -> None
	|
	|  writeModel(...)
	|      writeModel( (IO)arg1, (Model)arg2) -> str
	|
	|  writeSparseModel(...)
	|      writeSparseModel( (IO)arg1, (SparseModel)arg2) -> str
	|
	|  ----------------------------------------------------------------------
	|  Data and other attributes defined here:
	|
	|  __instance_size__ = 24
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class IncrementalPruning(Boost.Python.instance)
	|  This class implements the Incremental Pruning algorithm.
	|
	|  This algorithm solves a POMDP Model perfectly. It computes solutions
	|  for each horizon incrementally, every new solution building upon the
	|  previous one.
	|
	|  From each solution, it computes the full set of possible
	|  projections. It then computes all possible cross-sums of such
	|  projections, in order to compute all possible vectors that can be
	|  included in the final solution.
	|
	|  What makes this method unique is its pruning strategy. Instead of
	|  generating every possible vector, combining them and pruning, it
	|  tries to prune at every possible occasion in order to minimize the
	|  number of possible vectors at any given time. Thus it will prune
	|  after creating the projections, after every single cross-sum, and
	|  in the end when combining all projections for each action.
	|
	|  The performances of this method are *heavily* dependent on the linear
	|  programming methods used. In particular, this code currently
	|  utilizes the lp_solve55 library. However, this library is not the
	|  most efficient implementation, as it defaults to a somewhat slow
	|  solver, and its problem-building API also tends to be slow due to
	|  lots of bounds checking (which are cool, but sometimes people know
	|  what they are doing). Still, to avoid replicating infinite amounts
	|  of code and managing memory by ourselves, we use its API. It would
	|  be nice if one day we could port directly into the code a fast lp
	|  implementation; for now we do what we can.
	|
	|  Method resolution order:
	|      IncrementalPruning
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __call__(...)
	|      __call__( (IncrementalPruning)self, (Model)model) -> object :
	|          This function solves a POMDP::Model completely.
	|
	|          This function is pretty expensive (as are possibly all POMDP
	|          solvers).  It generates for each new solved timestep the
	|          whole set of possible ValueFunctions, and prunes it
	|          incrementally, trying to reduce as much as possible the
	|          linear programming solves required.
	|
	|          @param model The POMDP model that needs to be solved.
	|
	|          @return A tuple containing the maximum variation for the
	|                  ValueFunction and the computed ValueFunction.
	|
	|      __call__( (IncrementalPruning)self, (SparseModel)model) -> object :
	|          This function solves a POMDP::Model completely.
	|
	|          This function is pretty expensive (as are possibly all POMDP
	|          solvers).  It generates for each new solved timestep the
	|          whole set of possible ValueFunctions, and prunes it
	|          incrementally, trying to reduce as much as possible the
	|          linear programming solves required.
	|
	|          @param model The POMDP model that needs to be solved.
	|
	|          @return A tuple containing the maximum variation for the
	|                  ValueFunction and the computed ValueFunction.
	|
	|  __init__(...)
	|      __init__( (object)self, (int)horizon, (float)tolerance) -> None :
	|          Basic constructor.
	|
	|          This constructor sets the default horizon used to solve a POMDP::Model.
	|
	|          The tolerance parameter must be >= 0.0, otherwise the
	|          constructor will throw an std::runtime_error. The tolerance
	|          parameter sets the convergence criterion. A tolerance of 0.0
	|          forces IncrementalPruning to perform a number of iterations
	|          equal to the horizon specified. Otherwise, IncrementalPruning
	|          will stop as soon as the difference between two iterations
	|          is less than the tolerance specified.
	|
	|          @param h The horizon chosen.
	|          @param tolerance The tolerance factor to stop the value iteration loop.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getHorizon(...)
	|      getHorizon( (IncrementalPruning)self) -> int :
	|          This function returns the currently set horizon parameter.
	|
	|  getTolerance(...)
	|      getTolerance( (IncrementalPruning)self) -> float :
	|          This function returns the currently set tolerance parameter.
	|
	|  setHorizon(...)
	|      setHorizon( (IncrementalPruning)self, (int)horizon) -> None :
	|          This function allows setting the horizon parameter.
	|
	|  setTolerance(...)
	|      setTolerance( (IncrementalPruning)self, (float)t) -> None :
	|          This function sets the tolerance parameter.
	|
	|          The tolerance parameter must be >= 0.0, otherwise the
	|          constructor will throw an std::runtime_error. The tolerance
	|          parameter sets the convergence criterion. A tolerance of 0.0
	|          forces IncrementalPruning to perform a number of iterations
	|          equal to the horizon specified. Otherwise, IncrementalPruning
	|          will stop as soon as the difference between two iterations
	|          is less than the tolerance specified.
	|
	|          @param t The new tolerance parameter.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class LinearSupport(Boost.Python.instance)
	|  This class represents the LinearSupport algorithm.
	|
	|  This method is similar in spirit to Witness. The idea is that we look at
	|  certain belief points, and we try to find the best alphavectors in those
	|  points. Rather than looking for them though, the idea here is that we
	|  *know* where they are, if there are any at all.
	|
	|  As the ValueFunction is piecewise linear and convex, if there's any
	|  other hyperplane that we can add to improve it, the improvements are
	|  going to be maximal at one of the vertices of the original surface.
	|
	|  The idea thus is the following: first we compute the set of alphavectors
	|  for the corners, so we can be sure about them. Then we find all vertices
	|  that those alphavectors create, and we compute the error between the
	|  true ValueFunction and their current values.
	|
	|  If the error is greater than a certain amount, we allow their supporting
	|  alphavector to join the ValueFunction, and we increase the size of the
	|  vertex set by adding all new vertices that are created by adding the new
	|  surface (and removing the ones that are made useless by it).
	|
	|  We repeat until we have checked all available vertices, and at that
	|  point we are done.
	|
	|  While this can be a very inefficient algorithm, the fact that vertices
	|  are checked in an orderly fashion, from highest error to lowest, allows
	|  if one needs it to convert this algorithm into an anytime algorithm.
	|  Even if there is limited time to compute the solution, the algorithm is
	|  guaranteed to work in the areas with high error first, allowing one to
	|  compute good approximations even without a lot of resources.
	|
	|  Method resolution order:
	|      LinearSupport
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __call__(...)
	|      __call__( (LinearSupport)self, (Model)model) -> object :
	|          This function solves a POMDP::Model completely.
	|
	|          This function is pretty expensive (as are possibly all POMDP
	|          solvers). It evaluates all vertices in the ValueFunction surface
	|          in order to determine whether it is complete, otherwise it
	|          improves it incrementally.
	|
	|          @param model The POMDP model that needs to be solved.
	|
	|          @return A tuple containing the maximum variation for the
	|                  ValueFunction and the computed ValueFunction.
	|
	|      __call__( (LinearSupport)self, (SparseModel)model) -> object :
	|          This function solves a POMDP::Model completely.
	|
	|          This function is pretty expensive (as are possibly all POMDP
	|          solvers). It evaluates all vertices in the ValueFunction surface
	|          in order to determine whether it is complete, otherwise it
	|          improves it incrementally.
	|
	|          @param model The POMDP model that needs to be solved.
	|
	|          @return A tuple containing the maximum variation for the
	|                  ValueFunction and the computed ValueFunction.
	|
	|  __init__(...)
	|      __init__( (object)self, (int)horizon, (float)tolerance) -> None :
	|          Basic constructor.
	|
	|          This constructor sets the default horizon used to solve a POMDP::Model.
	|
	|          The tolerance parameter must be >= 0.0, otherwise the
	|          constructor will throw an std::runtime_error. The tolerance
	|          parameter sets the convergence criterion. A tolerance of 0.0
	|          forces LinearSupport to perform a number of iterations equal to
	|          the horizon specified. Otherwise, LinearSupport will stop as soon
	|          as the difference between two iterations is less than the
	|          tolerance specified.
	|
	|          @param h The horizon chosen.
	|          @param tolerance The tolerance factor to stop the value iteration loop.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getHorizon(...)
	|      getHorizon( (LinearSupport)self) -> int :
	|          This function returns the currently set horizon parameter.
	|
	|  getTolerance(...)
	|      getTolerance( (LinearSupport)self) -> float :
	|          This function returns the currently set tolerance parameter.
	|
	|  setHorizon(...)
	|      setHorizon( (LinearSupport)self, (int)horizon) -> None :
	|          This function allows setting the horizon parameter.
	|
	|  setTolerance(...)
	|      setTolerance( (LinearSupport)self, (float)t) -> None :
	|          This function sets the tolerance parameter.
	|
	|          The tolerance parameter must be >= 0.0, otherwise the
	|          constructor will throw an std::runtime_error. The tolerance
	|          parameter sets the convergence criterion. A tolerance of 0.0
	|          forces LinearSupport to perform a number of iterations equal to
	|          the horizon specified. Otherwise, LinearSupport will stop as soon
	|          as the difference between two iterations is less than the
	|          tolerance specified.
	|
	|          @param t The new tolerance parameter.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class Model(AIToolbox.MDP.Model)
	|  This class represents a Partially Observable Markov Decision Process.
	|
	|  This class inherits from any valid MDP model type, so that it can
	|  use its base methods, and it builds from those. Templated inheritance
	|  was chosen to improve performance and keep code small, instead of
	|  doing composition.
	|
	|  A POMDP is an MDP where the agent, at each timestep, does not know
	|  in which state it is. Instead, after each action is performed, it
	|  obtains an 'observation', which offers some information as to which
	|  new state the agent has transitioned to. This observation is
	|  determined by an 'observation function', that maps S'xAxO to a
	|  probability: the probability of obtaining observation O after taking
	|  action A and *landing* in state S'.
	|
	|  Since now its knowledge is imperfect, in order to represent the
	|  knowledge of the state it is currently in, the agent is thus forced
	|  to use Beliefs: probability distributions over states.
	|
	|  The way a Belief works is that, after each action and observation,
	|  the agent can reason as follows: given my previous Belief
	|  (distribution over states) that I think I was in, what is now the
	|  probability that I transitioned to any particular state? This new
	|  Belief can be computed from the Model, given that the agent knows
	|  the distributions of the transition and observation functions.
	|
	|  Turns out that a POMDP can be viewed as an MDP with an infinite
	|  number of states, where each state is essentially a Belief. Since a
	|  Belief is a vector of real numbers, there are infinite of them, thus
	|  the infinite number of states. While POMDPs can be much more
	|  powerful than MDPs for modeling real world problems, where
	|  information is usually not perfect, it turns out that this
	|  infinite-state property makes them so much harder to solve
	|  perfectly, and their solutions much more complex.
	|
	|  A POMDP solution is composed by several policies, which apply in
	|  different ranges of the Belief space, and suggest different actions
	|  depending on the observations received by the agent at each
	|  timestep. The values of those policies can be, in the same way,
	|  represented as a number of value vectors (called alpha vectors in
	|  the literature) that apply in those same ranges of the Belief space.
	|  Each alpha vector is somewhat similar to an MDP ValueFunction.
	|
	|  Method resolution order:
	|      Model
	|      AIToolbox.MDP.Model
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __getinitargs__(...)
	|      __getinitargs__( (Model)arg1) -> tuple
	|
	|  __getstate__(...)
	|      __getstate__( (Model)arg1) -> tuple
	|
	|  __init__(...)
	|      __init__( (object)self, (int)o, (int)s, (int)a [, (float)discount]) -> None :
	|          Basic constructor.
	|
	|          This constructor initializes the observation function
	|          so that all actions will return observation 0.
	|
	|          This constructor initializes the Model so that all
	|          transitions happen with probability 0 but for transitions
	|          that bring back to the same state, no matter the action.
	|
	|          All rewards are set to 0. The discount parameter is set to
	|          1.
	|
	|          @param o The number of possible observations the agent could make.
	|          @param s The number of states of the world.
	|          @param a The number of actions available to the agent.
	|          @param discount The discount factor for the MDP.
	|
	|
	|      __init__( (object)self, (Model)model) -> None :
	|          This allows to copy from any other model. A nice use for this is to
	|          convert any model which computes probabilities on the fly into an
	|          MDP::Model where probabilities are all stored for fast access. Of
	|          course such a solution can be done only when the number of states
	|          and actions is not too big.
	|
	|      __init__( (object)self, (SparseModel)sparseModel) -> None :
	|          This allows to copy from any other model. A nice use for this is to
	|          convert any model which computes probabilities on the fly into an
	|          MDP::Model where probabilities are all stored for fast access. Of
	|          course such a solution can be done only when the number of states
	|          and actions is not too big.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  __setstate__(...)
	|      __setstate__( (Model)arg1, (tuple)arg2) -> None
	|
	|  getO(...)
	|      getO( (Model)self) -> int :
	|          This function returns the number of observations possible.
	|
	|  getObservationProbability(...)
	|      getObservationProbability( (Model)self, (int)s, (int)a, (int)s1) -> float :
	|          This function returns the stored observation probability for the specified state-action pair.
	|
	|  sampleOR(...)
	|      sampleOR( (Model)self, (int)s, (int)a, (int)s1) -> object :
	|          This function samples the POMDP for the specified state action pair.
	|
	|          This function samples the model for simulated experience.
	|          The transition, observation and reward functions are used to
	|          produce, from the state, action and new state inserted as
	|          arguments, a possible new observation and reward. The
	|          observation and rewards are picked so that they are
	|          consistent with the specified new state.
	|
	|          @param s The state that needs to be sampled.
	|          @param a The action that needs to be sampled.
	|          @param s1 The resulting state of the s,a transition.
	|
	|          @return A tuple containing a new observation and reward.
	|
	|  sampleSOR(...)
	|      sampleSOR( (Model)self, (int)s, (int)a) -> object :
	|          This function samples the POMDP for the specified state action pair.
	|
	|          This function samples the model for simulated experience. The
	|          transition, observation and reward functions are used to
	|          produce, from the state action pair inserted as arguments, a
	|          possible new state with respective observation and reward.
	|          The new state is picked from all possible states that the
	|MDP allows transitioning to, each with probability equal to
	|          the same probability of the transition in the model. After a
	|          new state is picked, an observation is sampled from the
	|          observation function distribution, and finally the reward is
	|          the corresponding reward contained in the reward function.
	|
	|          @param s The state that needs to be sampled.
	|          @param a The action that needs to be sampled.
	|
	|          @return A tuple containing a new state, observation and reward.
	|
	|  setObservationFunction(...)
	|      setObservationFunction( (Model)self, (object)observationFunction3D) -> None :
	|          This function replaces the Model observation function with the one provided.
	|
	|          Currently the Python wrappings support reading through native 3d Python
	|          arrays (so [][][]). As long as the dimensions are correct and they contain
	|          correct probabilities everything should be fine. The code should reject
	|          them otherwise.
	|
	|  ----------------------------------------------------------------------
	|  Data and other attributes defined here:
	|
	|  __safe_for_unpickling__ = True
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from AIToolbox.MDP.Model:
	|
	|  getA(...)
	|      getA( (Model)self) -> int :
	|          This function returns the number of available actions to the agent.
	|
	|  getDiscount(...)
	|      getDiscount( (Model)self) -> float :
	|          This function returns the currently set discount factor.
	|
	|  getExpectedReward(...)
	|      getExpectedReward( (Model)self, (int)s, (int)a, (int)s1) -> float :
	|          This function returns the stored expected reward for the specified transition.
	|
	|  getS(...)
	|      getS( (Model)self) -> int :
	|          This function returns the number of states of the world.
	|
	|  getTransitionProbability(...)
	|      getTransitionProbability( (Model)self, (int)s, (int)a, (int)s1) -> float :
	|          This function returns the stored transition probability for the specified transition.
	|
	|  isTerminal(...)
	|      isTerminal( (Model)self, (int)s) -> bool :
	|          This function returns whether a given state is a terminal.
	|
	|  sampleSR(...)
	|      sampleSR( (Model)self, (int)s, (int)a) -> object :
	|          This function samples the MDP for the specified state action pair.
	|
	|          This function samples the model for simulated experience.
	|          The transition and reward functions are used to produce,
	|          from the state action pair inserted as arguments, a possible
	|          new state with respective reward.  The new state is picked
	|          from all possible states that the MDP allows transitioning
	|          to, each with probability equal to the same probability of
	|          the transition in the model. After a new state is picked,
	|          the reward is the corresponding reward contained in the
	|          reward function.
	|
	|          @param s The state that needs to be sampled.
	|          @param a The action that needs to be sampled.
	|
	|          @return A tuple containing a new state and a reward.
	|
	|  setDiscount(...)
	|      setDiscount( (Model)self, (float)discount) -> None :
	|          This function sets a new discount factor for the Model.
	|
	|  setRewardFunction(...)
	|      setRewardFunction( (Model)self, (object)rewardFunction3D) -> None :
	|          This function replaces the Model reward function with the one provided.
	|
	|          Currently the Python wrappings support reading through native 3d Python
	|          arrays (so [][][]). As long as the dimensions are correct and they contain
	|          correct probabilities everything should be fine. The code should reject
	|          them otherwise.
	|
	|  setTransitionFunction(...)
	|      setTransitionFunction( (Model)self, (object)transitionFunction3D) -> None :
	|          This function replaces the Model transition function with the one provided.
	|
	|          Currently the Python wrappings support reading through native 3d Python
	|          arrays (so [][][]). As long as the dimensions are correct and they contain
	|          correct probabilities everything should be fine. The code should reject
	|          them otherwise.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class PBVI(Boost.Python.instance)
	|  This class implements the Point Based Value Iteration algorithm.
	|
	|  The idea behind this algorithm is to solve a POMDP Model
	|  approximately. When computing a perfect solution, the main problem
	|  is pruning the resulting ValueFunction in order to contain only a
	|  parsimonious representation. What this means is that many vectors
	|  inside can be dominated by others, and so they do not add any
	|  additional information, while at the same time occupying memory and
	|  computational time.
	|
	|  The way this method tries to fix the problem is by solving the Model
	|  in a set of specified Beliefs. Doing so results in no need for
	|  pruning at all, since every belief uniquely identifies one of the
	|  optimal solution vectors (only uniqueness in the final set is
	|  required, but it is way cheaper than linear programming).
	|
	|  The set of Beliefs are stochastically computed as to cover as much
	|  as possible of the belief space, to ensure minimization of the final
	|  error. The final solution will thus be correct 100% in the Beliefs
	|  that have been selected, and will (possibly) overshoot in
	|  non-covered Beliefs.
	|
	|  In addition, the fact that we solve only for a fixed set of Beliefs
	|  guarantees that our final solution is limited in size, which is
	|  useful since even small POMDP true solutions can explode in size
	|  with high horizons, for very little gain.
	|
	|  There is no convergence guarantee of this method, but the error is
	|  bounded.
	|
	|  Method resolution order:
	|      PBVI
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __call__(...)
	|      __call__( (PBVI)self, (Model)model, (POMDP_VFun)v) -> object :
	|          This function solves a POMDP::Model approximately.
	|
	|          This function computes a set of beliefs for which to solve
	|          the input model. The beliefs are chosen stochastically,
	|          trying to cover as much as possible of the belief space in
	|          order to offer as precise a solution as possible. The final
	|          solution will only contain ValueFunctions for those Beliefs
	|          and will interpolate them for points it did not solve for.
	|          Even though the resulting solution is approximate very often
	|          it is good enough, and this comes with an incredible
	|          increase in speed.
	|
	|          Note that even in the beliefs sampled the solution is not
	|          guaranteed to be optimal. This is because a solution for
	|          horizon h can only be computed with the true solution from
	|          horizon h-1. If such a solution is approximate (and it is
	|          here), then the solution for h will not be optimal by
	|          definition.
	|
	|          @param model The POMDP model that needs to be solved.
	|          @param v The ValueFunction to startup the process from, if needed.
	|
	|          @return A tuple containing the maximum variation for the
	|                  ValueFunction and the computed ValueFunction.
	|
	|      __call__( (PBVI)self, (SparseModel)model, (POMDP_VFun)v) -> object :
	|          This function solves a POMDP::Model approximately.
	|
	|          This function computes a set of beliefs for which to solve
	|          the input model. The beliefs are chosen stochastically,
	|          trying to cover as much as possible of the belief space in
	|          order to offer as precise a solution as possible. The final
	|          solution will only contain ValueFunctions for those Beliefs
	|          and will interpolate them for points it did not solve for.
	|          Even though the resulting solution is approximate very often
	|          it is good enough, and this comes with an incredible
	|          increase in speed.
	|
	|          Note that even in the beliefs sampled the solution is not
	|          guaranteed to be optimal. This is because a solution for
	|          horizon h can only be computed with the true solution from
	|          horizon h-1. If such a solution is approximate (and it is
	|          here), then the solution for h will not be optimal by
	|          definition.
	|
	|          @param model The POMDP model that needs to be solved.
	|          @param v The ValueFunction to startup the process from, if needed.
	|
	|          @return A tuple containing the maximum variation for the
	|                  ValueFunction and the computed ValueFunction.
	|
	|  __init__(...)
	|      __init__( (object)self, (int)nBeliefs, (int)h, (float)tolerance) -> None :
	|          Basic constructor.
	|
	|          This constructor sets the default horizon/tolerance used to
	|          solve a POMDP::Model and the number of beliefs used to
	|          approximate the ValueFunction.
	|
	|          @param nBeliefs The number of support beliefs to use.
	|          @param h The horizon chosen.
	|          @param tolerance The tolerance factor to stop the PBVI loop.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getBeliefSize(...)
	|      getBeliefSize( (PBVI)self) -> int :
	|          This function returns the currently set number of support beliefs to use during a solve pass.
	|
	|  getHorizon(...)
	|      getHorizon( (PBVI)self) -> int :
	|          This function returns the currently set horizon parameter.
	|
	|  getTolerance(...)
	|      getTolerance( (PBVI)self) -> float :
	|          This function returns the currently set tolerance parameter.
	|
	|  setBeliefSize(...)
	|      setBeliefSize( (PBVI)self, (int)nBeliefs) -> None :
	|          This function sets a new number of support beliefs.
	|
	|  setHorizon(...)
	|      setHorizon( (PBVI)self, (int)horizon) -> None :
	|          This function sets a new horizon parameter.
	|
	|  setTolerance(...)
	|      setTolerance( (PBVI)self, (float)t) -> None :
	|          This function sets the tolerance parameter.
	|
	|          The tolerance parameter must be >= 0.0, otherwise the
	|          constructor will throw an std::runtime_error. The tolerance
	|          parameter sets the convergence criterion. A tolerance of 0.0
	|          forces PBVI to perform a number of iterations equal to
	|          the horizon specified. Otherwise, PBVI will stop as soon
	|          as the difference between two iterations is less than the
	|          tolerance specified.
	|
	|          @param t The new tolerance parameter.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class PERSEUS(Boost.Python.instance)
	|  This class implements the PERSEUS algorithm.
	|
	|  The idea behind this algorithm is very similar to PBVI. The thing
	|  that changes is how beliefs are considered; in PERSEUS we only try
	|  to find as little VEntries as possible as to ensure that all beliefs
	|  considered are improved. This allows to skip generating VEntry for
	|  most beliefs considered, since usually few VEntry are responsible
	|  for supporting most of the beliefs.
	|
	|  At the same time, this means that solutions found by PERSEUS may be
	|  *extremely* approximate with respect to the true Value Functions. This
	|  is because as long as the values for all the particle beliefs are
	|  increased, no matter how slightly, the algorithm stops looking - in
	|  effect simply guaranteeing that the worst action is never taken.
	|  However for many problems the solution found is actually very good,
	|  also given that due to the increased performance PERSEUS can do
	|  many more iterations than, for example, PBVI.
	|
	|  This method is works best when it is allowed to iterate until convergence,
	|  and thus shouldn't be used on problems with finite horizons.
	|
	|  Method resolution order:
	|      PERSEUS
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __call__(...)
	|      __call__( (PERSEUS)self, (Model)model, (float)minReward) -> object :
	|          This function solves a POMDP::Model approximately.
	|
	|          This function computes a set of beliefs for which to solve
	|          the input model. The beliefs are chosen stochastically,
	|          trying to cover as much as possible of the belief space in
	|          order to offer as precise a solution as possible.
	|
	|          The final solution will try to be as small as possible, in
	|          order to drastically improve performances, while at the same
	|          time provide a reasonably good result.
	|
	|          Note that the model input cannot have a discount of 1, due to
	|          how PERSEUS initializes the value function internally; if
	|          the model provided has a discount of 1 we throw.
	|
	|          @param model The POMDP model that needs to be solved.
	|          @param minReward The minimum reward obtainable from this model.
	|
	|          @return A tuple containing the maximum variation for the
	|                  ValueFunction and the computed ValueFunction.
	|
	|      __call__( (PERSEUS)self, (SparseModel)model, (float)minReward) -> object :
	|          This function solves a POMDP::Model approximately.
	|
	|          This function computes a set of beliefs for which to solve
	|          the input model. The beliefs are chosen stochastically,
	|          trying to cover as much as possible of the belief space in
	|          order to offer as precise a solution as possible.
	|
	|          The final solution will try to be as small as possible, in
	|          order to drastically improve performances, while at the same
	|          time provide a reasonably good result.
	|
	|          Note that the model input cannot have a discount of 1, due to
	|          how PERSEUS initializes the value function internally; if
	|          the model provided has a discount of 1 we throw.
	|
	|          @param model The POMDP model that needs to be solved.
	|          @param minReward The minimum reward obtainable from this model.
	|
	|          @return A tuple containing the maximum variation for the
	|                  ValueFunction and the computed ValueFunction.
	|
	|  __init__(...)
	|      __init__( (object)self, (int)nBeliefs, (int)h, (float)tolerance) -> None :
	|          Basic constructor.
	|
	|          This constructor sets the default horizon/tolerance used to
	|          solve a POMDP::Model and the number of beliefs used to
	|          approximate the ValueFunction.
	|
	|          @param nBeliefs The number of support beliefs to use.
	|          @param h The horizon chosen.
	|          @param tolerance The tolerance factor to stop the PERSEUS loop.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getBeliefSize(...)
	|      getBeliefSize( (PERSEUS)self) -> int :
	|          This function returns the currently set number of support beliefs to use during a solve pass.
	|
	|  getHorizon(...)
	|      getHorizon( (PERSEUS)self) -> int :
	|          This function returns the currently set horizon parameter.
	|
	|  getTolerance(...)
	|      getTolerance( (PERSEUS)self) -> float :
	|          This function returns the currently set tolerance parameter.
	|
	|  setBeliefSize(...)
	|      setBeliefSize( (PERSEUS)self, (int)nBeliefs) -> None :
	|          This function sets a new number of support beliefs.
	|
	|  setHorizon(...)
	|      setHorizon( (PERSEUS)self, (int)horizon) -> None :
	|          This function sets a new horizon parameter.
	|
	|  setTolerance(...)
	|      setTolerance( (PERSEUS)self, (float)tolerance) -> None :
	|          This function sets the tolerance parameter.
	|
	|          The tolerance parameter must be >= 0.0, otherwise the
	|          constructor will throw an std::runtime_error. The tolerance
	|          parameter sets the convergence criterion. A tolerance of 0.0
	|          forces PERSEUS to perform a number of iterations equal to
	|          the horizon specified. Otherwise, PERSEUS will stop as soon
	|          as the difference between two iterations is less than the
	|          tolerance specified.
	|
	|          @param tolerance The new tolerance parameter.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class POMCPModel(Boost.Python.instance)
	|  This class represents the POMCP online planner using UCB1 for Model.
	|
	|  NOTE: This algorithm is wrapped in Python, but as it uses the internal
	|  Models rather than a custom generative model to simulate rollouts it will
	|  probably be rather slow for interesting applications. You are of course
	|  welcome to try it out, but it is recommended that the generative model
	|  is written in C++.
	|
	|  This algorithm is an online planner for POMDPs. As an online planner,
	|  it needs to have a generative model of the problem. This means that
	|  it only needs a way to sample transitions and rewards from the
	|  model, but it does not need to know directly the distribution
	|  probabilities for them.
	|
	|  POMCP plans for a single belief at a time. It follows the logic of
	|  Monte Carlo Tree Sampling, where a tree structure is build
	|  progressively and action values are deduced as averages of the
	|  obtained rewards over rollouts. If the number of sample episodes is
	|  high enough, it is guaranteed to converge to the optimal solution.
	|
	|  At each rollout, we follow each action and observation within the
	|  tree from root to leaves. During this path we chose actions using an
	|  algorithm called UCT. What this does is privilege the most promising
	|  actions, while guaranteeing that in the limit every action will still
	|  be tried an infinite amount of times.
	|
	|  Once we arrive to a leaf in the tree, we then expand it with a
	|  single new node, representing a new observation we just collected.
	|  We then proceed outside the tree following a random policy, but this
	|  time we do not track which actions and observations we actually
	|  take/obtain. The final reward obtained by this random rollout policy
	|  is used to approximate the values for all nodes visited in this
	|  rollout inside the tree, before leaving it.
	|
	|  Since POMCP expands a tree, it can reuse work it has done if
	|  multiple action requests are done in order. To do so, it simply asks
	|  for the action that has been performed and its respective obtained
	|  observation. Then it simply makes that root branch the new root, and
	|  starts again.
	|
	|  In order to avoid performing belief updates between each
	|  action/observation pair, which can be expensive, POMCP uses particle
	|  beliefs. These approximate the beliefs at every step, and are used
	|  to select states in the rollouts.
	|
	|  A weakness of this implementation is that, as every particle
	|  approximation of continuous values, it will lose particles in time.
	|  To fight this a possibility is to implement a particle
	|  reinvigoration method, which would introduce noise in the particle
	|  beliefs in order to keep them 'fresh' (possibly using domain
	|  knowledge).
	|
	|  Method resolution order:
	|      POMCPModel
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (Model)m, (int)beliefSize, (int)iterations, (float)exp) -> None :
	|          Basic constructor.
	|
	|          @param m The POMDP model that POMCP will operate upon.
	|          @param beliefSize The size of the initial particle belief.
	|          @param iterations The number of episodes to run before completion.
	|          @param exp The exploration constant. This parameter is VERY important
	|                     to determine the final POMCP performance.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getBeliefSize(...)
	|      getBeliefSize( (POMCPModel)self) -> int :
	|          This function returns the initial particle size for converted Beliefs.
	|
	|  getExploration(...)
	|      getExploration( (POMCPModel)self) -> float :
	|          This function returns the currently set exploration constant.
	|
	|  getIterations(...)
	|      getIterations( (POMCPModel)self) -> int :
	|          This function returns the number of iterations performed to plan for an action.
	|
	|  getModel(...)
	|      getModel( (POMCPModel)self) -> Model :
	|          This function returns the POMDP generative model being used.
	|
	|  sampleAction(...)
	|      sampleAction( (POMCPModel)self, (Vector)b, (int)horizon) -> int :
	|          This function resets the internal graph and samples for the provided state and horizon.
	|
	|          In general it would be better if the belief did not contain
	|          any terminal states; although not necessary, it would
	|          prevent unnecessary work from being performed.
	|
	|          @param b The initial belief for the environment.
	|          @param horizon The horizon to plan for.
	|
	|          @return The best action.
	|
	|      sampleAction( (POMCPModel)self, (int)a, (int)o, (int)horizon) -> int :
	|          This function uses the internal graph to plan.
	|
	|          This function can be called after a previous call to
	|          sampleAction with a Belief. Otherwise, it will invoke it
	|          anyway with a random belief.
	|
	|          If a graph is already present though, this function will
	|          select the branch defined by the input action and
	|          observation, and prune the rest. The search will be started
	|          using the existing graph: this should make search faster,
	|          and also not require any belief updates.
	|
	|          NOTE: Currently there is no particle reinvigoration
	|          implemented, so for long horizons you can expect
	|          progressively degrading performances.
	|
	|          @param a The action taken in the last timestep.
	|          @param o The observation received in the last timestep.
	|          @param horizon The horizon to plan for.
	|
	|          @return The best action.
	|
	|  setBeliefSize(...)
	|      setBeliefSize( (POMCPModel)self, (int)beliefSize) -> None :
	|          This function sets the new size for initial beliefs created from sampleAction().
	|
	|          Note that this parameter does not bound particle beliefs
	|          created within the tree by result of rollouts: only the ones
	|          directly created from true Beliefs.
	|
	|          @param beliefSize The new particle belief size.
	|
	|  setExploration(...)
	|      setExploration( (POMCPModel)self, (float)exp) -> None :
	|          This function sets the new exploration constant for POMCP.
	|
	|          This parameter is EXTREMELY important to determine POMCP
	|          performance and, ultimately, convergence. In general it is
	|          better to find it empirically, by testing some values and
	|          see which one performs best. Tune this parameter, it really
	|          matters!
	|
	|          @param exp The new exploration constant.
	|
	|  setIterations(...)
	|      setIterations( (POMCPModel)self, (int)iterations) -> None :
	|          This function sets the number of performed rollouts in POMCP.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class POMCPSparseModel(Boost.Python.instance)
	|  This class represents the POMCP online planner using UCB1 for SparseModel.
	|
	|  NOTE: This algorithm is wrapped in Python, but as it uses the internal
	|  Models rather than a custom generative model to simulate rollouts it will
	|  probably be rather slow for interesting applications. You are of course
	|  welcome to try it out, but it is recommended that the generative model
	|  is written in C++.
	|
	|  This algorithm is an online planner for POMDPs. As an online planner,
	|  it needs to have a generative model of the problem. This means that
	|  it only needs a way to sample transitions and rewards from the
	|  model, but it does not need to know directly the distribution
	|  probabilities for them.
	|
	|  POMCP plans for a single belief at a time. It follows the logic of
	|  Monte Carlo Tree Sampling, where a tree structure is build
	|  progressively and action values are deduced as averages of the
	|  obtained rewards over rollouts. If the number of sample episodes is
	|  high enough, it is guaranteed to converge to the optimal solution.
	|
	|  At each rollout, we follow each action and observation within the
	|  tree from root to leaves. During this path we chose actions using an
	|  algorithm called UCT. What this does is privilege the most promising
	|  actions, while guaranteeing that in the limit every action will still
	|  be tried an infinite amount of times.
	|
	|  Once we arrive to a leaf in the tree, we then expand it with a
	|  single new node, representing a new observation we just collected.
	|  We then proceed outside the tree following a random policy, but this
	|  time we do not track which actions and observations we actually
	|  take/obtain. The final reward obtained by this random rollout policy
	|  is used to approximate the values for all nodes visited in this
	|  rollout inside the tree, before leaving it.
	|
	|  Since POMCP expands a tree, it can reuse work it has done if
	|  multiple action requests are done in order. To do so, it simply asks
	|  for the action that has been performed and its respective obtained
	|  observation. Then it simply makes that root branch the new root, and
	|  starts again.
	|
	|  In order to avoid performing belief updates between each
	|  action/observation pair, which can be expensive, POMCP uses particle
	|  beliefs. These approximate the beliefs at every step, and are used
	|  to select states in the rollouts.
	|
	|  A weakness of this implementation is that, as every particle
	|  approximation of continuous values, it will lose particles in time.
	|  To fight this a possibility is to implement a particle
	|  reinvigoration method, which would introduce noise in the particle
	|  beliefs in order to keep them 'fresh' (possibly using domain
	|  knowledge).
	|
	|  Method resolution order:
	|      POMCPSparseModel
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (SparseModel)m, (int)beliefSize, (int)iterations, (float)exp) -> None :
	|          Basic constructor.
	|
	|          @param m The POMDP model that POMCP will operate upon.
	|          @param beliefSize The size of the initial particle belief.
	|          @param iterations The number of episodes to run before completion.
	|          @param exp The exploration constant. This parameter is VERY important
	|                     to determine the final POMCP performance.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getBeliefSize(...)
	|      getBeliefSize( (POMCPSparseModel)self) -> int :
	|          This function returns the initial particle size for converted Beliefs.
	|
	|  getExploration(...)
	|      getExploration( (POMCPSparseModel)self) -> float :
	|          This function returns the currently set exploration constant.
	|
	|  getIterations(...)
	|      getIterations( (POMCPSparseModel)self) -> int :
	|          This function returns the number of iterations performed to plan for an action.
	|
	|  getModel(...)
	|      getModel( (POMCPSparseModel)self) -> SparseModel :
	|          This function returns the POMDP generative model being used.
	|
	|  sampleAction(...)
	|      sampleAction( (POMCPSparseModel)self, (Vector)b, (int)horizon) -> int :
	|          This function resets the internal graph and samples for the provided state and horizon.
	|
	|          In general it would be better if the belief did not contain
	|          any terminal states; although not necessary, it would
	|          prevent unnecessary work from being performed.
	|
	|          @param b The initial belief for the environment.
	|          @param horizon The horizon to plan for.
	|
	|          @return The best action.
	|
	|      sampleAction( (POMCPSparseModel)self, (int)a, (int)o, (int)horizon) -> int :
	|          This function uses the internal graph to plan.
	|
	|          This function can be called after a previous call to
	|          sampleAction with a Belief. Otherwise, it will invoke it
	|          anyway with a random belief.
	|
	|          If a graph is already present though, this function will
	|          select the branch defined by the input action and
	|          observation, and prune the rest. The search will be started
	|          using the existing graph: this should make search faster,
	|          and also not require any belief updates.
	|
	|          NOTE: Currently there is no particle reinvigoration
	|          implemented, so for long horizons you can expect
	|          progressively degrading performances.
	|
	|          @param a The action taken in the last timestep.
	|          @param o The observation received in the last timestep.
	|          @param horizon The horizon to plan for.
	|
	|          @return The best action.
	|
	|  setBeliefSize(...)
	|      setBeliefSize( (POMCPSparseModel)self, (int)beliefSize) -> None :
	|          This function sets the new size for initial beliefs created from sampleAction().
	|
	|          Note that this parameter does not bound particle beliefs
	|          created within the tree by result of rollouts: only the ones
	|          directly created from true Beliefs.
	|
	|          @param beliefSize The new particle belief size.
	|
	|  setExploration(...)
	|      setExploration( (POMCPSparseModel)self, (float)exp) -> None :
	|          This function sets the new exploration constant for POMCP.
	|
	|          This parameter is EXTREMELY important to determine POMCP
	|          performance and, ultimately, convergence. In general it is
	|          better to find it empirically, by testing some values and
	|          see which one performs best. Tune this parameter, it really
	|          matters!
	|
	|          @param exp The new exploration constant.
	|
	|  setIterations(...)
	|      setIterations( (POMCPSparseModel)self, (int)iterations) -> None :
	|          This function sets the number of performed rollouts in POMCP.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class POMDP_VFun(Boost.Python.instance)
	|  Method resolution order:
	|      POMDP_VFun
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __contains__(...)
	|      __contains__( (POMDP_VFun)arg1, (object)arg2) -> bool
	|
	|  __delitem__(...)
	|      __delitem__( (POMDP_VFun)arg1, (object)arg2) -> None
	|
	|  __getitem__(...)
	|      __getitem__( (object)arg1, (object)arg2) -> object
	|
	|  __init__(...)
	|      __init__( (object)arg1) -> None
	|
	|  __iter__(...)
	|      __iter__( (object)arg1) -> object
	|
	|  __len__(...)
	|      __len__( (POMDP_VFun)arg1) -> int
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  __setitem__(...)
	|      __setitem__( (POMDP_VFun)arg1, (object)arg2, (object)arg3) -> None
	|
	|  append(...)
	|      append( (POMDP_VFun)arg1, (object)arg2) -> None
	|
	|  extend(...)
	|      extend( (POMDP_VFun)arg1, (object)arg2) -> None
	|
	|  ----------------------------------------------------------------------
	|  Data and other attributes defined here:
	|
	|  __instance_size__ = 40
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class Policy(PolicyInterface)
	|  This class represents a POMDP Policy.
	|
	|  This class currently represents a basic Policy adaptor for a
	|  POMDP::ValueFunction. What this class does is to extract the policy
	|  tree contained within a POMDP::ValueFunction. The idea is that, at
	|  each horizon, the ValueFunction contains a set of applicable
	|  solutions (alpha vectors) for the POMDP. At each Belief point, only
	|  one of those vectors applies.
	|
	|  This class finds out at every belief which is the vector that
	|  applies, and returns the appropriate action. At the same time, it
	|  provides facilities to follow the chosen vector along the tree
	|  (since future actions depend on the observations obtained by the
	|  agent).
	|
	|  Method resolution order:
	|      Policy
	|      PolicyInterface
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __getinitargs__(...)
	|      __getinitargs__( (Policy)arg1) -> tuple
	|
	|  __getstate__(...)
	|      __getstate__( (Policy)arg1) -> tuple
	|
	|  __init__(...)
	|      __init__( (object)self, (int)s, (int)a, (int)o) -> None :
	|          Basic constrctor.
	|
	|          This constructor initializes the internal ValueFunction as
	|          having only the horizon 0 no values solution. This is most
	|          useful if the Policy needs to be read from a file.
	|
	|          @param s The number of states of the world.
	|          @param a The number of actions available to the agent.
	|          @param o The number of possible observations the agent could make.
	|
	|      __init__( (object)self, (int)s, (int)a, (int)o, (POMDP_VFun)v) -> None :
	|          Basic constrctor.
	|
	|          This constructor copies the implied policy contained in a
	|          ValueFunction.  Keep in mind that the policy stored within a
	|          ValueFunction is non-stochastic in nature, since for each
	|          state it can only save a single action.
	|
	|          @param s The number of states of the world.
	|          @param a The number of actions available to the agent.
	|          @param o The number of possible observations the agent could make.
	|          @param v The ValueFunction used as a basis for the Policy.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  __setstate__(...)
	|      __setstate__( (Policy)arg1, (tuple)arg2) -> None
	|
	|  getActionProbability(...)
	|      getActionProbability( (Policy)self, (Vector)b, (int)a, (int)horizon) -> float :
	|          This function returns the probability of taking the specified action in the specified belief.
	|
	|          @param b The selected belief.
	|          @param a The selected action.
	|          @param horizon The requested horizon, meaning the number of timesteps missing until
	|          the end of the 'episode'.
	|
	|          @return The probability of taking the selected action in
	|                  the specified belief in the specified horizon.
	|
	|  getH(...)
	|      getH( (Policy)self) -> int :
	|          This function returns the highest horizon available within this Policy.
	|
	|          Note that all functions that accept an horizon as a
	|          parameter DO NOT check the bounds of that variable. In
	|          addition, note that while for S,A,O getters you get a number
	|          that exceeds by 1 the values allowed (since counting starts
	|          from 0), here the bound is actually included in the limit,
	|          as horizon 0 does not really do anything.
	|
	|          Example: getH() returns 5. This means that 5 is the highest
	|          allowed parameter for an horizon in any other Policy method.
	|
	|          @return The highest horizon policied.
	|
	|  getO(...)
	|      getO( (Policy)self) -> int :
	|          This function returns the number of observations possible for the agent.
	|
	|  getValueFunction(...)
	|      getValueFunction( (Policy)self) -> POMDP_VFun :
	|          This function returns the internally stored ValueFunction.
	|
	|  sampleAction(...)
	|      sampleAction( (Policy)self, (Vector)b, (int)horizon) -> object :
	|          This function chooses a random action for belief b when horizon steps are missing, following the policy distribution.
	|
	|          There are a couple of differences between this sampling
	|          function and the simpler version. The first one is that this
	|          function is actually able to sample from different
	|          timesteps, since this class is able to maintain a full
	|          policy tree over time.
	|
	|          The second difference is that it returns two values. The
	|          first one is the requested action.  The second return value
	|          is an id that allows the policy to compute more efficiently
	|          the sampled action during the next timestep, if provided to
	|          the Policy together with the obtained observation.
	|
	|          @param b The sampled belief of the policy.
	|          @param horizon The requested horizon, meaning the number of
	|                         timesteps missing until the end of the
	|                         'episode'. horizon 0 will return a valid,
	|                         non-specified action.
	|
	|          @return A tuple containing the chosen action, plus an id
	|                  useful to sample an action more efficiently at the
	|                  next timestep, if required.
	|
	|      sampleAction( (Policy)self, (int)id, (int)o, (int)horizon) -> object :
	|          This function chooses a random action after performing a sampled action and observing observation o, for a particular horizon.
	|
	|          This sampling function is provided in case an already
	|          sampled action has been performed, an observation
	|          registered, and now a new action is needed for the next
	|          timestep. Using this function is highly recommended, as no
	|          belief update is necessary, and no lookup in a possibly very
	|          long list of VEntries required.
	|
	|          Note that this function works if and only if the horizon is
	|          going to be 1 (one) less than the value used for the
	|          previous sampling, otherwise anything could happen.
	|
	|          To keep things simple, the id does not store internally the
	|          needed horizon value, and you are requested to keep track of
	|          it yourself.
	|
	|          An example of usage for this function would be:
	|
	|          ~~~~~~~~~~~~~~~~~~~~~~~{.cpp}
	|          horizon = 3;
	|          // First sample
	|          auto result = sampleAction(belief, horizon);
	|          // We do the action, something happens, we get an observation.
	|          size_t observation = performAction(std::get<0>(result));
	|          --horizon;
	|          // We sample again, after reducing the horizon, with the previous id.
	|          result = sampleAction(std::get<1>(result), observation, horizon);
	|          ~~~~~~~~~~~~~~~~~~~~~~~
	|
	|          @param id An id returned from a previous call of sampleAction.
	|          @param o The observation obtained after performing a previously
	|                   sampled action.
	|          @param horizon The new horizon, equal to the old sampled horizon-1.
	|
	|          @return A tuple containing the chosen action, plus an id
	|                  useful to sample an action more efficiently at the
	|                  next timestep, if required.
	|
	|  ----------------------------------------------------------------------
	|  Data and other attributes defined here:
	|
	|  __safe_for_unpickling__ = True
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from PolicyInterface:
	|
	|  getA(...)
	|      getA( (PolicyInterface)self) -> int :
	|          This function returns the number of available actions to the agent.
	|
	|  getS(...)
	|      getS( (PolicyInterface)self) -> int :
	|          This function returns the number of states of the world.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class PolicyInterface(Boost.Python.instance)
	|  This class represents the base interface for policies in POMDPs.
	|
	|  This class represents an interface that all policies must conform to.
	|  The interface is generic as different methods may have very different
	|  ways to store and compute policies, and this interface simply asks
	|  for a way to sample them.
	|
	|  In case of POMDPs, the template parameter is of type Belief, which
	|  allows us to sample the policy from different beliefs.
	|
	|  Method resolution order:
	|      PolicyInterface
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      Raises an exception
	|      This class cannot be instantiated from Python
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getA(...)
	|      getA( (PolicyInterface)self) -> int :
	|          This function returns the number of available actions to the agent.
	|
	|  getActionProbability(...)
	|      getActionProbability( (PolicyInterface)self, (Vector)s, (int)a) -> float :
	|          This function returns the probability of taking the specified action in the specified state.
	|
	|          @param s The selected state.
	|          @param a The selected action.
	|
	|          @return The probability of taking the selected action in the specified state.
	|
	|  getS(...)
	|      getS( (PolicyInterface)self) -> int :
	|          This function returns the number of states of the world.
	|
	|  sampleAction(...)
	|      sampleAction( (PolicyInterface)self, (Vector)s) -> int :
	|          This function chooses a random action for state s, following the policy distribution.
	|
	|          @param s The sampled state of the policy.
	|
	|          @return The chosen action.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class QMDP(Boost.Python.instance)
	|  This class implements the QMDP algorithm.
	|
	|  QMDP is a particular way to approach a POMDP problem and solve it
	|  approximately. The idea is to compute a solution that disregards the
	|  partial observability for all timesteps but the next one. Thus, we
	|  assume that after the next action the agent will suddenly be able to
	|  see the true state of the environment, and act accordingly. In doing
	|  so then, it will use an MDP value function.
	|
	|  Remember that only the solution process acts this way. When time to
	|  act the QMDP solution is simply applied at every timestep, every
	|  time assuming that the partial observability is going to last one
	|  step.
	|
	|  All in all, this class is pretty much a converter of an
	|  MDP::ValueFunction into a POMDP::ValueFunction.
	|
	|  Although the solution is approximate and overconfident (since we
	|  assume that partial observability is going to go away, we think we
	|  are going to get more reward), it is still good to obtain a closer
	|  upper bound on the true solution. This can be used, for example, to
	|  boost bounds on online methods, decreasing the time they take to
	|  converge.
	|
	|  The solution returned by QMDP will thus have only horizon 1, since
	|  the horizon requested is implicitly encoded in the MDP part of the
	|  solution.
	|
	|  Method resolution order:
	|      QMDP
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __call__(...)
	|      __call__( (QMDP)self, (Model)m) -> object :
	|          This function applies the QMDP algorithm on the input POMDP.
	|
	|          This function computes the MDP::QFunction of the underlying MDP
	|          of the input POMDP with the parameters set using ValueIteration.
	|
	|          It then converts this solution into the equivalent
	|          POMDP::ValueFunction. Finally it returns both (plus the
	|          variation for the last iteration of ValueIteration).
	|
	|          Note that no pruning is performed here, so some vectors might be
	|          dominated.
	|
	|          @param m The POMDP to be solved
	|
	|          @return A tuple containing the maximum variation for the
	|                  ValueFunction, the computed ValueFunction and the
	|                  equivalent MDP::QFunction.
	|
	|      __call__( (QMDP)self, (SparseModel)m) -> object :
	|          This function applies the QMDP algorithm on the input POMDP.
	|
	|          This function computes the MDP::QFunction of the underlying MDP
	|          of the input POMDP with the parameters set using ValueIteration.
	|
	|          It then converts this solution into the equivalent
	|          POMDP::ValueFunction. Finally it returns both (plus the
	|          variation for the last iteration of ValueIteration).
	|
	|          Note that no pruning is performed here, so some vectors might be
	|          dominated.
	|
	|          @param m The POMDP to be solved
	|
	|          @return A tuple containing the maximum variation for the
	|                  ValueFunction, the computed ValueFunction and the
	|                  equivalent MDP::QFunction.
	|
	|  __init__(...)
	|      __init__( (object)self, (int)horizon, (float)tolerance) -> None :
	|          Basic constructor.
	|
	|          QMDP uses MDP::ValueIteration in order to solve the
	|          underlying MDP of the POMDP. Thus, its parameters (and
	|          bounds) are the same.
	|
	|          @param horizon The maximum number of iterations to perform.
	|          @param tolerance The tolerance factor to stop the value iteration loop.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getHorizon(...)
	|      getHorizon( (QMDP)self) -> int :
	|          This function returns the currently set horizon parameter.
	|
	|  getTolerance(...)
	|      getTolerance( (QMDP)self) -> float :
	|          This function returns the currently set tolerance parameter.
	|
	|  setHorizon(...)
	|      setHorizon( (QMDP)self, (int)horizon) -> None :
	|          This function sets the horizon parameter.
	|
	|  setTolerance(...)
	|      setTolerance( (QMDP)self, (float)tolerance) -> None :
	|          This function sets the tolerance parameter.
	|
	|          The tolerance parameter must be >= 0.0, otherwise the function
	|          will throw an std::invalid_argument. The tolerance parameter
	|          sets the convergence criterion. A tolerance of 0.0 forces the
	|          internal ValueIteration to perform a number of iterations
	|          equal to the horizon specified. Otherwise, ValueIteration
	|          will stop as soon as the difference between two iterations
	|          is less than the tolerance specified.
	|
	|          @param tolerance The new tolerance parameter.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class RTBSSModel(Boost.Python.instance)
	|  This class represents the RTBSS online planner for Model.
	|
	|  This algorithm is an online planner for POMDPs. It works by pretty
	|  much solving the whole POMDP in a straightforward manner, but just
	|  for the belief it is currently in, and the horizon specified.
	|
	|  Additionally, it uses an heuristic function in order to prune
	|  branches which cannot possibly help in determining which action is
	|  the actual best. Currently this heuristic is very crude, as it
	|  requires the user to manually input a maximum possible reward, and
	|  using it as an upper bound.
	|
	|  Additionally, in theory one would want to explore branches from the
	|  most promising to the least promising, to maximize pruning. This is
	|  currently not done here, since an heuristic is intrinsically
	|  determined by a particular problem. At the same time, it is easy to
	|  add one, as the code specifies where one should be inserted.
	|
	|  This method is able to return not only the best available action,
	|  but also the (in theory) true value of that action in the current
	|  belief. Note that values computed in different methods may differ
	|  due to floating point approximation errors.
	|
	|  Method resolution order:
	|      RTBSSModel
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (Model)m, (float)maxR) -> None :
	|          Basic constructor.
	|
	|          @param m The POMDP model that POMCP will operate upon.
	|          @param maxR The max reward obtainable in the model.
	|                 This is used for the pruning heuristic.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getModel(...)
	|      getModel( (RTBSSModel)self) -> Model :
	|          This function returns the POMDP generative model being used.
	|
	|  sampleAction(...)
	|      sampleAction( (RTBSSModel)self, (Vector)b, (int)horizon) -> object :
	|          This function computes the best value for a given belief and its value.
	|
	|          @param b The initial belief for the environment.
	|          @param horizon The horizon to plan for.
	|
	|          @return The best action and its value in the model.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class RTBSSSparseModel(Boost.Python.instance)
	|  This class represents the RTBSS online planner for SparseModel.
	|
	|  This algorithm is an online planner for POMDPs. It works by pretty
	|  much solving the whole POMDP in a straightforward manner, but just
	|  for the belief it is currently in, and the horizon specified.
	|
	|  Additionally, it uses an heuristic function in order to prune
	|  branches which cannot possibly help in determining which action is
	|  the actual best. Currently this heuristic is very crude, as it
	|  requires the user to manually input a maximum possible reward, and
	|  using it as an upper bound.
	|
	|  Additionally, in theory one would want to explore branches from the
	|  most promising to the least promising, to maximize pruning. This is
	|  currently not done here, since an heuristic is intrinsically
	|  determined by a particular problem. At the same time, it is easy to
	|  add one, as the code specifies where one should be inserted.
	|
	|  This method is able to return not only the best available action,
	|  but also the (in theory) true value of that action in the current
	|  belief. Note that values computed in different methods may differ
	|  due to floating point approximation errors.
	|
	|  Method resolution order:
	|      RTBSSSparseModel
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __init__(...)
	|      __init__( (object)self, (SparseModel)m, (float)maxR) -> None :
	|          Basic constructor.
	|
	|          @param m The POMDP model that POMCP will operate upon.
	|          @param maxR The max reward obtainable in the model.
	|                 This is used for the pruning heuristic.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getModel(...)
	|      getModel( (RTBSSSparseModel)self) -> SparseModel :
	|          This function returns the POMDP generative model being used.
	|
	|  sampleAction(...)
	|      sampleAction( (RTBSSSparseModel)self, (Vector)b, (int)horizon) -> object :
	|          This function computes the best value for a given belief and its value.
	|
	|          @param b The initial belief for the environment.
	|          @param horizon The horizon to plan for.
	|
	|          @return The best action and its value in the model.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class SparseModel(AIToolbox.MDP.SparseModel)
	|  This class represents a Partially Observable Markov Decision Process.
	|
	|  This class inherits from any valid MDP model type, so that it can
	|  use its base methods, and it builds from those. Templated inheritance
	|  was chosen to improve performance and keep code small, instead of
	|  doing composition.
	|
	|  A POMDP is an MDP where the agent, at each timestep, does not know
	|  in which state it is. Instead, after each action is performed, it
	|  obtains an 'observation', which offers some information as to which
	|  new state the agent has transitioned to. This observation is
	|  determined by an 'observation function', that maps S'xAxO to a
	|  probability: the probability of obtaining observation O after taking
	|  action A and *landing* in state S'.
	|
	|  Since now its knowledge is imperfect, in order to represent the
	|  knowledge of the state it is currently in, the agent is thus forced
	|  to use Beliefs: probability distributions over states.
	|
	|  The way a Belief works is that, after each action and observation,
	|  the agent can reason as follows: given my previous Belief
	|  (distribution over states) that I think I was in, what is now the
	|  probability that I transitioned to any particular state? This new
	|  Belief can be computed from the Model, given that the agent knows
	|  the distributions of the transition and observation functions.
	|
	|  Turns out that a POMDP can be viewed as an MDP with an infinite
	|  number of states, where each state is essentially a Belief. Since a
	|  Belief is a vector of real numbers, there are infinite of them, thus
	|  the infinite number of states. While POMDPs can be much more
	|  powerful than MDPs for modeling real world problems, where
	|  information is usually not perfect, it turns out that this
	|  infinite-state property makes them so much harder to solve
	|  perfectly, and their solutions much more complex.
	|
	|  A POMDP solution is composed by several policies, which apply in
	|  different ranges of the Belief space, and suggest different actions
	|  depending on the observations received by the agent at each
	|  timestep. The values of those policies can be, in the same way,
	|  represented as a number of value vectors (called alpha vectors in
	|  the literature) that apply in those same ranges of the Belief space.
	|  Each alpha vector is somewhat similar to an MDP ValueFunction.
	|
	|  The difference between this class and the POMDP::Model class is that
	|  this class stores observations in a sparce matrix. This results in a
	|  possibly slower access to individual probabilities, but immeasurably
	|  speeds up computation with some classes of planning algorithms in
	|  case the number of possible observations is very small with respect
	|  to the total theoretic observation space of SxAxO. It also of course
	|  incredibly reduces memory consumption in such cases, which may also
	|  improve speed by effect of improved caching.
	|
	|  Method resolution order:
	|      SparseModel
	|      AIToolbox.MDP.SparseModel
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __getinitargs__(...)
	|      __getinitargs__( (SparseModel)arg1) -> tuple
	|
	|  __getstate__(...)
	|      __getstate__( (SparseModel)arg1) -> tuple
	|
	|  __init__(...)
	|      __init__( (object)self, (int)o, (int)s, (int)a [, (float)discount]) -> None :
	|          Basic constructor.
	|
	|          This constructor initializes the observation function
	|          so that all actions will return observation 0.
	|
	|          This constructor initializes the SparseModel so that all
	|          transitions happen with probability 0 but for transitions
	|          that bring back to the same state, no matter the action.
	|
	|          All rewards are set to 0. The discount parameter is set to
	|          1.
	|
	|          @param o The number of possible observations the agent could make.
	|          @param s The number of states of the world.
	|          @param a The number of actions available to the agent.
	|          @param discount The discount factor for the MDP.
	|
	|
	|      __init__( (object)self, (Model)model) -> None :
	|          This allows to copy from any other model. A nice use for this is to
	|          convert any model which computes probabilities on the fly into an
	|          MDP::Model where probabilities are all stored for fast access. Of
	|          course such a solution can be done only when the number of states
	|          and actions is not too big.
	|
	|      __init__( (object)self, (SparseModel)sparseModel) -> None :
	|          This allows to copy from any other model. A nice use for this is to
	|          convert any model which computes probabilities on the fly into an
	|          MDP::Model where probabilities are all stored for fast access. Of
	|          course such a solution can be done only when the number of states
	|          and actions is not too big.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  __setstate__(...)
	|      __setstate__( (SparseModel)arg1, (tuple)arg2) -> None
	|
	|  getO(...)
	|      getO( (SparseModel)self) -> int :
	|          This function returns the number of observations possible.
	|
	|  getObservationProbability(...)
	|      getObservationProbability( (SparseModel)self, (int)s, (int)a, (int)s1) -> float :
	|          This function returns the stored observation probability for the specified state-action pair.
	|
	|  sampleOR(...)
	|      sampleOR( (SparseModel)self, (int)s, (int)a, (int)s1) -> object :
	|          This function samples the POMDP for the specified state action pair.
	|
	|          This function samples the model for simulated experience.
	|          The transition, observation and reward functions are used to
	|          produce, from the state, action and new state inserted as
	|          arguments, a possible new observation and reward. The
	|          observation and rewards are picked so that they are
	|          consistent with the specified new state.
	|
	|          @param s The state that needs to be sampled.
	|          @param a The action that needs to be sampled.
	|          @param s1 The resulting state of the s,a transition.
	|
	|          @return A tuple containing a new observation and reward.
	|
	|  sampleSOR(...)
	|      sampleSOR( (SparseModel)self, (int)s, (int)a) -> object :
	|          This function samples the POMDP for the specified state action pair.
	|
	|          This function samples the model for simulated experience. The
	|          transition, observation and reward functions are used to
	|          produce, from the state action pair inserted as arguments, a
	|          possible new state with respective observation and reward.
	|          The new state is picked from all possible states that the
	|          MDP allows transitioning to, each with probability equal to
	|          the same probability of the transition in the model. After a
	|          new state is picked, an observation is sampled from the
	|          observation function distribution, and finally the reward is
	|          the corresponding reward contained in the reward function.
	|
	|          @param s The state that needs to be sampled.
	|          @param a The action that needs to be sampled.
	|
	|          @return A tuple containing a new state, observation and reward.
	|
	|  setObservationFunction(...)
	|      setObservationFunction( (SparseModel)self, (object)observationFunction3D) -> None :
	|          This function replaces the SparseModel observation function with the one provided.
	|
	|          Currently the Python wrappings support reading through native 3d Python
	|          arrays (so [][][]). As long as the dimensions are correct and they contain
	|          correct probabilities everything should be fine. The code should reject
	|          them otherwise.
	|
	|  ----------------------------------------------------------------------
	|  Data and other attributes defined here:
	|
	|  __safe_for_unpickling__ = True
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from AIToolbox.MDP.SparseModel:
	|
	|  getA(...)
	|      getA( (SparseModel)self) -> int :
	|          This function returns the number of available actions to the agent.
	|
	|  getDiscount(...)
	|      getDiscount( (SparseModel)self) -> float :
	|          This function returns the currently set discount factor.
	|
	|  getExpectedReward(...)
	|      getExpectedReward( (SparseModel)self, (int)s, (int)a, (int)s1) -> float :
	|          This function returns the stored expected reward for the specified transition.
	|
	|  getS(...)
	|      getS( (SparseModel)self) -> int :
	|          This function returns the number of states of the world.
	|
	|  getTransitionProbability(...)
	|      getTransitionProbability( (SparseModel)self, (int)s, (int)a, (int)s1) -> float :
	|          This function returns the stored transition probability for the specified transition.
	|
	|  isTerminal(...)
	|      isTerminal( (SparseModel)self, (int)s) -> bool :
	|          This function returns whether a given state is a terminal.
	|
	|  sampleSR(...)
	|      sampleSR( (SparseModel)self, (int)s, (int)a) -> object :
	|          This function samples the MDP for the specified state action pair.
	|
	|          This function samples the model for simulated experience.
	|          The transition and reward functions are used to produce,
	|          from the state action pair inserted as arguments, a possible
	|          new state with respective reward.  The new state is picked
	|          from all possible states that the MDP allows transitioning
	|          to, each with probability equal to the same probability of
	|          the transition in the model. After a new state is picked,
	|          the reward is the corresponding reward contained in the
	|          reward function.
	|
	|          @param s The state that needs to be sampled.
	|          @param a The action that needs to be sampled.
	|
	|          @return A tuple containing a new state and a reward.
	|
	|  setDiscount(...)
	|      setDiscount( (SparseModel)self, (float)discount) -> None :
	|          This function sets a new discount factor for the SparseModel.
	|
	|  setRewardFunction(...)
	|      setRewardFunction( (SparseModel)self, (object)rewardFunction3D) -> None :
	|          This function replaces the SparseModel reward function with the one provided.
	|
	|          Currently the Python wrappings support reading through native 3d Python
	|          arrays (so [][][]). As long as the dimensions are correct and they contain
	|          correct probabilities everything should be fine. The code should reject
	|          them otherwise.
	|
	|  setTransitionFunction(...)
	|      setTransitionFunction( (SparseModel)self, (object)transitionFunction3D) -> None :
	|          This function replaces the SparseModel transition function with the one provided.
	|
	|          Currently the Python wrappings support reading through native 3d Python
	|          arrays (so [][][]). As long as the dimensions are correct and they contain
	|          correct probabilities everything should be fine. The code should reject
	|          them otherwise.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class VEntry(Boost.Python.instance)
	|  Method resolution order:
	|      VEntry
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __eq__(...)
	|      __eq__( (VEntry)arg1, (VEntry)arg2) -> bool
	|
	|  __init__(...)
	|      __init__( (object)arg1) -> None
	|
	|  __lt__(...)
	|      __lt__( (VEntry)arg1, (VEntry)arg2) -> bool
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors defined here:
	|
	|  action
	|
	|  observations
	|
	|  values
	|
	|  ----------------------------------------------------------------------
	|  Data and other attributes defined here:
	|
	|  __instance_size__ = 64
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class VList(Boost.Python.instance)
	|  Method resolution order:
	|      VList
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __contains__(...)
	|      __contains__( (VList)arg1, (object)arg2) -> bool
	|
	|  __delitem__(...)
	|      __delitem__( (VList)arg1, (object)arg2) -> None
	|
	|  __getitem__(...)
	|      __getitem__( (object)arg1, (object)arg2) -> object
	|
	|  __init__(...)
	|      __init__( (object)arg1) -> None
	|
	|  __iter__(...)
	|      __iter__( (object)arg1) -> object
	|
	|  __len__(...)
	|      __len__( (VList)arg1) -> int
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  __setitem__(...)
	|      __setitem__( (VList)arg1, (object)arg2, (object)arg3) -> None
	|
	|  append(...)
	|      append( (VList)arg1, (object)arg2) -> None
	|
	|  extend(...)
	|      extend( (VList)arg1, (object)arg2) -> None
	|
	|  ----------------------------------------------------------------------
	|  Data and other attributes defined here:
	|
	|  __instance_size__ = 40
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	class Witness(Boost.Python.instance)
	|  This class implements the Witness algorithm.
	|
	|  This algorithm solves a POMDP Model perfectly. It computes solutions
	|  for each horizon incrementally, every new solution building upon the
	|  previous one.
	|
	|  The Witness algorithm tries to avoid creating all possible cross-sums
	|  of the projected vectors. Instead, it relies on a proof that states
	|  that if a VEntry is suboptimal, then we can at least find a better one
	|  by modifying a single subtree.
	|
	|  Given this, the Witness algorithm starts off by finding a single optimal
	|  VEntry for a random belief. Then, using the theorem, it knows that if a
	|  better VEntry exists, then there must be at least one VEntry completely
	|  equal to the one we just found but for a subtree, and that one will
	|  be better. Thus, it adds to an agenda all possible variations of the
	|  found optimal VEntry.
	|
	|  From there, it examines each one of them, trying to look for a witness
	|  point. Once found, again it produces an optimal VEntry for that point
	|  and adds to the agenda all of its possible variations. VEntry which do
	|  not have any witness points are removed from the agenda.
	|
	|  In addition, Witness will not add to the agenda any VEntry which it has
	|  already added; it uses a set to keep track of which combinations of
	|  subtrees it has already tried.
	|
	|  Method resolution order:
	|      Witness
	|      Boost.Python.instance
	|      builtins.object
	|
	|  Static methods defined here:
	|
	|  __call__(...)
	|      __call__( (Witness)self, (Model)model) -> object :
	|          This function solves a POMDP::Model completely.
	|
	|          This function is pretty expensive (as are possibly all POMDP
	|          solvers). It solves a series of LPs trying to find all possible
	|          beliefs where an alphavector has not yet been found.
	|
	|          @param model The POMDP model that needs to be solved.
	|
	|          @return A tuple containing the maximum variation for the
	|                  ValueFunction and the computed ValueFunction.
	|
	|      __call__( (Witness)self, (SparseModel)model) -> object :
	|          This function solves a POMDP::Model completely.
	|
	|          This function is pretty expensive (as are possibly all POMDP
	|          solvers). It solves a series of LPs trying to find all possible
	|          beliefs where an alphavector has not yet been found.
	|
	|          @param model The POMDP model that needs to be solved.
	|
	|          @return A tuple containing the maximum variation for the
	|                  ValueFunction and the computed ValueFunction.
	|
	|  __init__(...)
	|      __init__( (object)self, (int)horizon, (float)tolerance) -> None :
	|          Basic constructor.
	|
	|          This constructor sets the default horizon used to solve a POMDP::Model.
	|
	|          The tolerance parameter must be >= 0.0, otherwise the
	|          constructor will throw an std::runtime_error. The tolerance
	|          parameter sets the convergence criterion. A tolerance of 0.0
	|          forces Witness to perform a number of iterations equal to
	|          the horizon specified. Otherwise, Witness will stop as soon
	|          as the difference between two iterations is less than the
	|          tolerance specified.
	|
	|          @param h The horizon chosen.
	|          @param tolerance The tolerance factor to stop the value iteration loop.
	|
	|  __reduce__ = <unnamed Boost.Python function>(...)
	|
	|  getHorizon(...)
	|      getHorizon( (Witness)self) -> int :
	|          This function returns the currently set horizon parameter.
	|
	|  getTolerance(...)
	|      getTolerance( (Witness)self) -> float :
	|          This function returns the currently set tolerance parameter.
	|
	|  setHorizon(...)
	|      setHorizon( (Witness)self, (int)horizon) -> None :
	|          This function allows setting the horizon parameter.
	|
	|  setTolerance(...)
	|      setTolerance( (Witness)self, (float)t) -> None :
	|          This function sets the tolerance parameter.
	|
	|          The tolerance parameter must be >= 0.0, otherwise the
	|          constructor will throw an std::runtime_error. The tolerance
	|          parameter sets the convergence criterion. A tolerance of 0.0
	|          forces Witness to perform a number of iterations equal to
	|          the horizon specified. Otherwise, Witness will stop as soon
	|          as the difference between two iterations is less than the
	|          tolerance specified.
	|
	|          @param t The new tolerance parameter.
	|
	|  ----------------------------------------------------------------------
	|  Static methods inherited from Boost.Python.instance:
	|
	|  __new__(*args, **kwargs) from Boost.Python.class
	|      Create and return a new object.  See help(type) for accurate signature.
	|
	|  ----------------------------------------------------------------------
	|  Data descriptors inherited from Boost.Python.instance:
	|
	|  __dict__
	|
	|  __weakref__
	
	FUNCTIONS
	computeOptimisticValue(...)
	computeOptimisticValue( (Vector)p, (vec_eigen_v)points, (object)values) -> float :
	This function computes the optimistic value of a point given known vertices and values.
	
	This function computes an LP to determine the best possible value of a
	point given all known best vertices around it.
	
	This function is needed in multi-objective settings (rather than
	POMDPs), since the step where we compute the optimal value for a given
	point is extremely expensive (it requires solving a full MDP). Thus
	linear programming is used in order to determine an optimistic bound
	when deciding the next point to extract from the queue during the linear
	support process.
	
	Note that the input is the same as a PointSurface; the two components
	have been kept as separate arguments simply to allow more freedom to the
	user.
	
	@param p The point where we want to compute the best possible value.
	@param points The points that make up the surface.
	@param values The respective values of the input points.
	
	@return The best possible value that the input point can have given the known vertices.
	
	findVerticesNaive(...)
	findVerticesNaive( (vec_eigen_v)tests, (vec_eigen_v)planes) -> object :
	This function implements a naive vertex enumeration algorithm.
	
	This function goes through every subset of planes of size S, and finds
	all vertices it can. In particular, it goes through the first list one
	element at a time, and joins it with S-1 elements from the second list.
	
	Even more precisely, we take >= 1 elements from the second list. The
	remaining elements (so that in total we still use S-1) are simply the
	simplex boundaries, which allows us to find the corners located there.
	
	This method may find duplicate vertices (it does not bother to prune
	them), as a vertex can be in the convergence of more than S planes.
	
	The advantage is that we do not need any linear programming, and simple
	matrix decomposition techniques suffice.
	
	Warning: the values of each vertex depends on the planes it has been
	found of, and thus may *not* be the true value if considering all planes
	at the same time!
	
	@param tests The range of the planes to find vertices for.
	@param planes The range of all other planes.
	
	@return A non-unique list of all the vertices found.
	
	updateBelief(...)
	updateBelief( (Model)model, (Vector)b, (int)a, (int)o) -> Vector :
	This function creates a new Belief by updating the input Belie
	with the input action and observation, following th
	transition/observation functions contained in the input model
	
	@param model The model used to update the belief
	@param b The old belief
	@param a The action taken during the transition
	@param o The observation registered
	
	updateBelief( (SparseModel)model, (Vector)b, (int)a, (int)o) -> Vector :
	This function creates a new Belief by updating the input Belie
	with the input action and observation, following th
	transition/observation functions contained in the input model
	
	@param model The model used to update the belief
	@param b The old belief
	@param a The action taken during the transition
	@param o The observation registered
	
	FILE
	(built-in)
	
	
exporting to image
	exporting layers 0/0 3.038
	writing image sha256:46e86ee8a5c491be25fabed5b497463d7bdb1a285b7ba82c808057cf39efe24c 0/0 0.001
	naming to docker.io/library/ai-toolbox-full:0 0/0 0.003
